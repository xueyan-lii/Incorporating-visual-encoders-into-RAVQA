/home/xl544/.conda/envs/RAVQA/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning
  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')
[38;20m[INFO] - __main__ : Initialization done with the config: {'DATA_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data', 'EXPERIMENT_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments', 'TENSORBOARD_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data_TB/tb_logs', 'WANDB': {'entity': 'xl544', 'project': 'RAVQA', 'tags': ['OKVQA', 'RAG', 'freeze_question_encoder', 'force_existence'], 'name': 'OKVQA_RA-VQA-FrDPR_FullCorpus.20289468'}, 'cache': {'default_folder': '../data/ok-vqa/cache', 'regenerate': {'ocr_feature_preprocessed': 0, 'test_data_preprocessed': 0, 'train_data_preprocessed': 0, 'vinvl_feature_preprocessed': 0}}, 'cuda': 0, 'data_loader': {'additional': {'max_decoder_source_length': 512, 'max_source_length': 512, 'max_target_length': 10, 'num_knowledge_passages': 5}, 'dataset_modules': {'module_dict': {'LoadGoogleOCRFeatures': {'config': {'combine_with_vinvl': True, 'test': '../data/ok-vqa/pre-extracted_features/OCR/valid', 'train': '../data/ok-vqa/pre-extracted_features/OCR/train'}, 'option': 'default', 'type': 'LoadGoogleOCRFeatures'}, 'LoadGoogleSearchAnnotations': {'config': {'annotations_path': {'test': '../data/ok-vqa/pre-extracted_features/passages/retriever_test.json', 'train': '../data/ok-vqa/pre-extracted_features/passages/retriever_train.json', 'valid': '../data/ok-vqa/pre-extracted_features/passages/retriever_testdev.json'}}, 'option': 'default', 'type': 'LoadGoogleSearchAnnotations'}, 'LoadGoogleSearchPassageData': {'config': {'passage_data_path': {'full': '../data/ok-vqa/pre-extracted_features/passages/okvqa_full_corpus.csv', 'train': '../data/ok-vqa/pre-extracted_features/passages/okvqa_train_corpus.csv'}, 'use_full_split': True}, 'option': 'default', 'type': 'LoadGoogleSearchPassageData'}, 'LoadOKVQAData': {'config': {'image_data_path': {'test': '../data/ok-vqa/val2014', 'train': '../data/ok-vqa/train2014'}, 'vqa_data_path': {'annotation_files': {'test': '../data/ok-vqa/mscoco_val2014_annotations.json', 'train': '../data/ok-vqa/mscoco_train2014_annotations.json'}, 'question_files': {'test': '../data/ok-vqa/OpenEnded_mscoco_val2014_questions.json', 'train': '../data/ok-vqa/OpenEnded_mscoco_train2014_questions.json'}}}, 'option': 'default', 'type': 'LoadOKVQAData'}, 'LoadOscarCaptionFeatures': {'config': {'test': '../data/ok-vqa/pre-extracted_features/captions/test_predictions.json', 'train': '../data/ok-vqa/pre-extracted_features/captions/train_predictions.json', 'valid': '../data/ok-vqa/pre-extracted_features/captions/valid_predictions.json'}, 'option': 'default', 'type': 'LoadOscarCaptionFeatures'}, 'LoadPretrainedDPROutputForGoogleSearchPassage': {'config': {'pretrained_dpr_outputs': {'test': '../Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/test/test_evaluation/test_predictions.json', 'train': '../Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/test/test_evaluation/train_predictions.json'}}, 'option': 'none', 'type': 'LoadPretrainedDPROutputForGoogleSearchPassage'}, 'LoadVinVLFeatures': {'config': {'test': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_testset_full/inference/vinvl_vg_x152c4/predictions.tsv', 'train': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_trainset_full/inference/vinvl_vg_x152c4/predictions.tsv'}, 'option': 'default', 'type': 'LoadVinVLFeatures'}}, 'module_list': ['LoadVinVLFeatures', 'LoadGoogleOCRFeatures', 'LoadOscarCaptionFeatures', 'LoadOKVQAData', 'LoadGoogleSearchPassageData']}, 'dataset_type': 'OKVQADataset', 'dummy_dataloader': 0, 'index_files': {'index_passages_path': '../data/ok-vqa/pre-extracted_features/faiss/ok-vqa-passages-full-caption-pretrained-NewRun/my_knowledge_dataset', 'index_path': '../data/ok-vqa/pre-extracted_features/faiss/ok-vqa-passages-full-caption-pretrained-NewRun/my_knowledge_dataset_hnsw_index.faiss'}, 'type': 'DataLoaderOKVQAWithKnowledge'}, 'experiment_name': 'OKVQA_RA-VQA-FrDPR_FullCorpus.20289468', 'gpu_device': 0, 'ignore_pretrained_weights': [], 'metrics': [{'name': 'compute_exact_match'}, {'name': 'compute_retrieval_metrics'}, {'name': 'compute_okvqa_scores'}], 'model_config': {'DECODER_SPECIAL_TOKENS': {'additional_special_tokens': ['<BOV>', '<SOV>', '<EOV>', '<BOQ>', '<EOQ>', '<BOC>', '<EOC>', '<BOK>', '<EOK>'], 'bos_token': '<PAD>', 'pad_token': '<PAD>'}, 'DecoderTokenizerClass': 'T5Tokenizer', 'DecoderTokenizerModelVersion': 't5-large', 'GeneratorConfigClass': 'T5Config', 'GeneratorModelClass': 'T5ForConditionalGeneration', 'GeneratorModelVersion': 't5-large', 'ModelClass': 'RagModel', 'QueryEncoderConfigClass': 'DPRConfig', 'QueryEncoderModelClass': 'DPRQuestionEncoder', 'QueryEncoderModelVersion': '../Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/train/saved_model/epoch6/query_encoder', 'RAVQA_loss_type': 'Approach6', 'SPECIAL_TOKENS': {'additional_special_tokens': ['<BOV>', '<SOV>', '<EOV>', '<BOQ>', '<EOQ>', '<BOC>', '<EOC>', '<BOK>', '<EOK>']}, 'TokenizerClass': 'DPRQuestionEncoderTokenizer', 'TokenizerModelVersion': 'facebook/dpr-question_encoder-single-nq-base', 'base_model': 'RAG', 'decoder_input_modules': {'module_list': [], 'postprocess_module_list': []}, 'input_modules': {'module_list': [{'option': 'default', 'separation_tokens': {'end': '<EOQ>', 'start': '<BOQ>'}, 'type': 'QuestionInput'}, {'option': 'caption', 'separation_tokens': {'end': '<EOC>', 'start': '<BOC>'}, 'type': 'TextBasedVisionInput'}, {'attribute_max': 3, 'attribute_thres': 0.05, 'object_max': 40, 'ocr': 1, 'option': 'object', 'separation_tokens': {'end': '<EOV>', 'sep': '<SOV>', 'start': '<BOV>'}, 'type': 'TextBasedVisionInput'}], 'postprocess_module_list': [{'option': 'default', 'type': 'PostProcessInputTokenization'}]}, 'loss_ratio': {'additional_loss': 0, 'nll_loss': 1, 'rag_loss': 0, 'retrieval_pseudo_loss': 0}, 'modules': ['freeze_question_encoder', 'force_existence'], 'output_modules': {'module_list': [{'option': 'default', 'type': 'GenerationOutput'}], 'postprocess_module_list': [{'option': 'default', 'type': 'PostProcessOutputTokenization'}]}, 'pretrained': 1, 'rag_modules': {'module_list': []}}, 'platform_type': 'pytorch', 'seed': 2021, 'test': {'additional': {'multiprocessing': 4}, 'batch_size': 32, 'evaluation_name': 'test_evaluation', 'load_best_model': 0, 'load_epoch': -1, 'load_model_path': '../Experiments/OKVQA_RA-VQA-FrDPR_FullCorpus.19792250/train/saved_model/model_6.ckpt', 'num_evaluation': 0}, 'train': {'adam_epsilon': 1e-08, 'additional': {'gradient_accumulation_steps': 4, 'gradient_clipping': 0, 'plugins': [], 'save_top_k': 1, 'save_top_k_metric': 'test/accuracy_overall', 'save_top_k_mode': 'max', 'warmup_steps': 0}, 'batch_size': 32, 'epochs': 9999, 'load_best_model': 0, 'load_epoch': -1, 'load_model_path': '', 'lr': 0.0001, 'retriever_lr': 1e-05, 'save_interval': 1, 'scheduler': 'none', 'type': 'RagExecutor'}, 'valid': {'additional': {}, 'batch_size': 32, 'break_interval': 3000, 'step_size': 100}, 'reset': False, 'mode': 'test', 'log_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-FrDPR_FullCorpus.20289468/test', 'experiment_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-FrDPR_FullCorpus.20289468', 'saved_model_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-FrDPR_FullCorpus.20289468/train/saved_model', 'imgs_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-FrDPR_FullCorpus.20289468/test/test_evaluation/imgs', 'results_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-FrDPR_FullCorpus.20289468/test/test_evaluation', 'tensorboard_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data_TB/tb_logs/OKVQA_RA-VQA-FrDPR_FullCorpus.20289468', 'args': {'config': '../configs/okvqa/RAVQA.jsonnet', 'DATA_FOLDER': '', 'EXPERIMENT_FOLDER': '', 'mode': 'test', 'reset': False, 'experiment_name': 'OKVQA_RA-VQA-FrDPR_FullCorpus.20289468', 'tags': [], 'modules': ['freeze_question_encoder', 'force_existence'], 'log_prediction_tables': True, 'test_batch_size': -1, 'test_evaluation_name': '', 'logger': True, 'checkpoint_callback': None, 'enable_checkpointing': True, 'default_root_dir': None, 'gradient_clip_val': None, 'gradient_clip_algorithm': None, 'process_position': 0, 'num_nodes': 1, 'num_processes': None, 'devices': '1', 'gpus': None, 'auto_select_gpus': False, 'tpu_cores': None, 'ipus': None, 'log_gpu_memory': None, 'progress_bar_refresh_rate': None, 'enable_progress_bar': True, 'overfit_batches': 0.0, 'track_grad_norm': -1, 'check_val_every_n_epoch': 1, 'fast_dev_run': False, 'accumulate_grad_batches': None, 'max_epochs': None, 'min_epochs': None, 'max_steps': -1, 'min_steps': None, 'max_time': None, 'limit_train_batches': None, 'limit_val_batches': None, 'limit_test_batches': None, 'limit_predict_batches': None, 'val_check_interval': None, 'flush_logs_every_n_steps': None, 'log_every_n_steps': 50, 'accelerator': 'auto', 'strategy': None, 'sync_batchnorm': False, 'precision': 32, 'enable_model_summary': True, 'weights_summary': 'top', 'weights_save_path': None, 'num_sanity_val_steps': 2, 'resume_from_checkpoint': None, 'profiler': None, 'benchmark': None, 'deterministic': None, 'reload_dataloaders_every_n_epochs': 0, 'auto_lr_find': False, 'replace_sampler_ddp': True, 'detect_anomaly': False, 'auto_scale_batch_size': False, 'prepare_data_per_node': None, 'plugins': None, 'amp_backend': 'native', 'amp_level': None, 'move_metrics_to_cpu': False, 'multiple_trainloader_mode': 'max_size_cycle', 'stochastic_weight_avg': False, 'terminate_on_nan': None, 'opts': ['data_loader.additional.num_knowledge_passages=5', 'test.load_model_path=../Experiments/OKVQA_RA-VQA-FrDPR_FullCorpus.19792250/train/saved_model/model_6.ckpt']}}[0m
Global seed set to 2021
[38;20m[INFO] - __main__ : All seeds have been set to 2021[0m
[38;20m[INFO] - __main__ : init wandb logger with the following settings: {'entity': 'xl544', 'project': 'RAVQA', 'tags': ['OKVQA', 'RAG', 'freeze_question_encoder', 'force_existence'], 'name': 'OKVQA_RA-VQA-FrDPR_FullCorpus.20289468'}[0m
wandb: Currently logged in as: xl544. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.2
wandb: Run data is saved locally in /rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/src/wandb/run-20230515_193153-vswun5ub
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run OKVQA_RA-VQA-FrDPR_FullCorpus.20289468
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xl544/RAVQA
wandb: üöÄ View run at https://wandb.ai/xl544/RAVQA/runs/vswun5ub
Multiprocessing is handled by SLURM.
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[38;20m[INFO] - __main__ : arguments passed to trainer: Namespace(DATA_FOLDER='', EXPERIMENT_FOLDER='', accelerator='auto', accumulate_grad_batches=None, amp_backend='native', amp_level=None, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, benchmark=None, check_val_every_n_epoch=1, checkpoint_callback=None, config='../configs/okvqa/RAVQA.jsonnet', default_root_dir=None, detect_anomaly=False, deterministic=None, devices='1', enable_checkpointing=True, enable_model_summary=True, enable_progress_bar=True, experiment_name='OKVQA_RA-VQA-FrDPR_FullCorpus.20289468', fast_dev_run=False, flush_logs_every_n_steps=None, gpus=None, gradient_clip_algorithm=None, gradient_clip_val=None, ipus=None, limit_predict_batches=None, limit_test_batches=None, limit_train_batches=None, limit_val_batches=None, log_every_n_steps=50, log_gpu_memory=None, log_prediction_tables=True, logger=True, max_epochs=None, max_steps=-1, max_time=None, min_epochs=None, min_steps=None, mode='test', modules=['freeze_question_encoder', 'force_existence'], move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=None, num_sanity_val_steps=2, opts=['data_loader.additional.num_knowledge_passages=5', 'test.load_model_path=../Experiments/OKVQA_RA-VQA-FrDPR_FullCorpus.19792250/train/saved_model/model_6.ckpt'], overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=None, process_position=0, profiler=None, progress_bar_refresh_rate=None, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, reset=False, resume_from_checkpoint=None, stochastic_weight_avg=False, strategy=None, sync_batchnorm=False, tags=[], terminate_on_nan=None, test_batch_size=-1, test_evaluation_name='', tpu_cores=None, track_grad_norm=-1, val_check_interval=None, weights_save_path=None, weights_summary='top')[0m
[38;20m[INFO] - __main__ : additional arguments passed to trainer: {'accumulate_grad_batches': 4, 'default_root_dir': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-FrDPR_FullCorpus.20289468/train/saved_model', 'max_epochs': 9999, 'logger': [<pytorch_lightning.loggers.tensorboard.TensorBoardLogger object at 0x1478787296a0>, <pytorch_lightning.loggers.wandb.WandbLogger object at 0x1478786f82e0>, <utils.metrics_log_callback.MetricsHistoryLogger object at 0x147807da4f10>], 'callbacks': [<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x147878704430>, <pytorch_lightning.callbacks.progress.tqdm_progress.TQDMProgressBar object at 0x147807823610>, <pytorch_lightning.callbacks.model_summary.ModelSummary object at 0x147807823c70>, <pytorch_lightning.callbacks.gradient_accumulation_scheduler.GradientAccumulationScheduler object at 0x147807823790>], 'plugins': [], 'log_every_n_steps': 10}[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'test': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_testset_full/inference/vinvl_vg_x152c4/predictions.tsv', 'train': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_trainset_full/inference/vinvl_vg_x152c4/predictions.tsv'}, 'option': 'default', 'type': 'LoadVinVLFeatures'}[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/vinvl_feature_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data Statistics] VinVL features 14031[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'combine_with_vinvl': True, 'test': '../data/ok-vqa/pre-extracted_features/OCR/valid', 'train': '../data/ok-vqa/pre-extracted_features/OCR/train'}, 'option': 'default', 'type': 'LoadGoogleOCRFeatures'}[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/ocr_feature_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data Statistics] OCR features 14031, 5462 has annotations.[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : OCR feature detected in VinVL feature dict...skipping..[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'test': '../data/ok-vqa/pre-extracted_features/captions/test_predictions.json', 'train': '../data/ok-vqa/pre-extracted_features/captions/train_predictions.json', 'valid': '../data/ok-vqa/pre-extracted_features/captions/valid_predictions.json'}, 'option': 'default', 'type': 'LoadOscarCaptionFeatures'}[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'image_data_path': {'test': '../data/ok-vqa/val2014', 'train': '../data/ok-vqa/train2014'}, 'vqa_data_path': {'annotation_files': {'test': '../data/ok-vqa/mscoco_val2014_annotations.json', 'train': '../data/ok-vqa/mscoco_train2014_annotations.json'}, 'question_files': {'test': '../data/ok-vqa/OpenEnded_mscoco_val2014_questions.json', 'train': '../data/ok-vqa/OpenEnded_mscoco_train2014_questions.json'}}}, 'option': 'default', 'type': 'LoadOKVQAData'}[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/train_data_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data statistics] split: train  entries: 9009[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/test_data_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data statistics] split: test  entries: 5046[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'passage_data_path': {'full': '../data/ok-vqa/pre-extracted_features/passages/okvqa_full_corpus.csv', 'train': '../data/ok-vqa/pre-extracted_features/passages/okvqa_train_corpus.csv'}, 'use_full_split': True}, 'option': 'default', 'type': 'LoadGoogleSearchPassageData'}[0m
  0%|          | 0/168380 [00:00<?, ?it/s] 10%|‚ñâ         | 16652/168380 [00:00<00:00, 166503.88it/s] 20%|‚ñà‚ñâ        | 33303/168380 [00:00<00:00, 164635.32it/s] 30%|‚ñà‚ñà‚ñâ       | 49877/168380 [00:00<00:00, 165126.52it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 66510/168380 [00:00<00:00, 165594.94it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 83097/168380 [00:00<00:00, 165690.53it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 99667/168380 [00:00<00:00, 165139.16it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 116848/168380 [00:00<00:00, 167308.74it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 135892/168380 [00:00<00:00, 174653.50it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 155409/168380 [00:00<00:00, 181054.42it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 168307/168380 [00:00<00:00, 174288.99it/s]
[38;20m[INFO] - data_loader_manager.datasets.okvqa_datasets : initialising OKVQADataset...[0m
[38;20m[INFO] - data_loader_manager.datasets.okvqa_datasets : initialising OKVQADataset...[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa_with_knowledge : [Data Statistics]: training data loader: 282;  test data loader: 158[0m
[38;20m[INFO] - torch.distributed.nn.jit.instantiator : Created a temporary directory at /tmp/tmpofbxk1rs[0m
[38;20m[INFO] - torch.distributed.nn.jit.instantiator : Writing /tmp/tmpofbxk1rs/_remote_module_non_scriptable.py[0m
[38;20m[INFO] - trainers.base_executor : Initializing RagExecutor...[0m
Restoring states from the checkpoint path at ../Experiments/OKVQA_RA-VQA-FrDPR_FullCorpus.19792250/train/saved_model/model_6.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Missing logger folder: /rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data_TB/tb_logs/OKVQA_RA-VQA-FrDPR_FullCorpus.20289468/OKVQA_RA-VQA-FrDPR_FullCorpus.20289468
Loaded model weights from checkpoint at ../Experiments/OKVQA_RA-VQA-FrDPR_FullCorpus.19792250/train/saved_model/model_6.ckpt
/home/xl544/.conda/envs/RAVQA/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [test]: {'test_evaluation/exact_match_at_1': 0.5618311533888228, 'test_evaluation/exact_match_at_2': 0.6858898137138327, 'test_evaluation/exact_match_at_3': 0.7160126833135156, 'test_evaluation/exact_match_at_4': 0.7265160523186682, 'test_evaluation/exact_match_at_5': 0.727705112960761, 'test_evaluation/recall_at_5': 0.8127229488703924, 'test_evaluation/precision_at_5': 0.5183511692429646, 'test_evaluation/gold_precision_at_5': 0.37360285374554103, 'test_evaluation/gold_recall_at_5': 0.6652794292508918, 'test_evaluation/successful_hit': 0.35723345223939756, 'test_evaluation/successful_no_hit': 0.17621878715814507, 'test_evaluation/failed_hit': 0.2534284581847008, 'test_evaluation/failed_no_hit': 0.21311930241775665, 'test_evaluation/selected_successful_hit': 0.41954022988505746, 'test_evaluation/selected_successful_no_hit': 0.14229092350376535, 'test_evaluation/selected_failed_hit': 0.2861672611969877, 'test_evaluation/selected_failed_no_hit': 0.15200158541418946, 'test_evaluation/n_retrieved_docs': 5, 'test_evaluation/accuracy_overall': 51.63, 'test_evaluation/accuracy_QuestionType_one': 47.82, 'test_evaluation/accuracy_QuestionType_eight': 49.98, 'test_evaluation/accuracy_QuestionType_other': 53.47, 'test_evaluation/accuracy_QuestionType_seven': 49.67, 'test_evaluation/accuracy_QuestionType_four': 57.32, 'test_evaluation/accuracy_QuestionType_five': 53.19, 'test_evaluation/accuracy_QuestionType_three': 50.98, 'test_evaluation/accuracy_QuestionType_nine': 40.0, 'test_evaluation/accuracy_QuestionType_ten': 55.19, 'test_evaluation/accuracy_QuestionType_two': 55.7, 'test_evaluation/accuracy_QuestionType_six': 50.21, 'test_evaluation/accuracy_AnswerType_other': 51.63, 'test_evaluation/epoch': 6}[0m
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                       epoch ‚ñÅ
wandb:   test_evaluation/accuracy_AnswerType_other ‚ñÅ
wandb: test_evaluation/accuracy_QuestionType_eight ‚ñÅ
wandb:  test_evaluation/accuracy_QuestionType_five ‚ñÅ
wandb:  test_evaluation/accuracy_QuestionType_four ‚ñÅ
wandb:  test_evaluation/accuracy_QuestionType_nine ‚ñÅ
wandb:   test_evaluation/accuracy_QuestionType_one ‚ñÅ
wandb: test_evaluation/accuracy_QuestionType_other ‚ñÅ
wandb: test_evaluation/accuracy_QuestionType_seven ‚ñÅ
wandb:   test_evaluation/accuracy_QuestionType_six ‚ñÅ
wandb:   test_evaluation/accuracy_QuestionType_ten ‚ñÅ
wandb: test_evaluation/accuracy_QuestionType_three ‚ñÅ
wandb:   test_evaluation/accuracy_QuestionType_two ‚ñÅ
wandb:            test_evaluation/accuracy_overall ‚ñÅ
wandb:                       test_evaluation/epoch ‚ñÅ
wandb:            test_evaluation/exact_match_at_1 ‚ñÅ
wandb:            test_evaluation/exact_match_at_2 ‚ñÅ
wandb:            test_evaluation/exact_match_at_3 ‚ñÅ
wandb:            test_evaluation/exact_match_at_4 ‚ñÅ
wandb:            test_evaluation/exact_match_at_5 ‚ñÅ
wandb:                  test_evaluation/failed_hit ‚ñÅ
wandb:               test_evaluation/failed_no_hit ‚ñÅ
wandb:         test_evaluation/gold_precision_at_5 ‚ñÅ
wandb:            test_evaluation/gold_recall_at_5 ‚ñÅ
wandb:            test_evaluation/n_retrieved_docs ‚ñÅ
wandb:              test_evaluation/precision_at_5 ‚ñÅ
wandb:                 test_evaluation/recall_at_5 ‚ñÅ
wandb:         test_evaluation/selected_failed_hit ‚ñÅ
wandb:      test_evaluation/selected_failed_no_hit ‚ñÅ
wandb:     test_evaluation/selected_successful_hit ‚ñÅ
wandb:  test_evaluation/selected_successful_no_hit ‚ñÅ
wandb:              test_evaluation/successful_hit ‚ñÅ
wandb:           test_evaluation/successful_no_hit ‚ñÅ
wandb:                         trainer/global_step ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                                       epoch 6
wandb:   test_evaluation/accuracy_AnswerType_other 51.63
wandb: test_evaluation/accuracy_QuestionType_eight 49.98
wandb:  test_evaluation/accuracy_QuestionType_five 53.19
wandb:  test_evaluation/accuracy_QuestionType_four 57.32
wandb:  test_evaluation/accuracy_QuestionType_nine 40.0
wandb:   test_evaluation/accuracy_QuestionType_one 47.82
wandb: test_evaluation/accuracy_QuestionType_other 53.47
wandb: test_evaluation/accuracy_QuestionType_seven 49.67
wandb:   test_evaluation/accuracy_QuestionType_six 50.21
wandb:   test_evaluation/accuracy_QuestionType_ten 55.19
wandb: test_evaluation/accuracy_QuestionType_three 50.98
wandb:   test_evaluation/accuracy_QuestionType_two 55.7
wandb:            test_evaluation/accuracy_overall 51.63
wandb:                       test_evaluation/epoch 6.0
wandb:            test_evaluation/exact_match_at_1 0.56183
wandb:            test_evaluation/exact_match_at_2 0.68589
wandb:            test_evaluation/exact_match_at_3 0.71601
wandb:            test_evaluation/exact_match_at_4 0.72652
wandb:            test_evaluation/exact_match_at_5 0.72771
wandb:                  test_evaluation/failed_hit 0.25343
wandb:               test_evaluation/failed_no_hit 0.21312
wandb:         test_evaluation/gold_precision_at_5 0.3736
wandb:            test_evaluation/gold_recall_at_5 0.66528
wandb:            test_evaluation/n_retrieved_docs 5.0
wandb:              test_evaluation/precision_at_5 0.51835
wandb:                 test_evaluation/recall_at_5 0.81272
wandb:         test_evaluation/selected_failed_hit 0.28617
wandb:      test_evaluation/selected_failed_no_hit 0.152
wandb:     test_evaluation/selected_successful_hit 0.41954
wandb:  test_evaluation/selected_successful_no_hit 0.14229
wandb:              test_evaluation/successful_hit 0.35723
wandb:           test_evaluation/successful_no_hit 0.17622
wandb:                         trainer/global_step 0
wandb: 
wandb: üöÄ View run OKVQA_RA-VQA-FrDPR_FullCorpus.20289468 at: https://wandb.ai/xl544/RAVQA/runs/vswun5ub
wandb: Synced 6 W&B file(s), 1 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230515_193153-vswun5ub/logs
