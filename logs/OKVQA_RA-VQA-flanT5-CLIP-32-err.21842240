/home/xl544/.conda/envs/RAVQA/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning
  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')
[38;20m[INFO] - __main__ : Initialization done with the config: {'DATA_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data', 'EXPERIMENT_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments', 'TENSORBOARD_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data_TB/tb_logs', 'WANDB': {'entity': 'xl544', 'project': 'RAVQA', 'tags': ['OKVQA', 'RAG', 'freeze_question_encoder', 'force_existence'], 'name': 'OKVQA_RA-VQA-flanT5-CLIP-32.21842240'}, 'cache': {'default_folder': '../data/ok-vqa/cache', 'regenerate': {'clip_embeddings': 0, 'ocr_feature_preprocessed': 0, 'qformer_embeddings': 0, 'test_data_preprocessed': 0, 'train_data_preprocessed': 0, 'vinvl_feature_preprocessed': 0}}, 'cuda': 0, 'data_loader': {'additional': {'max_decoder_source_length': 512, 'max_source_length': 512, 'max_target_length': 10, 'num_knowledge_passages': 5}, 'dataset_modules': {'module_dict': {'LoadClipEmbeddings': {'config': {'clip_embeddings': {'train': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_train2014.pkl', 'val': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_val2014.pkl'}, 'qformer_embeddings': {'train': '../data/ok-vqa/pre-extracted_features/blip2_head_embeddings/coco_eva_clip_g_qformer_train2014.pkl', 'val': '../data/ok-vqa/pre-extracted_features/blip2_head_embeddings/coco_eva_clip_g_qformer_val2014.pkl'}}, 'option': 'default', 'type': 'EmbeddingInput'}, 'LoadGoogleOCRFeatures': {'config': {'combine_with_vinvl': True, 'test': '../data/ok-vqa/pre-extracted_features/OCR/valid', 'train': '../data/ok-vqa/pre-extracted_features/OCR/train'}, 'option': 'default', 'type': 'LoadGoogleOCRFeatures'}, 'LoadGoogleSearchAnnotations': {'config': {'annotations_path': {'test': '../data/ok-vqa/pre-extracted_features/passages/retriever_test.json', 'train': '../data/ok-vqa/pre-extracted_features/passages/retriever_train.json', 'valid': '../data/ok-vqa/pre-extracted_features/passages/retriever_testdev.json'}}, 'option': 'default', 'type': 'LoadGoogleSearchAnnotations'}, 'LoadGoogleSearchPassageData': {'config': {'passage_data_path': {'full': '../data/ok-vqa/pre-extracted_features/passages/okvqa_full_corpus.csv', 'train': '../data/ok-vqa/pre-extracted_features/passages/okvqa_train_corpus.csv'}, 'use_full_split': True}, 'option': 'default', 'type': 'LoadGoogleSearchPassageData'}, 'LoadOKVQAData': {'config': {'image_data_path': {'test': '../data/ok-vqa/val2014', 'train': '../data/ok-vqa/train2014'}, 'vqa_data_path': {'annotation_files': {'test': '../data/ok-vqa/mscoco_val2014_annotations.json', 'train': '../data/ok-vqa/mscoco_train2014_annotations.json'}, 'question_files': {'test': '../data/ok-vqa/OpenEnded_mscoco_val2014_questions.json', 'train': '../data/ok-vqa/OpenEnded_mscoco_train2014_questions.json'}}}, 'option': 'default', 'type': 'LoadOKVQAData'}, 'LoadOscarCaptionFeatures': {'config': {'test': '../data/ok-vqa/pre-extracted_features/captions/test_predictions.json', 'train': '../data/ok-vqa/pre-extracted_features/captions/train_predictions.json', 'valid': '../data/ok-vqa/pre-extracted_features/captions/valid_predictions.json'}, 'option': 'default', 'type': 'LoadOscarCaptionFeatures'}, 'LoadPretrainedDPROutputForGoogleSearchPassage': {'config': {'pretrained_dpr_outputs': {'test': '../Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/test/test_evaluation/test_predictions.json', 'train': '../Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/test/test_evaluation/train_predictions.json'}}, 'option': 'none', 'type': 'LoadPretrainedDPROutputForGoogleSearchPassage'}, 'LoadVinVLFeatures': {'config': {'test': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_testset_full/inference/vinvl_vg_x152c4/predictions.tsv', 'train': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_trainset_full/inference/vinvl_vg_x152c4/predictions.tsv'}, 'option': 'default', 'type': 'LoadVinVLFeatures'}}, 'module_list': ['LoadVinVLFeatures', 'LoadGoogleOCRFeatures', 'LoadOscarCaptionFeatures', 'LoadOKVQAData', 'LoadGoogleSearchPassageData', 'LoadClipEmbeddings']}, 'dataset_type': 'OKVQADataset', 'dummy_dataloader': 0, 'index_files': {'index_passages_path': '../data/ok-vqa/pre-extracted_features/faiss/ok-vqa-passages-full-caption-pretrained-NewRun/my_knowledge_dataset', 'index_path': '../data/ok-vqa/pre-extracted_features/faiss/ok-vqa-passages-full-caption-pretrained-NewRun/my_knowledge_dataset_hnsw_index.faiss'}, 'type': 'DataLoaderOKVQAWithKnowledge'}, 'experiment_name': 'OKVQA_RA-VQA-flanT5-CLIP-32.21842240', 'gpu_device': 0, 'ignore_pretrained_weights': [], 'metrics': [{'name': 'compute_exact_match'}, {'name': 'compute_retrieval_metrics'}, {'name': 'compute_okvqa_scores'}], 'model_config': {'DECODER_SPECIAL_TOKENS': {'additional_special_tokens': ['<BOV>', '<SOV>', '<EOV>', '<BOQ>', '<EOQ>', '<BOC>', '<EOC>', '<BOK>', '<EOK>'], 'bos_token': '<PAD>', 'pad_token': '<PAD>'}, 'DecoderTokenizerClass': 'T5Tokenizer', 'DecoderTokenizerModelVersion': 'google/flan-t5-large', 'GeneratorConfigClass': 'T5Config', 'GeneratorModelClass': 'T5ForConditionalGeneration', 'GeneratorModelVersion': 'google/flan-t5-large', 'LoadPretrainedMLPWeights': 0, 'ModelClass': 'RagModel', 'PretrainedMLPPath': '', 'QueryEncoderConfigClass': 'DPRConfig', 'QueryEncoderModelClass': 'DPRQuestionEncoder', 'QueryEncoderModelVersion': '../Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/train/saved_model/epoch6/query_encoder', 'RAVQA_loss_type': 'Approach6', 'SPECIAL_TOKENS': {'additional_special_tokens': ['<BOV>', '<SOV>', '<EOV>', '<BOQ>', '<EOQ>', '<BOC>', '<EOC>', '<BOK>', '<EOK>']}, 'TokenizerClass': 'DPRQuestionEncoderTokenizer', 'TokenizerModelVersion': 'facebook/dpr-question_encoder-single-nq-base', 'UsePrefixEmb': 1, 'UseQformerEmb': 0, 'base_model': 'RAG', 'decoder_input_modules': {'module_list': [], 'postprocess_module_list': []}, 'input_modules': {'module_list': [{'option': 'default', 'separation_tokens': {'end': '<EOQ>', 'start': '<BOQ>'}, 'type': 'QuestionInput'}, {'option': 'caption', 'separation_tokens': {'end': '<EOC>', 'start': '<BOC>'}, 'type': 'TextBasedVisionInput'}, {'attribute_max': 3, 'attribute_thres': 0.05, 'object_max': 40, 'ocr': 1, 'option': 'object', 'separation_tokens': {'end': '<EOV>', 'sep': '<SOV>', 'start': '<BOV>'}, 'type': 'TextBasedVisionInput'}, {'option': 'default', 'type': 'EmbeddingInput'}], 'postprocess_module_list': [{'option': 'default', 'type': 'PostProcessInputTokenization'}, {'option': 'default', 'type': 'PostProcessClipEmbeddings'}]}, 'loss_ratio': {'additional_loss': 0, 'nll_loss': 1, 'rag_loss': 0, 'retrieval_pseudo_loss': 0}, 'modules': ['freeze_question_encoder', 'force_existence'], 'output_modules': {'module_list': [{'option': 'default', 'type': 'GenerationOutput'}], 'postprocess_module_list': [{'option': 'default', 'type': 'PostProcessOutputTokenization'}]}, 'pretrained': 1, 'rag_modules': {'module_list': []}}, 'platform_type': 'pytorch', 'seed': 2021, 'test': {'additional': {'multiprocessing': 4}, 'batch_size': 32, 'evaluation_name': 'test_evaluation', 'load_best_model': 0, 'load_epoch': -1, 'load_model_path': '', 'num_evaluation': 0}, 'train': {'adam_epsilon': 1e-08, 'additional': {'gradient_accumulation_steps': 16, 'gradient_clipping': 0, 'plugins': [], 'save_top_k': 1, 'save_top_k_metric': 'test/accuracy_overall', 'save_top_k_mode': 'max', 'warmup_steps': 0}, 'batch_size': 2, 'epochs': 8, 'load_best_model': 0, 'load_epoch': -1, 'load_model_path': '', 'lr': 6e-05, 'retriever_lr': 1e-05, 'save_interval': 1, 'scheduler': 'linear', 'type': 'RagExecutor'}, 'valid': {'additional': {}, 'batch_size': 32, 'break_interval': 3000, 'step_size': 0.25}, 'reset': False, 'mode': 'train', 'log_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-flanT5-CLIP-32.21842240/train', 'experiment_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-flanT5-CLIP-32.21842240', 'saved_model_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-flanT5-CLIP-32.21842240/train/saved_model', 'imgs_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-flanT5-CLIP-32.21842240/train/imgs', 'tensorboard_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data_TB/tb_logs/OKVQA_RA-VQA-flanT5-CLIP-32.21842240', 'args': {'config': '../configs/okvqa/RAVQA.jsonnet', 'DATA_FOLDER': '', 'EXPERIMENT_FOLDER': '', 'mode': 'train', 'reset': False, 'experiment_name': 'OKVQA_RA-VQA-flanT5-CLIP-32.21842240', 'tags': [], 'modules': ['freeze_question_encoder', 'force_existence'], 'log_prediction_tables': False, 'test_batch_size': -1, 'test_evaluation_name': '', 'logger': True, 'checkpoint_callback': None, 'enable_checkpointing': True, 'default_root_dir': None, 'gradient_clip_val': None, 'gradient_clip_algorithm': None, 'process_position': 0, 'num_nodes': 1, 'num_processes': None, 'devices': '1', 'gpus': None, 'auto_select_gpus': False, 'tpu_cores': None, 'ipus': None, 'log_gpu_memory': None, 'progress_bar_refresh_rate': None, 'enable_progress_bar': True, 'overfit_batches': 0.0, 'track_grad_norm': -1, 'check_val_every_n_epoch': 1, 'fast_dev_run': False, 'accumulate_grad_batches': None, 'max_epochs': None, 'min_epochs': None, 'max_steps': -1, 'min_steps': None, 'max_time': None, 'limit_train_batches': None, 'limit_val_batches': None, 'limit_test_batches': None, 'limit_predict_batches': None, 'val_check_interval': None, 'flush_logs_every_n_steps': None, 'log_every_n_steps': 50, 'accelerator': 'auto', 'strategy': None, 'sync_batchnorm': False, 'precision': 32, 'enable_model_summary': True, 'weights_summary': 'top', 'weights_save_path': None, 'num_sanity_val_steps': 2, 'resume_from_checkpoint': None, 'profiler': None, 'benchmark': None, 'deterministic': None, 'reload_dataloaders_every_n_epochs': 0, 'auto_lr_find': False, 'replace_sampler_ddp': True, 'detect_anomaly': False, 'auto_scale_batch_size': False, 'prepare_data_per_node': None, 'plugins': None, 'amp_backend': 'native', 'amp_level': None, 'move_metrics_to_cpu': False, 'multiple_trainloader_mode': 'max_size_cycle', 'stochastic_weight_avg': False, 'terminate_on_nan': None, 'opts': ['train.epochs=8', 'train.batch_size=2', 'valid.step_size=0.25', 'valid.batch_size=32', 'train.additional.gradient_accumulation_steps=16', 'train.lr=0.00006', 'train.retriever_lr=0.00001', 'train.scheduler=linear', 'data_loader.additional.num_knowledge_passages=5', 'model_config.UsePrefixEmb=1', 'model_config.DecoderTokenizerModelVersion=google/flan-t5-large', 'model_config.GeneratorModelVersion=google/flan-t5-large']}}[0m
Global seed set to 2021
[38;20m[INFO] - __main__ : All seeds have been set to 2021[0m
[38;20m[INFO] - __main__ : init wandb logger with the following settings: {'entity': 'xl544', 'project': 'RAVQA', 'tags': ['OKVQA', 'RAG', 'freeze_question_encoder', 'force_existence'], 'name': 'OKVQA_RA-VQA-flanT5-CLIP-32.21842240'}[0m
wandb: Currently logged in as: xl544. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.2
wandb: Run data is saved locally in /rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/src/wandb/run-20230607_015016-grkat660
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run OKVQA_RA-VQA-flanT5-CLIP-32.21842240
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xl544/RAVQA
wandb: üöÄ View run at https://wandb.ai/xl544/RAVQA/runs/grkat660
Multiprocessing is handled by SLURM.
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[38;20m[INFO] - __main__ : arguments passed to trainer: Namespace(DATA_FOLDER='', EXPERIMENT_FOLDER='', accelerator='auto', accumulate_grad_batches=None, amp_backend='native', amp_level=None, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, benchmark=None, check_val_every_n_epoch=1, checkpoint_callback=None, config='../configs/okvqa/RAVQA.jsonnet', default_root_dir=None, detect_anomaly=False, deterministic=None, devices='1', enable_checkpointing=True, enable_model_summary=True, enable_progress_bar=True, experiment_name='OKVQA_RA-VQA-flanT5-CLIP-32.21842240', fast_dev_run=False, flush_logs_every_n_steps=None, gpus=None, gradient_clip_algorithm=None, gradient_clip_val=None, ipus=None, limit_predict_batches=None, limit_test_batches=None, limit_train_batches=None, limit_val_batches=None, log_every_n_steps=50, log_gpu_memory=None, log_prediction_tables=False, logger=True, max_epochs=None, max_steps=-1, max_time=None, min_epochs=None, min_steps=None, mode='train', modules=['freeze_question_encoder', 'force_existence'], move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=None, num_sanity_val_steps=2, opts=['train.epochs=8', 'train.batch_size=2', 'valid.step_size=0.25', 'valid.batch_size=32', 'train.additional.gradient_accumulation_steps=16', 'train.lr=0.00006', 'train.retriever_lr=0.00001', 'train.scheduler=linear', 'data_loader.additional.num_knowledge_passages=5', 'model_config.UsePrefixEmb=1', 'model_config.DecoderTokenizerModelVersion=google/flan-t5-large', 'model_config.GeneratorModelVersion=google/flan-t5-large'], overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=None, process_position=0, profiler=None, progress_bar_refresh_rate=None, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, reset=False, resume_from_checkpoint=None, stochastic_weight_avg=False, strategy=None, sync_batchnorm=False, tags=[], terminate_on_nan=None, test_batch_size=-1, test_evaluation_name='', tpu_cores=None, track_grad_norm=-1, val_check_interval=None, weights_save_path=None, weights_summary='top')[0m
[38;20m[INFO] - __main__ : additional arguments passed to trainer: {'accumulate_grad_batches': 16, 'default_root_dir': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-flanT5-CLIP-32.21842240/train/saved_model', 'max_epochs': 8, 'logger': [<pytorch_lightning.loggers.tensorboard.TensorBoardLogger object at 0x7fe0d80b62e0>, <pytorch_lightning.loggers.wandb.WandbLogger object at 0x7fe0d80b6f40>, <utils.metrics_log_callback.MetricsHistoryLogger object at 0x7fe0d801dd90>], 'callbacks': [<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7fe0d80b6e20>, <pytorch_lightning.callbacks.progress.tqdm_progress.TQDMProgressBar object at 0x7fe0d17bf9a0>, <pytorch_lightning.callbacks.model_summary.ModelSummary object at 0x7fe0d17bf0a0>, <pytorch_lightning.callbacks.gradient_accumulation_scheduler.GradientAccumulationScheduler object at 0x7fe0d17bf040>], 'plugins': [], 'log_every_n_steps': 10, 'val_check_interval': 0.25}[0m
[33;20m[WARNING] - __main__ : No checkpoint exists from '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-flanT5-CLIP-32.21842240/train/saved_model/last.ckpt'. Skipping...[0m
[38;20m[INFO] - __main__ : **First time to train**[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'test': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_testset_full/inference/vinvl_vg_x152c4/predictions.tsv', 'train': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_trainset_full/inference/vinvl_vg_x152c4/predictions.tsv'}, 'option': 'default', 'type': 'LoadVinVLFeatures'}[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/vinvl_feature_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data Statistics] VinVL features 14031[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'combine_with_vinvl': True, 'test': '../data/ok-vqa/pre-extracted_features/OCR/valid', 'train': '../data/ok-vqa/pre-extracted_features/OCR/train'}, 'option': 'default', 'type': 'LoadGoogleOCRFeatures'}[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/ocr_feature_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data Statistics] OCR features 14031, 5462 has annotations.[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : OCR feature detected in VinVL feature dict...skipping..[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'test': '../data/ok-vqa/pre-extracted_features/captions/test_predictions.json', 'train': '../data/ok-vqa/pre-extracted_features/captions/train_predictions.json', 'valid': '../data/ok-vqa/pre-extracted_features/captions/valid_predictions.json'}, 'option': 'default', 'type': 'LoadOscarCaptionFeatures'}[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'image_data_path': {'test': '../data/ok-vqa/val2014', 'train': '../data/ok-vqa/train2014'}, 'vqa_data_path': {'annotation_files': {'test': '../data/ok-vqa/mscoco_val2014_annotations.json', 'train': '../data/ok-vqa/mscoco_train2014_annotations.json'}, 'question_files': {'test': '../data/ok-vqa/OpenEnded_mscoco_val2014_questions.json', 'train': '../data/ok-vqa/OpenEnded_mscoco_train2014_questions.json'}}}, 'option': 'default', 'type': 'LoadOKVQAData'}[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/train_data_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data statistics] split: train  entries: 9009[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/test_data_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data statistics] split: test  entries: 5046[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'passage_data_path': {'full': '../data/ok-vqa/pre-extracted_features/passages/okvqa_full_corpus.csv', 'train': '../data/ok-vqa/pre-extracted_features/passages/okvqa_train_corpus.csv'}, 'use_full_split': True}, 'option': 'default', 'type': 'LoadGoogleSearchPassageData'}[0m
  0%|          | 0/168380 [00:00<?, ?it/s] 10%|‚ñà         | 16864/168380 [00:00<00:00, 168624.08it/s] 20%|‚ñà‚ñà        | 33727/168380 [00:00<00:00, 164950.92it/s] 30%|‚ñà‚ñà‚ñâ       | 50228/168380 [00:00<00:00, 163984.95it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 66644/168380 [00:00<00:00, 164041.15it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 83351/168380 [00:00<00:00, 165123.88it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 99866/168380 [00:00<00:00, 162196.29it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 117148/168380 [00:00<00:00, 165608.77it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 136314/168380 [00:00<00:00, 173813.35it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 155924/168380 [00:00<00:00, 180726.77it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 168307/168380 [00:00<00:00, 173210.99it/s]
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'clip_embeddings': {'train': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_train2014.pkl', 'val': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_val2014.pkl'}, 'qformer_embeddings': {'train': '../data/ok-vqa/pre-extracted_features/blip2_head_embeddings/coco_eva_clip_g_qformer_train2014.pkl', 'val': '../data/ok-vqa/pre-extracted_features/blip2_head_embeddings/coco_eva_clip_g_qformer_val2014.pkl'}}, 'option': 'default', 'type': 'EmbeddingInput'}[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/clip_embeddings.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data Statistics] CLIP embeddings 123287[0m
[38;20m[INFO] - data_loader_manager.datasets.okvqa_datasets : initialising OKVQADataset...[0m
[38;20m[INFO] - data_loader_manager.datasets.okvqa_datasets : initialising OKVQADataset...[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa_with_knowledge : [Data Statistics]: training data loader: 4505;  test data loader: 158[0m
[38;20m[INFO] - torch.distributed.nn.jit.instantiator : Created a temporary directory at /tmp/tmp8sacqake[0m
[38;20m[INFO] - torch.distributed.nn.jit.instantiator : Writing /tmp/tmp8sacqake/_remote_module_non_scriptable.py[0m
[38;20m[INFO] - trainers.base_executor : Initializing RagExecutor...[0m
[38;20m[INFO] - __main__ : config file was successfully saved to /rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-flanT5-CLIP-32.21842240 for future use.[0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[38;20m[INFO] - trainers.rag_executor : using different learning rate for retriever[0m
[38;20m[INFO] - trainers.rag_executor : #params: 558   lr: 6e-05[0m
[38;20m[INFO] - trainers.rag_executor : #params: 2   lr: 1e-05[0m
Loading `train_dataloader` to estimate number of stepping batches.
/home/xl544/.conda/envs/RAVQA/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(

  | Name  | Type     | Params
-----------------------------------
0 | model | RagModel | 917 M 
-----------------------------------
808 M     Trainable params
109 M     Non-trainable params
917 M     Total params
3,671.204 Total estimated model params size (MB)
Missing logger folder: /rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data_TB/tb_logs/OKVQA_RA-VQA-flanT5-CLIP-32.21842240/OKVQA_RA-VQA-flanT5-CLIP-32.21842240
SLURM auto-requeueing enabled. Setting signal handlers.
/home/xl544/.conda/envs/RAVQA/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Failed to compute OKVQA scores: Results do not correspond to current VQA set. Either the results do not have predictions for all question ids in annotation file or there is atleast one question id that does not belong to the question ids in the annotation file.This could be due to the fact that OKVQA parser requires all questions to evaluatethe accuracy. Ignore this error if this is the sanity check.[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [sanity_check]: {'test/exact_match_at_1': 0.03125, 'test/exact_match_at_2': 0.046875, 'test/exact_match_at_3': 0.046875, 'test/exact_match_at_4': 0.0625, 'test/exact_match_at_5': 0.0625, 'test/recall_at_5': 0.796875, 'test/precision_at_5': 0.515625, 'test/gold_precision_at_5': 0.34062500000000007, 'test/gold_recall_at_5': 0.65625, 'test/successful_hit': 0.0125, 'test/successful_no_hit': 0.003125, 'test/failed_hit': 0.54375, 'test/failed_no_hit': 0.440625, 'test/selected_successful_hit': 0.03125, 'test/selected_successful_no_hit': 0.0, 'test/selected_failed_hit': 0.609375, 'test/selected_failed_no_hit': 0.359375, 'test/n_retrieved_docs': 5, 'test/epoch': 0}[0m
[33;20m[WARNING] - root : Sanity check mode, not saving to loggers.[0m
/home/xl544/.conda/envs/RAVQA/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.4902893380895759, 'test/exact_match_at_2': 0.6206896551724138, 'test/exact_match_at_3': 0.6676575505350772, 'test/exact_match_at_4': 0.6805390408244154, 'test/exact_match_at_5': 0.6852952833927863, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37138327388030123, 'test/gold_recall_at_5': 0.6632976615140705, 'test/successful_hit': 0.32782401902497027, 'test/successful_no_hit': 0.11708283789139913, 'test/failed_hit': 0.3730875941339675, 'test/failed_no_hit': 0.1820055489496631, 'test/selected_successful_hit': 0.39437177962742764, 'test/selected_successful_no_hit': 0.09591755846214824, 'test/selected_failed_hit': 0.40725326991676575, 'test/selected_failed_no_hit': 0.10245739199365834, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 44.98, 'test/accuracy_QuestionType_one': 39.9, 'test/accuracy_QuestionType_eight': 42.93, 'test/accuracy_QuestionType_other': 45.3, 'test/accuracy_QuestionType_seven': 45.89, 'test/accuracy_QuestionType_four': 51.71, 'test/accuracy_QuestionType_five': 46.19, 'test/accuracy_QuestionType_three': 43.6, 'test/accuracy_QuestionType_nine': 40.24, 'test/accuracy_QuestionType_ten': 44.03, 'test/accuracy_QuestionType_two': 52.09, 'test/accuracy_QuestionType_six': 47.8, 'test/accuracy_AnswerType_other': 44.98, 'test/epoch': 0}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5150614347998415, 'test/exact_match_at_2': 0.6454617518826794, 'test/exact_match_at_3': 0.6866825208085613, 'test/exact_match_at_4': 0.7013476020610384, 'test/exact_match_at_5': 0.7055093143083631, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37138327388030123, 'test/gold_recall_at_5': 0.6632976615140705, 'test/successful_hit': 0.3428458184700753, 'test/successful_no_hit': 0.13047958779231075, 'test/failed_hit': 0.3498216409036861, 'test/failed_no_hit': 0.17685295283392785, 'test/selected_successful_hit': 0.4114149821640904, 'test/selected_successful_no_hit': 0.10364645263575109, 'test/selected_failed_hit': 0.37871581450653985, 'test/selected_failed_no_hit': 0.10622275069361871, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 47.2, 'test/accuracy_QuestionType_one': 42.02, 'test/accuracy_QuestionType_eight': 44.7, 'test/accuracy_QuestionType_other': 49.34, 'test/accuracy_QuestionType_seven': 47.15, 'test/accuracy_QuestionType_four': 53.51, 'test/accuracy_QuestionType_five': 49.47, 'test/accuracy_QuestionType_three': 46.59, 'test/accuracy_QuestionType_nine': 32.62, 'test/accuracy_QuestionType_ten': 45.58, 'test/accuracy_QuestionType_two': 54.07, 'test/accuracy_QuestionType_six': 48.37, 'test/accuracy_AnswerType_other': 47.2, 'test/epoch': 0}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5330955212049148, 'test/exact_match_at_2': 0.6577487118509711, 'test/exact_match_at_3': 0.6993658343242172, 'test/exact_match_at_4': 0.7126436781609196, 'test/exact_match_at_5': 0.715021799445105, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37138327388030123, 'test/gold_recall_at_5': 0.6632976615140705, 'test/successful_hit': 0.35204122076892586, 'test/successful_no_hit': 0.13975426080063416, 'test/failed_hit': 0.3399920729290527, 'test/failed_no_hit': 0.16821244550138723, 'test/selected_successful_hit': 0.4231074118113357, 'test/selected_successful_no_hit': 0.10998810939357907, 'test/selected_failed_hit': 0.3701942132382085, 'test/selected_failed_no_hit': 0.09671026555687673, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 48.99, 'test/accuracy_QuestionType_one': 44.48, 'test/accuracy_QuestionType_eight': 48.05, 'test/accuracy_QuestionType_other': 48.89, 'test/accuracy_QuestionType_seven': 49.91, 'test/accuracy_QuestionType_four': 55.38, 'test/accuracy_QuestionType_five': 50.96, 'test/accuracy_QuestionType_three': 46.82, 'test/accuracy_QuestionType_nine': 37.86, 'test/accuracy_QuestionType_ten': 46.82, 'test/accuracy_QuestionType_two': 54.07, 'test/accuracy_QuestionType_six': 50.5, 'test/accuracy_AnswerType_other': 48.99, 'test/epoch': 0}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5386444708680143, 'test/exact_match_at_2': 0.6668648434403488, 'test/exact_match_at_3': 0.7080856123662307, 'test/exact_match_at_4': 0.7237415774871185, 'test/exact_match_at_5': 0.7275069361870788, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37138327388030123, 'test/gold_recall_at_5': 0.6632976615140705, 'test/successful_hit': 0.360166468489893, 'test/successful_no_hit': 0.13515655965120887, 'test/failed_hit': 0.35648038049940545, 'test/failed_no_hit': 0.14819659135949267, 'test/selected_successful_hit': 0.4351961950059453, 'test/selected_successful_no_hit': 0.10344827586206896, 'test/selected_failed_hit': 0.3755449861276258, 'test/selected_failed_no_hit': 0.08581054300435989, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 49.68, 'test/accuracy_QuestionType_one': 46.22, 'test/accuracy_QuestionType_eight': 46.79, 'test/accuracy_QuestionType_other': 50.69, 'test/accuracy_QuestionType_seven': 50.14, 'test/accuracy_QuestionType_four': 56.75, 'test/accuracy_QuestionType_five': 51.3, 'test/accuracy_QuestionType_three': 48.55, 'test/accuracy_QuestionType_nine': 37.86, 'test/accuracy_QuestionType_ten': 48.22, 'test/accuracy_QuestionType_two': 53.49, 'test/accuracy_QuestionType_six': 50.78, 'test/accuracy_AnswerType_other': 49.68, 'test/epoch': 0}[0m
/home/xl544/.conda/envs/RAVQA/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
Epoch 0, global step 282: 'test/accuracy_overall' reached 49.68000 (best 49.68000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-flanT5-CLIP-32.21842240/train/saved_model/model_0.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5505350772889417, 'test/exact_match_at_2': 0.6724137931034483, 'test/exact_match_at_3': 0.7086801426872771, 'test/exact_match_at_4': 0.72294887039239, 'test/exact_match_at_5': 0.7282996432818074, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37138327388030123, 'test/gold_recall_at_5': 0.6632976615140705, 'test/successful_hit': 0.3589377724930638, 'test/successful_no_hit': 0.14819659135949267, 'test/failed_hit': 0.3275069361870789, 'test/failed_no_hit': 0.16535869996036465, 'test/selected_successful_hit': 0.4353943717796274, 'test/selected_successful_no_hit': 0.11514070550931431, 'test/selected_failed_hit': 0.3507728894173603, 'test/selected_failed_no_hit': 0.09869203329369798, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 50.61, 'test/accuracy_QuestionType_one': 47.13, 'test/accuracy_QuestionType_eight': 48.35, 'test/accuracy_QuestionType_other': 52.18, 'test/accuracy_QuestionType_seven': 51.78, 'test/accuracy_QuestionType_four': 57.21, 'test/accuracy_QuestionType_five': 51.88, 'test/accuracy_QuestionType_three': 49.11, 'test/accuracy_QuestionType_nine': 40.24, 'test/accuracy_QuestionType_ten': 49.15, 'test/accuracy_QuestionType_two': 50.7, 'test/accuracy_QuestionType_six': 52.06, 'test/accuracy_AnswerType_other': 50.61, 'test/epoch': 1}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5616329766151407, 'test/exact_match_at_2': 0.6825208085612366, 'test/exact_match_at_3': 0.7183908045977011, 'test/exact_match_at_4': 0.7316686484344035, 'test/exact_match_at_5': 0.7348394768133175, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37138327388030123, 'test/gold_recall_at_5': 0.6632976615140705, 'test/successful_hit': 0.3610384462940943, 'test/successful_no_hit': 0.1573523583036068, 'test/failed_hit': 0.31391200951248516, 'test/failed_no_hit': 0.16769718588981372, 'test/selected_successful_hit': 0.4383670233848593, 'test/selected_successful_no_hit': 0.12326595323028142, 'test/selected_failed_hit': 0.3388822829964328, 'test/selected_failed_no_hit': 0.09948474038842647, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 51.48, 'test/accuracy_QuestionType_one': 48.29, 'test/accuracy_QuestionType_eight': 49.79, 'test/accuracy_QuestionType_other': 53.44, 'test/accuracy_QuestionType_seven': 52.66, 'test/accuracy_QuestionType_four': 56.72, 'test/accuracy_QuestionType_five': 52.97, 'test/accuracy_QuestionType_three': 50.09, 'test/accuracy_QuestionType_nine': 43.33, 'test/accuracy_QuestionType_ten': 50.85, 'test/accuracy_QuestionType_two': 50.7, 'test/accuracy_QuestionType_six': 49.08, 'test/accuracy_AnswerType_other': 51.48, 'test/epoch': 1}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5614347998414586, 'test/exact_match_at_2': 0.6829171621086009, 'test/exact_match_at_3': 0.7179944510503369, 'test/exact_match_at_4': 0.7312722948870393, 'test/exact_match_at_5': 0.7338485929449069, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37138327388030123, 'test/gold_recall_at_5': 0.6632976615140705, 'test/successful_hit': 0.3654379706698375, 'test/successful_no_hit': 0.15136741973840667, 'test/failed_hit': 0.3214427269124059, 'test/failed_no_hit': 0.16175188267934998, 'test/selected_successful_hit': 0.4417360285374554, 'test/selected_successful_no_hit': 0.11969877130400317, 'test/selected_failed_hit': 0.3388822829964328, 'test/selected_failed_no_hit': 0.0996829171621086, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 51.5, 'test/accuracy_QuestionType_one': 48.34, 'test/accuracy_QuestionType_eight': 49.23, 'test/accuracy_QuestionType_other': 53.31, 'test/accuracy_QuestionType_seven': 51.59, 'test/accuracy_QuestionType_four': 58.1, 'test/accuracy_QuestionType_five': 52.42, 'test/accuracy_QuestionType_three': 50.51, 'test/accuracy_QuestionType_nine': 39.76, 'test/accuracy_QuestionType_ten': 52.71, 'test/accuracy_QuestionType_two': 53.37, 'test/accuracy_QuestionType_six': 50.07, 'test/accuracy_AnswerType_other': 51.5, 'test/epoch': 1}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5669837495045581, 'test/exact_match_at_2': 0.692627824019025, 'test/exact_match_at_3': 0.7279032897344431, 'test/exact_match_at_4': 0.7409829567974633, 'test/exact_match_at_5': 0.7433610780816489, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37138327388030123, 'test/gold_recall_at_5': 0.6632976615140705, 'test/successful_hit': 0.3682917162108601, 'test/successful_no_hit': 0.1573523583036068, 'test/failed_hit': 0.3134760206103845, 'test/failed_no_hit': 0.16087990487514864, 'test/selected_successful_hit': 0.4425287356321839, 'test/selected_successful_no_hit': 0.12445501387237416, 'test/selected_failed_hit': 0.3345223939754261, 'test/selected_failed_no_hit': 0.09849385652001585, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 52.18, 'test/accuracy_QuestionType_one': 49.42, 'test/accuracy_QuestionType_eight': 50.67, 'test/accuracy_QuestionType_other': 54.73, 'test/accuracy_QuestionType_seven': 52.43, 'test/accuracy_QuestionType_four': 56.51, 'test/accuracy_QuestionType_five': 54.21, 'test/accuracy_QuestionType_three': 50.33, 'test/accuracy_QuestionType_nine': 40.24, 'test/accuracy_QuestionType_ten': 50.54, 'test/accuracy_QuestionType_two': 53.14, 'test/accuracy_QuestionType_six': 49.36, 'test/accuracy_AnswerType_other': 52.18, 'test/epoch': 1}[0m
Epoch 1, global step 564: 'test/accuracy_overall' reached 52.18000 (best 52.18000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-flanT5-CLIP-32.21842240/train/saved_model/model_1.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.573325406262386, 'test/exact_match_at_2': 0.6961950059453033, 'test/exact_match_at_3': 0.7255251684502576, 'test/exact_match_at_4': 0.7338485929449069, 'test/exact_match_at_5': 0.735632183908046, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37138327388030123, 'test/gold_recall_at_5': 0.6632976615140705, 'test/successful_hit': 0.3638525564803805, 'test/successful_no_hit': 0.17586206896551723, 'test/failed_hit': 0.27253269916765754, 'test/failed_no_hit': 0.18775267538644472, 'test/selected_successful_hit': 0.4326198969480777, 'test/selected_successful_no_hit': 0.14070550931430836, 'test/selected_failed_hit': 0.30598493856520015, 'test/selected_failed_no_hit': 0.1206896551724138, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 52.62, 'test/accuracy_QuestionType_one': 48.66, 'test/accuracy_QuestionType_eight': 51.02, 'test/accuracy_QuestionType_other': 55.06, 'test/accuracy_QuestionType_seven': 51.31, 'test/accuracy_QuestionType_four': 58.1, 'test/accuracy_QuestionType_five': 55.5, 'test/accuracy_QuestionType_three': 51.96, 'test/accuracy_QuestionType_nine': 39.76, 'test/accuracy_QuestionType_ten': 53.49, 'test/accuracy_QuestionType_two': 53.14, 'test/accuracy_QuestionType_six': 48.37, 'test/accuracy_AnswerType_other': 52.62, 'test/epoch': 2}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5739199365834324, 'test/exact_match_at_2': 0.6991676575505351, 'test/exact_match_at_3': 0.7322631787554499, 'test/exact_match_at_4': 0.7409829567974633, 'test/exact_match_at_5': 0.7421720174395561, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37138327388030123, 'test/gold_recall_at_5': 0.6632976615140705, 'test/successful_hit': 0.3653983353151011, 'test/successful_no_hit': 0.17459373761395164, 'test/failed_hit': 0.27808164883075703, 'test/failed_no_hit': 0.18192627824019025, 'test/selected_successful_hit': 0.4357907253269917, 'test/selected_successful_no_hit': 0.13812921125644073, 'test/selected_failed_hit': 0.31093935790725324, 'test/selected_failed_no_hit': 0.11514070550931431, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 52.7, 'test/accuracy_QuestionType_one': 48.59, 'test/accuracy_QuestionType_eight': 50.77, 'test/accuracy_QuestionType_other': 55.12, 'test/accuracy_QuestionType_seven': 54.49, 'test/accuracy_QuestionType_four': 57.88, 'test/accuracy_QuestionType_five': 54.46, 'test/accuracy_QuestionType_three': 53.18, 'test/accuracy_QuestionType_nine': 38.57, 'test/accuracy_QuestionType_ten': 50.54, 'test/accuracy_QuestionType_two': 53.26, 'test/accuracy_QuestionType_six': 49.5, 'test/accuracy_AnswerType_other': 52.7, 'test/epoch': 2}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5745144669044788, 'test/exact_match_at_2': 0.6950059453032105, 'test/exact_match_at_3': 0.7294887039239001, 'test/exact_match_at_4': 0.7399920729290527, 'test/exact_match_at_5': 0.7419738406658739, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37138327388030123, 'test/gold_recall_at_5': 0.6632976615140705, 'test/successful_hit': 0.3670630202140309, 'test/successful_no_hit': 0.16860879904875148, 'test/failed_hit': 0.2882282996432818, 'test/failed_no_hit': 0.1760998810939358, 'test/selected_successful_hit': 0.44133967499009114, 'test/selected_successful_no_hit': 0.13317479191438764, 'test/selected_failed_hit': 0.31747919143876335, 'test/selected_failed_no_hit': 0.10800634165675783, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 52.75, 'test/accuracy_QuestionType_one': 48.61, 'test/accuracy_QuestionType_eight': 50.53, 'test/accuracy_QuestionType_other': 54.96, 'test/accuracy_QuestionType_seven': 53.55, 'test/accuracy_QuestionType_four': 58.84, 'test/accuracy_QuestionType_five': 54.98, 'test/accuracy_QuestionType_three': 52.15, 'test/accuracy_QuestionType_nine': 40.48, 'test/accuracy_QuestionType_ten': 51.78, 'test/accuracy_QuestionType_two': 54.65, 'test/accuracy_QuestionType_six': 48.51, 'test/accuracy_AnswerType_other': 52.75, 'test/epoch': 2}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5804597701149425, 'test/exact_match_at_2': 0.6977804201347602, 'test/exact_match_at_3': 0.7352358303606817, 'test/exact_match_at_4': 0.7435592548553309, 'test/exact_match_at_5': 0.7449464922711059, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37138327388030123, 'test/gold_recall_at_5': 0.6632976615140705, 'test/successful_hit': 0.3655965120887832, 'test/successful_no_hit': 0.17534680935394373, 'test/failed_hit': 0.2751486325802616, 'test/failed_no_hit': 0.1839080459770115, 'test/selected_successful_hit': 0.43896155370590567, 'test/selected_successful_no_hit': 0.14149821640903687, 'test/selected_failed_hit': 0.30578676179151804, 'test/selected_failed_no_hit': 0.11375346809353944, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.35, 'test/accuracy_QuestionType_one': 49.91, 'test/accuracy_QuestionType_eight': 50.95, 'test/accuracy_QuestionType_other': 55.67, 'test/accuracy_QuestionType_seven': 53.64, 'test/accuracy_QuestionType_four': 60.49, 'test/accuracy_QuestionType_five': 54.68, 'test/accuracy_QuestionType_three': 52.24, 'test/accuracy_QuestionType_nine': 41.67, 'test/accuracy_QuestionType_ten': 52.71, 'test/accuracy_QuestionType_two': 53.95, 'test/accuracy_QuestionType_six': 50.64, 'test/accuracy_AnswerType_other': 53.35, 'test/epoch': 2}[0m
Epoch 2, global step 846: 'test/accuracy_overall' reached 53.35000 (best 53.35000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-flanT5-CLIP-32.21842240/train/saved_model/model_2.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5788743559254855, 'test/exact_match_at_2': 0.7027348394768134, 'test/exact_match_at_3': 0.7364248910027745, 'test/exact_match_at_4': 0.7493063812921126, 'test/exact_match_at_5': 0.751684502576298, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37138327388030123, 'test/gold_recall_at_5': 0.6632976615140705, 'test/successful_hit': 0.36821244550138726, 'test/successful_no_hit': 0.17411811335711455, 'test/failed_hit': 0.27907253269916765, 'test/failed_no_hit': 0.17859690844233056, 'test/selected_successful_hit': 0.4397542608006342, 'test/selected_successful_no_hit': 0.13912009512485138, 'test/selected_failed_hit': 0.30598493856520015, 'test/selected_failed_no_hit': 0.11514070550931431, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.07, 'test/accuracy_QuestionType_one': 48.83, 'test/accuracy_QuestionType_eight': 50.84, 'test/accuracy_QuestionType_other': 55.77, 'test/accuracy_QuestionType_seven': 52.38, 'test/accuracy_QuestionType_four': 59.51, 'test/accuracy_QuestionType_five': 55.13, 'test/accuracy_QuestionType_three': 53.83, 'test/accuracy_QuestionType_nine': 44.76, 'test/accuracy_QuestionType_ten': 48.99, 'test/accuracy_QuestionType_two': 55.7, 'test/accuracy_QuestionType_six': 46.81, 'test/accuracy_AnswerType_other': 53.07, 'test/epoch': 3}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5850178359096314, 'test/exact_match_at_2': 0.7013476020610384, 'test/exact_match_at_3': 0.7354340071343638, 'test/exact_match_at_4': 0.7463337296868807, 'test/exact_match_at_5': 0.7479191438763377, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37138327388030123, 'test/gold_recall_at_5': 0.6632976615140705, 'test/successful_hit': 0.36948077685295283, 'test/successful_no_hit': 0.17459373761395164, 'test/failed_hit': 0.27491082045184306, 'test/failed_no_hit': 0.18101466508125247, 'test/selected_successful_hit': 0.4417360285374554, 'test/selected_successful_no_hit': 0.14328180737217597, 'test/selected_failed_hit': 0.30261593341260407, 'test/selected_failed_no_hit': 0.11236623067776456, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.66, 'test/accuracy_QuestionType_one': 50.58, 'test/accuracy_QuestionType_eight': 51.86, 'test/accuracy_QuestionType_other': 55.61, 'test/accuracy_QuestionType_seven': 52.62, 'test/accuracy_QuestionType_four': 60.11, 'test/accuracy_QuestionType_five': 55.73, 'test/accuracy_QuestionType_three': 53.46, 'test/accuracy_QuestionType_nine': 42.86, 'test/accuracy_QuestionType_ten': 51.78, 'test/accuracy_QuestionType_two': 55.93, 'test/accuracy_QuestionType_six': 45.25, 'test/accuracy_AnswerType_other': 53.66, 'test/epoch': 3}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5814506539833532, 'test/exact_match_at_2': 0.7027348394768134, 'test/exact_match_at_3': 0.7360285374554102, 'test/exact_match_at_4': 0.7459373761395165, 'test/exact_match_at_5': 0.748315497423702, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37138327388030123, 'test/gold_recall_at_5': 0.6632976615140705, 'test/successful_hit': 0.36963931827189855, 'test/successful_no_hit': 0.17348394768133174, 'test/failed_hit': 0.28656361474435194, 'test/failed_no_hit': 0.17031311930241777, 'test/selected_successful_hit': 0.441141498216409, 'test/selected_successful_no_hit': 0.1403091557669441, 'test/selected_failed_hit': 0.30598493856520015, 'test/selected_failed_no_hit': 0.11256440745144669, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.31, 'test/accuracy_QuestionType_one': 48.17, 'test/accuracy_QuestionType_eight': 51.16, 'test/accuracy_QuestionType_other': 56.38, 'test/accuracy_QuestionType_seven': 52.1, 'test/accuracy_QuestionType_four': 59.65, 'test/accuracy_QuestionType_five': 55.5, 'test/accuracy_QuestionType_three': 53.55, 'test/accuracy_QuestionType_nine': 45.24, 'test/accuracy_QuestionType_ten': 50.7, 'test/accuracy_QuestionType_two': 57.56, 'test/accuracy_QuestionType_six': 49.5, 'test/accuracy_AnswerType_other': 53.31, 'test/epoch': 3}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5824415378517638, 'test/exact_match_at_2': 0.7033293697978596, 'test/exact_match_at_3': 0.739001189060642, 'test/exact_match_at_4': 0.7481173206500198, 'test/exact_match_at_5': 0.7495045580657946, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37138327388030123, 'test/gold_recall_at_5': 0.6632976615140705, 'test/successful_hit': 0.37185889813713835, 'test/successful_no_hit': 0.17602061038446293, 'test/failed_hit': 0.26932223543400713, 'test/failed_no_hit': 0.1827982560443916, 'test/selected_successful_hit': 0.4407451446690448, 'test/selected_successful_no_hit': 0.14169639318271898, 'test/selected_failed_hit': 0.30023781212841855, 'test/selected_failed_no_hit': 0.11732065001981767, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.44, 'test/accuracy_QuestionType_one': 49.72, 'test/accuracy_QuestionType_eight': 50.77, 'test/accuracy_QuestionType_other': 56.93, 'test/accuracy_QuestionType_seven': 52.71, 'test/accuracy_QuestionType_four': 59.61, 'test/accuracy_QuestionType_five': 55.73, 'test/accuracy_QuestionType_three': 51.92, 'test/accuracy_QuestionType_nine': 44.05, 'test/accuracy_QuestionType_ten': 51.63, 'test/accuracy_QuestionType_two': 56.05, 'test/accuracy_QuestionType_six': 48.94, 'test/accuracy_AnswerType_other': 53.44, 'test/epoch': 3}[0m
Epoch 3, global step 1128: 'test/accuracy_overall' reached 53.44000 (best 53.44000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-flanT5-CLIP-32.21842240/train/saved_model/model_3.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5830360681728102, 'test/exact_match_at_2': 0.6973840665873959, 'test/exact_match_at_3': 0.730875941339675, 'test/exact_match_at_4': 0.739001189060642, 'test/exact_match_at_5': 0.7409829567974633, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37138327388030123, 'test/gold_recall_at_5': 0.6632976615140705, 'test/successful_hit': 0.36555687673404674, 'test/successful_no_hit': 0.18224336107808164, 'test/failed_hit': 0.26044391597304795, 'test/failed_no_hit': 0.1917558462148236, 'test/selected_successful_hit': 0.43638525564803804, 'test/selected_successful_no_hit': 0.1466508125247721, 'test/selected_failed_hit': 0.2867617915180341, 'test/selected_failed_no_hit': 0.13020214030915578, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.58, 'test/accuracy_QuestionType_one': 48.41, 'test/accuracy_QuestionType_eight': 51.84, 'test/accuracy_QuestionType_other': 57.38, 'test/accuracy_QuestionType_seven': 52.06, 'test/accuracy_QuestionType_four': 60.14, 'test/accuracy_QuestionType_five': 55.23, 'test/accuracy_QuestionType_three': 53.69, 'test/accuracy_QuestionType_nine': 45.24, 'test/accuracy_QuestionType_ten': 53.18, 'test/accuracy_QuestionType_two': 56.51, 'test/accuracy_QuestionType_six': 47.52, 'test/accuracy_AnswerType_other': 53.58, 'test/epoch': 4}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5840269520412208, 'test/exact_match_at_2': 0.702338485929449, 'test/exact_match_at_3': 0.7348394768133175, 'test/exact_match_at_4': 0.7413793103448276, 'test/exact_match_at_5': 0.7435592548553309, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37138327388030123, 'test/gold_recall_at_5': 0.6632976615140705, 'test/successful_hit': 0.3661910424098296, 'test/successful_no_hit': 0.18418549346016647, 'test/failed_hit': 0.25707491082045186, 'test/failed_no_hit': 0.19254855330955212, 'test/selected_successful_hit': 0.4349980182322632, 'test/selected_successful_no_hit': 0.1490289338089576, 'test/selected_failed_hit': 0.28894173602853745, 'test/selected_failed_no_hit': 0.12703131193024178, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.69, 'test/accuracy_QuestionType_one': 48.86, 'test/accuracy_QuestionType_eight': 51.63, 'test/accuracy_QuestionType_other': 56.77, 'test/accuracy_QuestionType_seven': 51.92, 'test/accuracy_QuestionType_four': 61.27, 'test/accuracy_QuestionType_five': 55.16, 'test/accuracy_QuestionType_three': 53.93, 'test/accuracy_QuestionType_nine': 44.05, 'test/accuracy_QuestionType_ten': 53.18, 'test/accuracy_QuestionType_two': 56.86, 'test/accuracy_QuestionType_six': 48.65, 'test/accuracy_AnswerType_other': 53.69, 'test/epoch': 4}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5899722552516845, 'test/exact_match_at_2': 0.7015457788347206, 'test/exact_match_at_3': 0.7336504161712247, 'test/exact_match_at_4': 0.7415774871185097, 'test/exact_match_at_5': 0.7425683709869203, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37138327388030123, 'test/gold_recall_at_5': 0.6632976615140705, 'test/successful_hit': 0.3651208878319461, 'test/successful_no_hit': 0.1880697582243361, 'test/failed_hit': 0.2507728894173603, 'test/failed_no_hit': 0.1960364645263575, 'test/selected_successful_hit': 0.43658343242172015, 'test/selected_successful_no_hit': 0.15338882282996433, 'test/selected_failed_hit': 0.2800237812128419, 'test/selected_failed_no_hit': 0.13000396353547364, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 54.1, 'test/accuracy_QuestionType_one': 48.93, 'test/accuracy_QuestionType_eight': 52.05, 'test/accuracy_QuestionType_other': 57.19, 'test/accuracy_QuestionType_seven': 51.21, 'test/accuracy_QuestionType_four': 62.57, 'test/accuracy_QuestionType_five': 56.15, 'test/accuracy_QuestionType_three': 54.63, 'test/accuracy_QuestionType_nine': 42.86, 'test/accuracy_QuestionType_ten': 51.63, 'test/accuracy_QuestionType_two': 57.79, 'test/accuracy_QuestionType_six': 48.65, 'test/accuracy_AnswerType_other': 54.1, 'test/epoch': 4}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.582837891399128, 'test/exact_match_at_2': 0.7043202536662703, 'test/exact_match_at_3': 0.7358303606817281, 'test/exact_match_at_4': 0.7449464922711059, 'test/exact_match_at_5': 0.7463337296868807, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37138327388030123, 'test/gold_recall_at_5': 0.6632976615140705, 'test/successful_hit': 0.3700356718192628, 'test/successful_no_hit': 0.1770907649623464, 'test/failed_hit': 0.2728894173602854, 'test/failed_no_hit': 0.17998414585810543, 'test/selected_successful_hit': 0.4401506143479984, 'test/selected_successful_no_hit': 0.1426872770511296, 'test/selected_failed_hit': 0.30103051922314705, 'test/selected_failed_no_hit': 0.11613158937772493, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.53, 'test/accuracy_QuestionType_one': 49.1, 'test/accuracy_QuestionType_eight': 51.19, 'test/accuracy_QuestionType_other': 56.7, 'test/accuracy_QuestionType_seven': 52.71, 'test/accuracy_QuestionType_four': 59.61, 'test/accuracy_QuestionType_five': 56.1, 'test/accuracy_QuestionType_three': 53.93, 'test/accuracy_QuestionType_nine': 39.76, 'test/accuracy_QuestionType_ten': 50.23, 'test/accuracy_QuestionType_two': 57.09, 'test/accuracy_QuestionType_six': 48.37, 'test/accuracy_AnswerType_other': 53.53, 'test/epoch': 4}[0m
Epoch 4, global step 1410: 'test/accuracy_overall' reached 53.53000 (best 53.53000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-flanT5-CLIP-32.21842240/train/saved_model/model_4.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5846214823622672, 'test/exact_match_at_2': 0.703725723345224, 'test/exact_match_at_3': 0.7322631787554499, 'test/exact_match_at_4': 0.7421720174395561, 'test/exact_match_at_5': 0.7429647245342846, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37138327388030123, 'test/gold_recall_at_5': 0.6632976615140705, 'test/successful_hit': 0.3650812524772097, 'test/successful_no_hit': 0.18656361474435196, 'test/failed_hit': 0.2539437177962743, 'test/failed_no_hit': 0.19441141498216408, 'test/selected_successful_hit': 0.43242172017439556, 'test/selected_successful_no_hit': 0.15219976218787157, 'test/selected_failed_hit': 0.28378913991280225, 'test/selected_failed_no_hit': 0.13158937772493065, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.74, 'test/accuracy_QuestionType_one': 49.47, 'test/accuracy_QuestionType_eight': 50.95, 'test/accuracy_QuestionType_other': 56.93, 'test/accuracy_QuestionType_seven': 53.27, 'test/accuracy_QuestionType_four': 60.18, 'test/accuracy_QuestionType_five': 56.02, 'test/accuracy_QuestionType_three': 53.74, 'test/accuracy_QuestionType_nine': 42.86, 'test/accuracy_QuestionType_ten': 51.32, 'test/accuracy_QuestionType_two': 56.86, 'test/accuracy_QuestionType_six': 48.65, 'test/accuracy_AnswerType_other': 53.74, 'test/epoch': 5}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5877923107411811, 'test/exact_match_at_2': 0.703923900118906, 'test/exact_match_at_3': 0.735632183908046, 'test/exact_match_at_4': 0.7437574316290131, 'test/exact_match_at_5': 0.7453428458184701, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37138327388030123, 'test/gold_recall_at_5': 0.6632976615140705, 'test/successful_hit': 0.3668648434403488, 'test/successful_no_hit': 0.1827982560443916, 'test/failed_hit': 0.2634165675782798, 'test/failed_no_hit': 0.1869203329369798, 'test/selected_successful_hit': 0.43797066983749505, 'test/selected_successful_no_hit': 0.14982164090368608, 'test/selected_failed_hit': 0.28814902893380895, 'test/selected_failed_no_hit': 0.12405866032500991, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.93, 'test/accuracy_QuestionType_one': 48.88, 'test/accuracy_QuestionType_eight': 52.02, 'test/accuracy_QuestionType_other': 56.77, 'test/accuracy_QuestionType_seven': 53.55, 'test/accuracy_QuestionType_four': 60.56, 'test/accuracy_QuestionType_five': 55.88, 'test/accuracy_QuestionType_three': 55.42, 'test/accuracy_QuestionType_nine': 42.86, 'test/accuracy_QuestionType_ten': 48.37, 'test/accuracy_QuestionType_two': 55.35, 'test/accuracy_QuestionType_six': 50.92, 'test/accuracy_AnswerType_other': 53.93, 'test/epoch': 5}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5848196591359492, 'test/exact_match_at_2': 0.7051129607609988, 'test/exact_match_at_3': 0.7364248910027745, 'test/exact_match_at_4': 0.748513674197384, 'test/exact_match_at_5': 0.749900911613159, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37138327388030123, 'test/gold_recall_at_5': 0.6632976615140705, 'test/successful_hit': 0.36793499801823226, 'test/successful_no_hit': 0.18311533888228299, 'test/failed_hit': 0.264922711058264, 'test/failed_no_hit': 0.18402695204122077, 'test/selected_successful_hit': 0.43559254855330953, 'test/selected_successful_no_hit': 0.1492271105826397, 'test/selected_failed_hit': 0.2919143876337693, 'test/selected_failed_no_hit': 0.12326595323028142, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.68, 'test/accuracy_QuestionType_one': 48.83, 'test/accuracy_QuestionType_eight': 51.77, 'test/accuracy_QuestionType_other': 56.67, 'test/accuracy_QuestionType_seven': 52.48, 'test/accuracy_QuestionType_four': 59.86, 'test/accuracy_QuestionType_five': 55.78, 'test/accuracy_QuestionType_three': 55.37, 'test/accuracy_QuestionType_nine': 44.05, 'test/accuracy_QuestionType_ten': 49.15, 'test/accuracy_QuestionType_two': 55.47, 'test/accuracy_QuestionType_six': 49.5, 'test/accuracy_AnswerType_other': 53.68, 'test/epoch': 5}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5868014268727705, 'test/exact_match_at_2': 0.7019421323820848, 'test/exact_match_at_3': 0.7338485929449069, 'test/exact_match_at_4': 0.7421720174395561, 'test/exact_match_at_5': 0.7439556084026953, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37138327388030123, 'test/gold_recall_at_5': 0.6632976615140705, 'test/successful_hit': 0.3654776060245739, 'test/successful_no_hit': 0.1880697582243361, 'test/failed_hit': 0.24891002774474832, 'test/failed_no_hit': 0.19754260800634166, 'test/selected_successful_hit': 0.4340071343638526, 'test/selected_successful_no_hit': 0.15279429250891796, 'test/selected_failed_hit': 0.2810146650812525, 'test/selected_failed_no_hit': 0.13218390804597702, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.8, 'test/accuracy_QuestionType_one': 49.59, 'test/accuracy_QuestionType_eight': 51.65, 'test/accuracy_QuestionType_other': 56.93, 'test/accuracy_QuestionType_seven': 51.68, 'test/accuracy_QuestionType_four': 59.89, 'test/accuracy_QuestionType_five': 55.98, 'test/accuracy_QuestionType_three': 54.35, 'test/accuracy_QuestionType_nine': 45.24, 'test/accuracy_QuestionType_ten': 51.16, 'test/accuracy_QuestionType_two': 55.23, 'test/accuracy_QuestionType_six': 50.92, 'test/accuracy_AnswerType_other': 53.8, 'test/epoch': 5}[0m
Epoch 5, global step 1692: 'test/accuracy_overall' reached 53.80000 (best 53.80000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-flanT5-CLIP-32.21842240/train/saved_model/model_5.ckpt' as top 1
