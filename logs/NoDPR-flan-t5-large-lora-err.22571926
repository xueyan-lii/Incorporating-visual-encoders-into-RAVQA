/home/xl544/.conda/envs/BLIP2/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
/home/xl544/.conda/envs/BLIP2/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning
  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')
[38;20m[INFO] - __main__ : Initialization done with the config: {'DATA_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data', 'EXPERIMENT_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments', 'TENSORBOARD_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data_TB/tb_logs', 'WANDB': {'entity': 'xl544', 'project': 'RAVQA', 'tags': ['OKVQA', 'T5'], 'name': 'NoDPR-flan-t5-large-lora.22571926'}, 'cache': {'default_folder': '../data/ok-vqa/cache', 'regenerate': {'clip_embeddings': 0, 'ocr_feature_preprocessed': 0, 'qformer_embeddings': 0, 'test_data_preprocessed': 0, 'train_data_preprocessed': 0, 'vinvl_feature_preprocessed': 0}}, 'cuda': 0, 'data_loader': {'additional': {'max_decoder_source_length': 512, 'max_source_length': 512, 'max_target_length': 10}, 'dataset_modules': {'module_dict': {'LoadClipEmbeddings': {'config': {'clip_embeddings': {'train': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_train2014.pkl', 'val': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_val2014.pkl'}, 'qformer_embeddings': {'train': '../data/ok-vqa/pre-extracted_features/blip2_head_embeddings/coco_hgface_qformer_train2014.pkl', 'val': '../data/ok-vqa/pre-extracted_features/blip2_head_embeddings/coco_hgface_qformer_val2014.pkl'}}, 'option': 'default', 'type': 'EmbeddingInput'}, 'LoadGoogleOCRFeatures': {'config': {'combine_with_vinvl': True, 'test': '../data/ok-vqa/pre-extracted_features/OCR/valid', 'train': '../data/ok-vqa/pre-extracted_features/OCR/train'}, 'option': 'default', 'type': 'LoadGoogleOCRFeatures'}, 'LoadGoogleSearchAnnotations': {'config': {'annotations_path': {'test': '../data/ok-vqa/pre-extracted_features/passages/retriever_test.json', 'train': '../data/ok-vqa/pre-extracted_features/passages/retriever_train.json', 'valid': '../data/ok-vqa/pre-extracted_features/passages/retriever_testdev.json'}}, 'option': 'default', 'type': 'LoadGoogleSearchAnnotations'}, 'LoadGoogleSearchPassageData': {'config': {'passage_data_path': {'full': '../data/ok-vqa/pre-extracted_features/passages/okvqa_full_corpus.csv', 'train': '../data/ok-vqa/pre-extracted_features/passages/okvqa_train_corpus.csv'}, 'use_full_split': True}, 'option': 'default', 'type': 'LoadGoogleSearchPassageData'}, 'LoadOKVQAData': {'config': {'image_data_path': {'test': '../data/ok-vqa/val2014', 'train': '../data/ok-vqa/train2014'}, 'vqa_data_path': {'annotation_files': {'test': '../data/ok-vqa/mscoco_val2014_annotations.json', 'train': '../data/ok-vqa/mscoco_train2014_annotations.json'}, 'question_files': {'test': '../data/ok-vqa/OpenEnded_mscoco_val2014_questions.json', 'train': '../data/ok-vqa/OpenEnded_mscoco_train2014_questions.json'}}}, 'option': 'default', 'type': 'LoadOKVQAData'}, 'LoadOscarCaptionFeatures': {'config': {'test': '../data/ok-vqa/pre-extracted_features/captions/test_predictions.json', 'train': '../data/ok-vqa/pre-extracted_features/captions/train_predictions.json', 'valid': '../data/ok-vqa/pre-extracted_features/captions/valid_predictions.json'}, 'option': 'default', 'type': 'LoadOscarCaptionFeatures'}, 'LoadPretrainedDPROutputForGoogleSearchPassage': {'config': {'pretrained_dpr_outputs': {'test': '../Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/test/test_evaluation/test_predictions.json', 'train': '../Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/test/test_evaluation/train_predictions.json'}}, 'option': 'none', 'type': 'LoadPretrainedDPROutputForGoogleSearchPassage'}, 'LoadVinVLFeatures': {'config': {'test': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_testset_full/inference/vinvl_vg_x152c4/predictions.tsv', 'train': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_trainset_full/inference/vinvl_vg_x152c4/predictions.tsv'}, 'option': 'default', 'type': 'LoadVinVLFeatures'}}, 'module_list': ['LoadVinVLFeatures', 'LoadGoogleOCRFeatures', 'LoadOscarCaptionFeatures', 'LoadOKVQAData', 'LoadClipEmbeddings']}, 'dataset_type': 'OKVQADataset', 'dummy_dataloader': 0, 'type': 'DataLoaderOKVQA'}, 'experiment_name': 'NoDPR-flan-t5-large-lora.22571926', 'gpu_device': 0, 'ignore_pretrained_weights': [], 'metrics': [{'name': 'compute_okvqa_scores'}], 'model_config': {'ConfigClass': 'T5Config', 'GeneratorModelClass': 'T5ForConditionalGeneration', 'LoadPretrainMLPs': 0, 'ModelClass': 'PrefixModelLora', 'ModelVersion': 'google/flan-t5-large', 'PretrainedMLPPath': '', 'SPECIAL_TOKENS': {'additional_special_tokens': ['<BOQ>', '<EOQ>'], 'bos_token': '<PAD>', 'pad_token': '<PAD>'}, 'TokenizerClass': 'T5Tokenizer', 'TokenizerModelVersion': 'google/flan-t5-large', 'UsePrefixEmb': 0, 'UseQformerEmb': 0, 'base_model': 'T5', 'decoder_input_modules': {'module_list': [], 'postprocess_module_list': []}, 'input_modules': {'module_list': [{'option': 'default', 'separation_tokens': {'end': '<EOQ>', 'start': '<BOQ>'}, 'type': 'QuestionInput'}, {'option': 'default', 'type': 'EmbeddingInput'}], 'postprocess_module_list': [{'option': 'default', 'type': 'PostProcessInputTokenization'}, {'option': 'default', 'type': 'PostProcessClipEmbeddings'}]}, 'loss_ratio': {'additional_loss': 0, 'nll_loss': 1, 'rag_loss': 0}, 'modules': [], 'output_modules': {'module_list': [{'option': 'default', 'type': 'GenerationOutput'}], 'postprocess_module_list': [{'option': 'default', 'type': 'PostProcessOutputTokenization'}]}, 'pretrained': 1, 'rag_modules': {'module_list': []}, 'LoadPretrainedMLP': 0}, 'platform_type': 'pytorch', 'seed': 2021, 'test': {'additional': {'multiprocessing': 4}, 'batch_size': 32, 'evaluation_name': 'test_evaluation', 'load_best_model': 0, 'load_epoch': -1, 'load_model_path': '', 'num_evaluation': 0}, 'train': {'MLP_lr': 0.0001, 'adam_epsilon': 1e-08, 'additional': {'gradient_accumulation_steps': 16, 'gradient_clipping': 0, 'plugins': [], 'save_top_k': 1, 'save_top_k_metric': 'test/accuracy_overall', 'save_top_k_mode': 'max', 'warmup_steps': 0}, 'batch_size': 2, 'epochs': 10, 'load_best_model': 0, 'load_epoch': -1, 'load_model_path': '', 'lr': 6e-05, 'retriever_lr': 1e-05, 'save_interval': 1, 'scheduler': 'linear', 'type': 'T5ExecutorWithPrefix'}, 'valid': {'additional': {}, 'batch_size': 32, 'break_interval': 3000, 'step_size': 0.5}, 'reset': False, 'mode': 'train', 'log_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/NoDPR-flan-t5-large-lora.22571926/train', 'experiment_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/NoDPR-flan-t5-large-lora.22571926', 'saved_model_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/NoDPR-flan-t5-large-lora.22571926/train/saved_model', 'imgs_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/NoDPR-flan-t5-large-lora.22571926/train/imgs', 'tensorboard_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data_TB/tb_logs/NoDPR-flan-t5-large-lora.22571926', 'args': {'config': '../configs/okvqa/T5_NoDPR_prefix_only_lora.jsonnet', 'DATA_FOLDER': '', 'EXPERIMENT_FOLDER': '', 'mode': 'train', 'reset': False, 'experiment_name': 'NoDPR-flan-t5-large-lora.22571926', 'tags': [], 'modules': [], 'log_prediction_tables': False, 'test_batch_size': -1, 'test_evaluation_name': '', 'logger': True, 'checkpoint_callback': None, 'enable_checkpointing': True, 'default_root_dir': None, 'gradient_clip_val': None, 'gradient_clip_algorithm': None, 'process_position': 0, 'num_nodes': 1, 'num_processes': None, 'devices': '1', 'gpus': None, 'auto_select_gpus': False, 'tpu_cores': None, 'ipus': None, 'log_gpu_memory': None, 'progress_bar_refresh_rate': None, 'enable_progress_bar': True, 'overfit_batches': 0.0, 'track_grad_norm': -1, 'check_val_every_n_epoch': 1, 'fast_dev_run': False, 'accumulate_grad_batches': None, 'max_epochs': None, 'min_epochs': None, 'max_steps': -1, 'min_steps': None, 'max_time': None, 'limit_train_batches': None, 'limit_val_batches': None, 'limit_test_batches': None, 'limit_predict_batches': None, 'val_check_interval': None, 'flush_logs_every_n_steps': None, 'log_every_n_steps': 50, 'accelerator': 'auto', 'strategy': None, 'sync_batchnorm': False, 'precision': 32, 'enable_model_summary': True, 'weights_summary': 'top', 'weights_save_path': None, 'num_sanity_val_steps': 2, 'resume_from_checkpoint': None, 'profiler': None, 'benchmark': None, 'deterministic': None, 'reload_dataloaders_every_n_epochs': 0, 'auto_lr_find': False, 'replace_sampler_ddp': True, 'detect_anomaly': False, 'auto_scale_batch_size': False, 'prepare_data_per_node': None, 'plugins': None, 'amp_backend': 'native', 'amp_level': None, 'move_metrics_to_cpu': False, 'multiple_trainloader_mode': 'max_size_cycle', 'stochastic_weight_avg': False, 'terminate_on_nan': None, 'opts': ['train.epochs=10', 'train.batch_size=2', 'valid.step_size=0.5', 'valid.batch_size=32', 'train.additional.gradient_accumulation_steps=16', 'train.lr=0.00006', 'train.MLP_lr=0.0001', 'train.scheduler=linear', 'model_config.UsePrefixEmb=0', 'model_config.UseQformerEmb=0', 'model_config.LoadPretrainedMLP=0', 'model_config.TokenizerModelVersion=google/flan-t5-large', 'model_config.ModelVersion=google/flan-t5-large']}}[0m
Global seed set to 2021
[38;20m[INFO] - __main__ : All seeds have been set to 2021[0m
[38;20m[INFO] - __main__ : init wandb logger with the following settings: {'entity': 'xl544', 'project': 'RAVQA', 'tags': ['OKVQA', 'T5'], 'name': 'NoDPR-flan-t5-large-lora.22571926'}[0m
wandb: Currently logged in as: xl544. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/src/wandb/run-20230620_143214-prmbb8bl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run NoDPR-flan-t5-large-lora.22571926
wandb: ⭐️ View project at https://wandb.ai/xl544/RAVQA
wandb: 🚀 View run at https://wandb.ai/xl544/RAVQA/runs/prmbb8bl
Multiprocessing is handled by SLURM.
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[38;20m[INFO] - __main__ : arguments passed to trainer: Namespace(DATA_FOLDER='', EXPERIMENT_FOLDER='', accelerator='auto', accumulate_grad_batches=None, amp_backend='native', amp_level=None, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, benchmark=None, check_val_every_n_epoch=1, checkpoint_callback=None, config='../configs/okvqa/T5_NoDPR_prefix_only_lora.jsonnet', default_root_dir=None, detect_anomaly=False, deterministic=None, devices='1', enable_checkpointing=True, enable_model_summary=True, enable_progress_bar=True, experiment_name='NoDPR-flan-t5-large-lora.22571926', fast_dev_run=False, flush_logs_every_n_steps=None, gpus=None, gradient_clip_algorithm=None, gradient_clip_val=None, ipus=None, limit_predict_batches=None, limit_test_batches=None, limit_train_batches=None, limit_val_batches=None, log_every_n_steps=50, log_gpu_memory=None, log_prediction_tables=False, logger=True, max_epochs=None, max_steps=-1, max_time=None, min_epochs=None, min_steps=None, mode='train', modules=[], move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=None, num_sanity_val_steps=2, opts=['train.epochs=10', 'train.batch_size=2', 'valid.step_size=0.5', 'valid.batch_size=32', 'train.additional.gradient_accumulation_steps=16', 'train.lr=0.00006', 'train.MLP_lr=0.0001', 'train.scheduler=linear', 'model_config.UsePrefixEmb=0', 'model_config.UseQformerEmb=0', 'model_config.LoadPretrainedMLP=0', 'model_config.TokenizerModelVersion=google/flan-t5-large', 'model_config.ModelVersion=google/flan-t5-large'], overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=None, process_position=0, profiler=None, progress_bar_refresh_rate=None, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, reset=False, resume_from_checkpoint=None, stochastic_weight_avg=False, strategy=None, sync_batchnorm=False, tags=[], terminate_on_nan=None, test_batch_size=-1, test_evaluation_name='', tpu_cores=None, track_grad_norm=-1, val_check_interval=None, weights_save_path=None, weights_summary='top')[0m
[38;20m[INFO] - __main__ : additional arguments passed to trainer: {'accumulate_grad_batches': 16, 'default_root_dir': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/NoDPR-flan-t5-large-lora.22571926/train/saved_model', 'max_epochs': 10, 'logger': [<pytorch_lightning.loggers.tensorboard.TensorBoardLogger object at 0x1526347afeb0>, <pytorch_lightning.loggers.wandb.WandbLogger object at 0x1526347bf760>, <utils.metrics_log_callback.MetricsHistoryLogger object at 0x1526340cac10>], 'callbacks': [<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x1526347bf1f0>, <pytorch_lightning.callbacks.progress.tqdm_progress.TQDMProgressBar object at 0x15254bdf10a0>, <pytorch_lightning.callbacks.model_summary.ModelSummary object at 0x15254bdf1fa0>, <pytorch_lightning.callbacks.gradient_accumulation_scheduler.GradientAccumulationScheduler object at 0x15254bdf1dc0>], 'plugins': [], 'log_every_n_steps': 10, 'val_check_interval': 0.5}[0m
[33;20m[WARNING] - __main__ : No checkpoint exists from '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/NoDPR-flan-t5-large-lora.22571926/train/saved_model/last.ckpt'. Skipping...[0m
[38;20m[INFO] - __main__ : **First time to train**[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'test': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_testset_full/inference/vinvl_vg_x152c4/predictions.tsv', 'train': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_trainset_full/inference/vinvl_vg_x152c4/predictions.tsv'}, 'option': 'default', 'type': 'LoadVinVLFeatures'}[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/vinvl_feature_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data Statistics] VinVL features 14031[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'combine_with_vinvl': True, 'test': '../data/ok-vqa/pre-extracted_features/OCR/valid', 'train': '../data/ok-vqa/pre-extracted_features/OCR/train'}, 'option': 'default', 'type': 'LoadGoogleOCRFeatures'}[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/ocr_feature_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data Statistics] OCR features 14031, 5462 has annotations.[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : OCR feature detected in VinVL feature dict...skipping..[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'test': '../data/ok-vqa/pre-extracted_features/captions/test_predictions.json', 'train': '../data/ok-vqa/pre-extracted_features/captions/train_predictions.json', 'valid': '../data/ok-vqa/pre-extracted_features/captions/valid_predictions.json'}, 'option': 'default', 'type': 'LoadOscarCaptionFeatures'}[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'image_data_path': {'test': '../data/ok-vqa/val2014', 'train': '../data/ok-vqa/train2014'}, 'vqa_data_path': {'annotation_files': {'test': '../data/ok-vqa/mscoco_val2014_annotations.json', 'train': '../data/ok-vqa/mscoco_train2014_annotations.json'}, 'question_files': {'test': '../data/ok-vqa/OpenEnded_mscoco_val2014_questions.json', 'train': '../data/ok-vqa/OpenEnded_mscoco_train2014_questions.json'}}}, 'option': 'default', 'type': 'LoadOKVQAData'}[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/train_data_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data statistics] split: train  entries: 9009[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/test_data_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data statistics] split: test  entries: 5046[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'clip_embeddings': {'train': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_train2014.pkl', 'val': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_val2014.pkl'}, 'qformer_embeddings': {'train': '../data/ok-vqa/pre-extracted_features/blip2_head_embeddings/coco_hgface_qformer_train2014.pkl', 'val': '../data/ok-vqa/pre-extracted_features/blip2_head_embeddings/coco_hgface_qformer_val2014.pkl'}}, 'option': 'default', 'type': 'EmbeddingInput'}[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/clip_embeddings.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data Statistics] CLIP embeddings 123287[0m
[38;20m[INFO] - data_loader_manager.datasets.okvqa_datasets : initialising OKVQADataset...[0m
[38;20m[INFO] - data_loader_manager.datasets.okvqa_datasets : initialising OKVQADataset...[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data Statistics]: training data loader: 4505;  test data loader: 158[0m
[38;20m[INFO] - torch.distributed.nn.jit.instantiator : Created a temporary directory at /tmp/tmpbar52nne[0m
[38;20m[INFO] - torch.distributed.nn.jit.instantiator : Writing /tmp/tmpbar52nne/_remote_module_non_scriptable.py[0m
[38;20m[INFO] - trainers.base_executor : Initializing T5ExecutorWithPrefix...[0m
Downloading model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]Downloading model.safetensors:   0%|          | 10.5M/3.13G [00:00<00:37, 83.4MB/s]Downloading model.safetensors:   1%|          | 31.5M/3.13G [00:00<00:28, 109MB/s] Downloading model.safetensors:   2%|▏         | 52.4M/3.13G [00:00<00:27, 113MB/s]Downloading model.safetensors:   2%|▏         | 73.4M/3.13G [00:00<00:26, 115MB/s]Downloading model.safetensors:   3%|▎         | 94.4M/3.13G [00:00<00:26, 116MB/s]Downloading model.safetensors:   4%|▎         | 115M/3.13G [00:01<00:25, 116MB/s] Downloading model.safetensors:   4%|▍         | 136M/3.13G [00:01<00:25, 117MB/s]Downloading model.safetensors:   5%|▌         | 157M/3.13G [00:01<00:25, 117MB/s]Downloading model.safetensors:   6%|▌         | 178M/3.13G [00:01<00:25, 117MB/s]Downloading model.safetensors:   6%|▋         | 199M/3.13G [00:01<00:25, 117MB/s]Downloading model.safetensors:   7%|▋         | 220M/3.13G [00:01<00:24, 117MB/s]Downloading model.safetensors:   8%|▊         | 241M/3.13G [00:02<00:24, 117MB/s]Downloading model.safetensors:   8%|▊         | 262M/3.13G [00:02<00:24, 117MB/s]Downloading model.safetensors:   9%|▉         | 283M/3.13G [00:02<00:24, 117MB/s]Downloading model.safetensors:  10%|▉         | 304M/3.13G [00:02<00:24, 117MB/s]Downloading model.safetensors:  10%|█         | 325M/3.13G [00:02<00:23, 117MB/s]Downloading model.safetensors:  11%|█         | 346M/3.13G [00:02<00:23, 117MB/s]Downloading model.safetensors:  12%|█▏        | 367M/3.13G [00:03<00:23, 117MB/s]Downloading model.safetensors:  12%|█▏        | 388M/3.13G [00:03<00:23, 117MB/s]Downloading model.safetensors:  13%|█▎        | 409M/3.13G [00:03<00:23, 117MB/s]Downloading model.safetensors:  14%|█▎        | 430M/3.13G [00:03<00:23, 117MB/s]Downloading model.safetensors:  14%|█▍        | 451M/3.13G [00:03<00:22, 117MB/s]Downloading model.safetensors:  15%|█▌        | 472M/3.13G [00:04<00:22, 117MB/s]Downloading model.safetensors:  16%|█▌        | 493M/3.13G [00:04<00:22, 117MB/s]Downloading model.safetensors:  16%|█▋        | 514M/3.13G [00:04<00:22, 117MB/s]Downloading model.safetensors:  17%|█▋        | 535M/3.13G [00:04<00:22, 117MB/s]Downloading model.safetensors:  18%|█▊        | 556M/3.13G [00:04<00:21, 117MB/s]Downloading model.safetensors:  18%|█▊        | 577M/3.13G [00:04<00:21, 117MB/s]Downloading model.safetensors:  19%|█▉        | 598M/3.13G [00:05<00:21, 117MB/s]Downloading model.safetensors:  20%|█▉        | 619M/3.13G [00:05<00:21, 117MB/s]Downloading model.safetensors:  20%|██        | 640M/3.13G [00:05<00:21, 117MB/s]Downloading model.safetensors:  21%|██        | 661M/3.13G [00:05<00:21, 117MB/s]Downloading model.safetensors:  22%|██▏       | 682M/3.13G [00:05<00:20, 117MB/s]Downloading model.safetensors:  22%|██▏       | 703M/3.13G [00:06<00:20, 117MB/s]Downloading model.safetensors:  23%|██▎       | 724M/3.13G [00:06<00:20, 117MB/s]Downloading model.safetensors:  24%|██▍       | 744M/3.13G [00:06<00:20, 117MB/s]Downloading model.safetensors:  24%|██▍       | 765M/3.13G [00:06<00:20, 117MB/s]Downloading model.safetensors:  25%|██▌       | 786M/3.13G [00:06<00:19, 117MB/s]Downloading model.safetensors:  26%|██▌       | 807M/3.13G [00:06<00:19, 117MB/s]Downloading model.safetensors:  26%|██▋       | 828M/3.13G [00:07<00:19, 117MB/s]Downloading model.safetensors:  27%|██▋       | 849M/3.13G [00:07<00:19, 117MB/s]Downloading model.safetensors:  28%|██▊       | 870M/3.13G [00:07<00:19, 117MB/s]Downloading model.safetensors:  28%|██▊       | 891M/3.13G [00:07<00:19, 117MB/s]Downloading model.safetensors:  29%|██▉       | 912M/3.13G [00:07<00:18, 117MB/s]Downloading model.safetensors:  30%|██▉       | 933M/3.13G [00:07<00:18, 117MB/s]Downloading model.safetensors:  30%|███       | 954M/3.13G [00:08<00:18, 117MB/s]Downloading model.safetensors:  31%|███       | 975M/3.13G [00:08<00:18, 117MB/s]Downloading model.safetensors:  32%|███▏      | 996M/3.13G [00:08<00:18, 117MB/s]Downloading model.safetensors:  32%|███▏      | 1.02G/3.13G [00:08<00:18, 118MB/s]Downloading model.safetensors:  33%|███▎      | 1.04G/3.13G [00:08<00:17, 117MB/s]Downloading model.safetensors:  34%|███▍      | 1.06G/3.13G [00:09<00:17, 118MB/s]Downloading model.safetensors:  34%|███▍      | 1.08G/3.13G [00:09<00:17, 117MB/s]Downloading model.safetensors:  35%|███▌      | 1.10G/3.13G [00:09<00:17, 118MB/s]Downloading model.safetensors:  36%|███▌      | 1.12G/3.13G [00:09<00:17, 118MB/s]Downloading model.safetensors:  36%|███▋      | 1.14G/3.13G [00:09<00:16, 117MB/s]Downloading model.safetensors:  37%|███▋      | 1.16G/3.13G [00:09<00:16, 118MB/s]Downloading model.safetensors:  38%|███▊      | 1.18G/3.13G [00:10<00:16, 118MB/s]Downloading model.safetensors:  38%|███▊      | 1.21G/3.13G [00:10<00:22, 86.5MB/s]Downloading model.safetensors:  39%|███▉      | 1.23G/3.13G [00:10<00:20, 93.9MB/s]Downloading model.safetensors:  40%|███▉      | 1.25G/3.13G [00:10<00:18, 99.9MB/s]Downloading model.safetensors:  41%|████      | 1.27G/3.13G [00:11<00:17, 105MB/s] Downloading model.safetensors:  41%|████      | 1.29G/3.13G [00:11<00:17, 108MB/s]Downloading model.safetensors:  42%|████▏     | 1.31G/3.13G [00:11<00:16, 110MB/s]Downloading model.safetensors:  43%|████▎     | 1.33G/3.13G [00:11<00:15, 113MB/s]Downloading model.safetensors:  43%|████▎     | 1.35G/3.13G [00:11<00:15, 114MB/s]Downloading model.safetensors:  44%|████▍     | 1.37G/3.13G [00:11<00:15, 115MB/s]Downloading model.safetensors:  45%|████▍     | 1.39G/3.13G [00:12<00:15, 116MB/s]Downloading model.safetensors:  45%|████▌     | 1.42G/3.13G [00:12<00:14, 116MB/s]Downloading model.safetensors:  46%|████▌     | 1.44G/3.13G [00:12<00:14, 117MB/s]Downloading model.safetensors:  47%|████▋     | 1.46G/3.13G [00:12<00:14, 117MB/s]Downloading model.safetensors:  47%|████▋     | 1.48G/3.13G [00:12<00:14, 117MB/s]Downloading model.safetensors:  48%|████▊     | 1.50G/3.13G [00:13<00:13, 117MB/s]Downloading model.safetensors:  49%|████▊     | 1.52G/3.13G [00:13<00:14, 112MB/s]Downloading model.safetensors:  49%|████▉     | 1.54G/3.13G [00:13<00:14, 113MB/s]Downloading model.safetensors:  50%|████▉     | 1.56G/3.13G [00:13<00:13, 114MB/s]Downloading model.safetensors:  51%|█████     | 1.58G/3.13G [00:13<00:13, 115MB/s]Downloading model.safetensors:  51%|█████     | 1.60G/3.13G [00:13<00:13, 116MB/s]Downloading model.safetensors:  52%|█████▏    | 1.63G/3.13G [00:14<00:13, 114MB/s]Downloading model.safetensors:  53%|█████▎    | 1.65G/3.13G [00:14<00:12, 117MB/s]Downloading model.safetensors:  53%|█████▎    | 1.67G/3.13G [00:14<00:13, 112MB/s]Downloading model.safetensors:  54%|█████▍    | 1.69G/3.13G [00:14<00:13, 111MB/s]Downloading model.safetensors:  55%|█████▍    | 1.71G/3.13G [00:14<00:12, 112MB/s]Downloading model.safetensors:  55%|█████▌    | 1.73G/3.13G [00:15<00:12, 114MB/s]Downloading model.safetensors:  56%|█████▌    | 1.75G/3.13G [00:15<00:12, 115MB/s]Downloading model.safetensors:  57%|█████▋    | 1.77G/3.13G [00:15<00:11, 115MB/s]Downloading model.safetensors:  57%|█████▋    | 1.79G/3.13G [00:15<00:11, 116MB/s]Downloading model.safetensors:  58%|█████▊    | 1.81G/3.13G [00:15<00:11, 116MB/s]Downloading model.safetensors:  59%|█████▊    | 1.84G/3.13G [00:15<00:11, 117MB/s]Downloading model.safetensors:  59%|█████▉    | 1.86G/3.13G [00:16<00:10, 117MB/s]Downloading model.safetensors:  60%|█████▉    | 1.88G/3.13G [00:16<00:10, 117MB/s]Downloading model.safetensors:  61%|██████    | 1.90G/3.13G [00:16<00:10, 117MB/s]Downloading model.safetensors:  61%|██████▏   | 1.92G/3.13G [00:16<00:10, 117MB/s]Downloading model.safetensors:  62%|██████▏   | 1.94G/3.13G [00:16<00:10, 117MB/s]Downloading model.safetensors:  63%|██████▎   | 1.96G/3.13G [00:17<00:09, 117MB/s]Downloading model.safetensors:  63%|██████▎   | 1.98G/3.13G [00:17<00:09, 117MB/s]Downloading model.safetensors:  64%|██████▍   | 2.00G/3.13G [00:17<00:09, 117MB/s]Downloading model.safetensors:  65%|██████▍   | 2.02G/3.13G [00:17<00:09, 116MB/s]Downloading model.safetensors:  65%|██████▌   | 2.04G/3.13G [00:17<00:09, 118MB/s]Downloading model.safetensors:  66%|██████▌   | 2.07G/3.13G [00:17<00:09, 117MB/s]Downloading model.safetensors:  67%|██████▋   | 2.09G/3.13G [00:18<00:08, 117MB/s]Downloading model.safetensors:  67%|██████▋   | 2.11G/3.13G [00:18<00:08, 117MB/s]Downloading model.safetensors:  68%|██████▊   | 2.13G/3.13G [00:18<00:08, 117MB/s]Downloading model.safetensors:  69%|██████▊   | 2.15G/3.13G [00:18<00:08, 117MB/s]Downloading model.safetensors:  69%|██████▉   | 2.17G/3.13G [00:18<00:08, 117MB/s]Downloading model.safetensors:  70%|██████▉   | 2.19G/3.13G [00:18<00:08, 117MB/s]Downloading model.safetensors:  71%|███████   | 2.21G/3.13G [00:19<00:07, 117MB/s]Downloading model.safetensors:  71%|███████▏  | 2.23G/3.13G [00:19<00:07, 117MB/s]Downloading model.safetensors:  72%|███████▏  | 2.25G/3.13G [00:19<00:07, 117MB/s]Downloading model.safetensors:  73%|███████▎  | 2.28G/3.13G [00:19<00:07, 117MB/s]Downloading model.safetensors:  73%|███████▎  | 2.30G/3.13G [00:19<00:07, 116MB/s]Downloading model.safetensors:  74%|███████▍  | 2.32G/3.13G [00:20<00:06, 118MB/s]Downloading model.safetensors:  75%|███████▍  | 2.34G/3.13G [00:20<00:06, 118MB/s]Downloading model.safetensors:  75%|███████▌  | 2.36G/3.13G [00:20<00:08, 87.3MB/s]Downloading model.safetensors:  76%|███████▌  | 2.38G/3.13G [00:20<00:07, 94.6MB/s]Downloading model.safetensors:  77%|███████▋  | 2.40G/3.13G [00:20<00:07, 100MB/s] Downloading model.safetensors:  77%|███████▋  | 2.42G/3.13G [00:21<00:06, 105MB/s]Downloading model.safetensors:  78%|███████▊  | 2.44G/3.13G [00:21<00:06, 104MB/s]Downloading model.safetensors:  79%|███████▊  | 2.46G/3.13G [00:21<00:06, 104MB/s]Downloading model.safetensors:  79%|███████▉  | 2.49G/3.13G [00:21<00:06, 106MB/s]Downloading model.safetensors:  80%|███████▉  | 2.51G/3.13G [00:21<00:05, 109MB/s]Downloading model.safetensors:  81%|████████  | 2.53G/3.13G [00:22<00:05, 112MB/s]Downloading model.safetensors:  81%|████████▏ | 2.55G/3.13G [00:22<00:05, 113MB/s]Downloading model.safetensors:  82%|████████▏ | 2.57G/3.13G [00:22<00:04, 114MB/s]Downloading model.safetensors:  83%|████████▎ | 2.59G/3.13G [00:22<00:04, 115MB/s]Downloading model.safetensors:  83%|████████▎ | 2.61G/3.13G [00:22<00:04, 116MB/s]Downloading model.safetensors:  84%|████████▍ | 2.63G/3.13G [00:23<00:04, 116MB/s]Downloading model.safetensors:  85%|████████▍ | 2.65G/3.13G [00:23<00:04, 117MB/s]Downloading model.safetensors:  85%|████████▌ | 2.67G/3.13G [00:23<00:03, 116MB/s]Downloading model.safetensors:  86%|████████▌ | 2.69G/3.13G [00:23<00:03, 117MB/s]Downloading model.safetensors:  87%|████████▋ | 2.72G/3.13G [00:23<00:03, 113MB/s]Downloading model.safetensors:  87%|████████▋ | 2.74G/3.13G [00:23<00:03, 116MB/s]Downloading model.safetensors:  88%|████████▊ | 2.76G/3.13G [00:24<00:03, 116MB/s]Downloading model.safetensors:  89%|████████▊ | 2.78G/3.13G [00:24<00:03, 116MB/s]Downloading model.safetensors:  89%|████████▉ | 2.80G/3.13G [00:24<00:02, 117MB/s]Downloading model.safetensors:  90%|█████████ | 2.82G/3.13G [00:24<00:02, 117MB/s]Downloading model.safetensors:  91%|█████████ | 2.84G/3.13G [00:24<00:02, 109MB/s]Downloading model.safetensors:  91%|█████████▏| 2.86G/3.13G [00:25<00:02, 110MB/s]Downloading model.safetensors:  92%|█████████▏| 2.88G/3.13G [00:25<00:02, 112MB/s]Downloading model.safetensors:  93%|█████████▎| 2.90G/3.13G [00:25<00:02, 113MB/s]Downloading model.safetensors:  93%|█████████▎| 2.93G/3.13G [00:25<00:01, 114MB/s]Downloading model.safetensors:  94%|█████████▍| 2.95G/3.13G [00:25<00:01, 115MB/s]Downloading model.safetensors:  95%|█████████▍| 2.97G/3.13G [00:25<00:01, 114MB/s]Downloading model.safetensors:  95%|█████████▌| 2.99G/3.13G [00:26<00:01, 115MB/s]Downloading model.safetensors:  96%|█████████▌| 3.01G/3.13G [00:26<00:01, 116MB/s]Downloading model.safetensors:  97%|█████████▋| 3.03G/3.13G [00:26<00:00, 116MB/s]Downloading model.safetensors:  97%|█████████▋| 3.05G/3.13G [00:26<00:00, 117MB/s]Downloading model.safetensors:  98%|█████████▊| 3.07G/3.13G [00:26<00:00, 117MB/s]Downloading model.safetensors:  99%|█████████▊| 3.09G/3.13G [00:27<00:00, 114MB/s]Downloading model.safetensors:  99%|█████████▉| 3.11G/3.13G [00:27<00:00, 118MB/s]Downloading model.safetensors: 100%|██████████| 3.13G/3.13G [00:27<00:00, 118MB/s]Downloading model.safetensors: 100%|██████████| 3.13G/3.13G [00:27<00:00, 115MB/s]
Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]Downloading (…)neration_config.json: 100%|██████████| 147/147 [00:00<00:00, 108kB/s]
[38;20m[INFO] - __main__ : config file was successfully saved to /rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/NoDPR-flan-t5-large-lora.22571926 for future use.[0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[38;20m[INFO] - trainers.t5_executor_with_prefix : #params: 846   lr: 6e-05[0m
Loading `train_dataloader` to estimate number of stepping batches.
/home/xl544/.conda/envs/BLIP2/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(

  | Name  | Type            | Params
------------------------------------------
0 | model | PrefixModelLora | 785 M 
------------------------------------------
785 M     Trainable params
0         Non-trainable params
785 M     Total params
3,141.833 Total estimated model params size (MB)
Missing logger folder: /rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data_TB/tb_logs/NoDPR-flan-t5-large-lora.22571926/NoDPR-flan-t5-large-lora.22571926
SLURM auto-requeueing enabled. Setting signal handlers.
/home/xl544/.conda/envs/BLIP2/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Failed to compute OKVQA scores: Results do not correspond to current VQA set. Either the results do not have predictions for all question ids in annotation file or there is atleast one question id that does not belong to the question ids in the annotation file.This could be due to the fact that OKVQA parser requires all questions to evaluatethe accuracy. Ignore this error if this is the sanity check.[0m
[38;20m[INFO] - trainers.t5_executor_with_prefix : Evaluation results [sanity_check]: {'test/epoch': 0}[0m
[33;20m[WARNING] - root : Sanity check mode, not saving to loggers.[0m
/home/xl544/.conda/envs/BLIP2/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.t5_executor_with_prefix : Evaluation results [validate]: {'test/accuracy_overall': 18.55, 'test/accuracy_QuestionType_one': 17.2, 'test/accuracy_QuestionType_eight': 15.6, 'test/accuracy_QuestionType_other': 20.03, 'test/accuracy_QuestionType_seven': 18.69, 'test/accuracy_QuestionType_four': 19.29, 'test/accuracy_QuestionType_five': 20.35, 'test/accuracy_QuestionType_three': 18.74, 'test/accuracy_QuestionType_nine': 18.57, 'test/accuracy_QuestionType_ten': 27.6, 'test/accuracy_QuestionType_two': 18.84, 'test/accuracy_QuestionType_six': 15.04, 'test/accuracy_AnswerType_other': 18.55, 'test/epoch': 0}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.t5_executor_with_prefix : Evaluation results [validate]: {'test/accuracy_overall': 19.86, 'test/accuracy_QuestionType_one': 18.15, 'test/accuracy_QuestionType_eight': 16.16, 'test/accuracy_QuestionType_other': 20.87, 'test/accuracy_QuestionType_seven': 21.4, 'test/accuracy_QuestionType_four': 18.2, 'test/accuracy_QuestionType_five': 22.56, 'test/accuracy_QuestionType_three': 22.2, 'test/accuracy_QuestionType_nine': 20.24, 'test/accuracy_QuestionType_ten': 31.32, 'test/accuracy_QuestionType_two': 19.3, 'test/accuracy_QuestionType_six': 17.3, 'test/accuracy_AnswerType_other': 19.86, 'test/epoch': 0}[0m
/home/xl544/.conda/envs/BLIP2/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
Epoch 0, global step 282: 'test/accuracy_overall' reached 19.86000 (best 19.86000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/NoDPR-flan-t5-large-lora.22571926/train/saved_model/model_0.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.t5_executor_with_prefix : Evaluation results [validate]: {'test/accuracy_overall': 21.84, 'test/accuracy_QuestionType_one': 20.12, 'test/accuracy_QuestionType_eight': 18.44, 'test/accuracy_QuestionType_other': 24.33, 'test/accuracy_QuestionType_seven': 22.99, 'test/accuracy_QuestionType_four': 20.88, 'test/accuracy_QuestionType_five': 23.73, 'test/accuracy_QuestionType_three': 23.97, 'test/accuracy_QuestionType_nine': 20.24, 'test/accuracy_QuestionType_ten': 30.23, 'test/accuracy_QuestionType_two': 20.81, 'test/accuracy_QuestionType_six': 19.01, 'test/accuracy_AnswerType_other': 21.84, 'test/epoch': 1}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.t5_executor_with_prefix : Evaluation results [validate]: {'test/accuracy_overall': 22.83, 'test/accuracy_QuestionType_one': 21.8, 'test/accuracy_QuestionType_eight': 19.35, 'test/accuracy_QuestionType_other': 25.53, 'test/accuracy_QuestionType_seven': 24.49, 'test/accuracy_QuestionType_four': 22.22, 'test/accuracy_QuestionType_five': 25.19, 'test/accuracy_QuestionType_three': 24.3, 'test/accuracy_QuestionType_nine': 19.76, 'test/accuracy_QuestionType_ten': 30.23, 'test/accuracy_QuestionType_two': 20.35, 'test/accuracy_QuestionType_six': 15.74, 'test/accuracy_AnswerType_other': 22.83, 'test/epoch': 1}[0m
Epoch 1, global step 564: 'test/accuracy_overall' reached 22.83000 (best 22.83000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/NoDPR-flan-t5-large-lora.22571926/train/saved_model/model_1.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.t5_executor_with_prefix : Evaluation results [validate]: {'test/accuracy_overall': 23.87, 'test/accuracy_QuestionType_one': 22.73, 'test/accuracy_QuestionType_eight': 20.86, 'test/accuracy_QuestionType_other': 27.3, 'test/accuracy_QuestionType_seven': 26.12, 'test/accuracy_QuestionType_four': 23.03, 'test/accuracy_QuestionType_five': 24.4, 'test/accuracy_QuestionType_three': 26.4, 'test/accuracy_QuestionType_nine': 19.76, 'test/accuracy_QuestionType_ten': 33.33, 'test/accuracy_QuestionType_two': 20.23, 'test/accuracy_QuestionType_six': 17.87, 'test/accuracy_AnswerType_other': 23.87, 'test/epoch': 2}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.t5_executor_with_prefix : Evaluation results [validate]: {'test/accuracy_overall': 24.3, 'test/accuracy_QuestionType_one': 22.12, 'test/accuracy_QuestionType_eight': 22.28, 'test/accuracy_QuestionType_other': 26.62, 'test/accuracy_QuestionType_seven': 27.29, 'test/accuracy_QuestionType_four': 22.89, 'test/accuracy_QuestionType_five': 25.32, 'test/accuracy_QuestionType_three': 25.19, 'test/accuracy_QuestionType_nine': 22.14, 'test/accuracy_QuestionType_ten': 33.95, 'test/accuracy_QuestionType_two': 25.7, 'test/accuracy_QuestionType_six': 18.01, 'test/accuracy_AnswerType_other': 24.3, 'test/epoch': 2}[0m
Epoch 2, global step 846: 'test/accuracy_overall' reached 24.30000 (best 24.30000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/NoDPR-flan-t5-large-lora.22571926/train/saved_model/model_2.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.t5_executor_with_prefix : Evaluation results [validate]: {'test/accuracy_overall': 24.86, 'test/accuracy_QuestionType_one': 23.44, 'test/accuracy_QuestionType_eight': 22.07, 'test/accuracy_QuestionType_other': 28.69, 'test/accuracy_QuestionType_seven': 27.52, 'test/accuracy_QuestionType_four': 23.32, 'test/accuracy_QuestionType_five': 25.57, 'test/accuracy_QuestionType_three': 25.79, 'test/accuracy_QuestionType_nine': 20.95, 'test/accuracy_QuestionType_ten': 31.01, 'test/accuracy_QuestionType_two': 25.0, 'test/accuracy_QuestionType_six': 20.85, 'test/accuracy_AnswerType_other': 24.86, 'test/epoch': 3}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.t5_executor_with_prefix : Evaluation results [validate]: {'test/accuracy_overall': 25.36, 'test/accuracy_QuestionType_one': 23.57, 'test/accuracy_QuestionType_eight': 23.4, 'test/accuracy_QuestionType_other': 27.66, 'test/accuracy_QuestionType_seven': 26.87, 'test/accuracy_QuestionType_four': 25.33, 'test/accuracy_QuestionType_five': 26.24, 'test/accuracy_QuestionType_three': 26.73, 'test/accuracy_QuestionType_nine': 19.76, 'test/accuracy_QuestionType_ten': 32.71, 'test/accuracy_QuestionType_two': 21.86, 'test/accuracy_QuestionType_six': 24.82, 'test/accuracy_AnswerType_other': 25.36, 'test/epoch': 3}[0m
Epoch 3, global step 1128: 'test/accuracy_overall' reached 25.36000 (best 25.36000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/NoDPR-flan-t5-large-lora.22571926/train/saved_model/model_3.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.t5_executor_with_prefix : Evaluation results [validate]: {'test/accuracy_overall': 26.2, 'test/accuracy_QuestionType_one': 23.54, 'test/accuracy_QuestionType_eight': 24.23, 'test/accuracy_QuestionType_other': 30.05, 'test/accuracy_QuestionType_seven': 29.16, 'test/accuracy_QuestionType_four': 26.31, 'test/accuracy_QuestionType_five': 25.14, 'test/accuracy_QuestionType_three': 26.59, 'test/accuracy_QuestionType_nine': 23.33, 'test/accuracy_QuestionType_ten': 34.57, 'test/accuracy_QuestionType_two': 26.28, 'test/accuracy_QuestionType_six': 25.96, 'test/accuracy_AnswerType_other': 26.2, 'test/epoch': 4}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.t5_executor_with_prefix : Evaluation results [validate]: {'test/accuracy_overall': 26.2, 'test/accuracy_QuestionType_one': 24.11, 'test/accuracy_QuestionType_eight': 23.74, 'test/accuracy_QuestionType_other': 27.63, 'test/accuracy_QuestionType_seven': 29.49, 'test/accuracy_QuestionType_four': 27.13, 'test/accuracy_QuestionType_five': 27.38, 'test/accuracy_QuestionType_three': 26.78, 'test/accuracy_QuestionType_nine': 22.14, 'test/accuracy_QuestionType_ten': 32.56, 'test/accuracy_QuestionType_two': 23.95, 'test/accuracy_QuestionType_six': 24.11, 'test/accuracy_AnswerType_other': 26.2, 'test/epoch': 4}[0m
Epoch 4, global step 1410: 'test/accuracy_overall' reached 26.20000 (best 26.20000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/NoDPR-flan-t5-large-lora.22571926/train/saved_model/model_4.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.t5_executor_with_prefix : Evaluation results [validate]: {'test/accuracy_overall': 26.65, 'test/accuracy_QuestionType_one': 24.23, 'test/accuracy_QuestionType_eight': 24.49, 'test/accuracy_QuestionType_other': 28.76, 'test/accuracy_QuestionType_seven': 29.35, 'test/accuracy_QuestionType_four': 27.09, 'test/accuracy_QuestionType_five': 27.78, 'test/accuracy_QuestionType_three': 27.85, 'test/accuracy_QuestionType_nine': 23.33, 'test/accuracy_QuestionType_ten': 32.56, 'test/accuracy_QuestionType_two': 25.58, 'test/accuracy_QuestionType_six': 22.27, 'test/accuracy_AnswerType_other': 26.65, 'test/epoch': 5}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.t5_executor_with_prefix : Evaluation results [validate]: {'test/accuracy_overall': 26.61, 'test/accuracy_QuestionType_one': 23.86, 'test/accuracy_QuestionType_eight': 24.16, 'test/accuracy_QuestionType_other': 28.76, 'test/accuracy_QuestionType_seven': 29.3, 'test/accuracy_QuestionType_four': 28.25, 'test/accuracy_QuestionType_five': 26.88, 'test/accuracy_QuestionType_three': 28.22, 'test/accuracy_QuestionType_nine': 20.95, 'test/accuracy_QuestionType_ten': 34.26, 'test/accuracy_QuestionType_two': 25.7, 'test/accuracy_QuestionType_six': 24.26, 'test/accuracy_AnswerType_other': 26.61, 'test/epoch': 5}[0m
Epoch 5, global step 1692: 'test/accuracy_overall' reached 26.61000 (best 26.61000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/NoDPR-flan-t5-large-lora.22571926/train/saved_model/model_5.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.t5_executor_with_prefix : Evaluation results [validate]: {'test/accuracy_overall': 27.0, 'test/accuracy_QuestionType_one': 25.71, 'test/accuracy_QuestionType_eight': 25.07, 'test/accuracy_QuestionType_other': 29.4, 'test/accuracy_QuestionType_seven': 28.97, 'test/accuracy_QuestionType_four': 28.25, 'test/accuracy_QuestionType_five': 26.66, 'test/accuracy_QuestionType_three': 28.22, 'test/accuracy_QuestionType_nine': 19.76, 'test/accuracy_QuestionType_ten': 31.47, 'test/accuracy_QuestionType_two': 26.28, 'test/accuracy_QuestionType_six': 23.83, 'test/accuracy_AnswerType_other': 27.0, 'test/epoch': 6}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.t5_executor_with_prefix : Evaluation results [validate]: {'test/accuracy_overall': 26.77, 'test/accuracy_QuestionType_one': 24.62, 'test/accuracy_QuestionType_eight': 24.16, 'test/accuracy_QuestionType_other': 28.59, 'test/accuracy_QuestionType_seven': 29.95, 'test/accuracy_QuestionType_four': 27.09, 'test/accuracy_QuestionType_five': 27.48, 'test/accuracy_QuestionType_three': 28.04, 'test/accuracy_QuestionType_nine': 20.95, 'test/accuracy_QuestionType_ten': 35.66, 'test/accuracy_QuestionType_two': 26.74, 'test/accuracy_QuestionType_six': 23.69, 'test/accuracy_AnswerType_other': 26.77, 'test/epoch': 6}[0m
Epoch 6, global step 1974: 'test/accuracy_overall' reached 26.77000 (best 26.77000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/NoDPR-flan-t5-large-lora.22571926/train/saved_model/model_6.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.t5_executor_with_prefix : Evaluation results [validate]: {'test/accuracy_overall': 26.58, 'test/accuracy_QuestionType_one': 24.53, 'test/accuracy_QuestionType_eight': 24.49, 'test/accuracy_QuestionType_other': 29.05, 'test/accuracy_QuestionType_seven': 29.25, 'test/accuracy_QuestionType_four': 26.46, 'test/accuracy_QuestionType_five': 27.2, 'test/accuracy_QuestionType_three': 27.57, 'test/accuracy_QuestionType_nine': 22.14, 'test/accuracy_QuestionType_ten': 33.02, 'test/accuracy_QuestionType_two': 25.93, 'test/accuracy_QuestionType_six': 23.83, 'test/accuracy_AnswerType_other': 26.58, 'test/epoch': 7}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.t5_executor_with_prefix : Evaluation results [validate]: {'test/accuracy_overall': 27.0, 'test/accuracy_QuestionType_one': 24.21, 'test/accuracy_QuestionType_eight': 25.47, 'test/accuracy_QuestionType_other': 30.21, 'test/accuracy_QuestionType_seven': 29.25, 'test/accuracy_QuestionType_four': 27.65, 'test/accuracy_QuestionType_five': 27.58, 'test/accuracy_QuestionType_three': 27.48, 'test/accuracy_QuestionType_nine': 23.33, 'test/accuracy_QuestionType_ten': 32.09, 'test/accuracy_QuestionType_two': 25.35, 'test/accuracy_QuestionType_six': 23.83, 'test/accuracy_AnswerType_other': 27.0, 'test/epoch': 7}[0m
Epoch 7, global step 2256: 'test/accuracy_overall' reached 27.00000 (best 27.00000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/NoDPR-flan-t5-large-lora.22571926/train/saved_model/model_7.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.t5_executor_with_prefix : Evaluation results [validate]: {'test/accuracy_overall': 27.01, 'test/accuracy_QuestionType_one': 24.43, 'test/accuracy_QuestionType_eight': 24.49, 'test/accuracy_QuestionType_other': 29.21, 'test/accuracy_QuestionType_seven': 28.74, 'test/accuracy_QuestionType_four': 27.34, 'test/accuracy_QuestionType_five': 27.9, 'test/accuracy_QuestionType_three': 28.88, 'test/accuracy_QuestionType_nine': 26.9, 'test/accuracy_QuestionType_ten': 36.9, 'test/accuracy_QuestionType_two': 24.77, 'test/accuracy_QuestionType_six': 24.11, 'test/accuracy_AnswerType_other': 27.01, 'test/epoch': 8}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.t5_executor_with_prefix : Evaluation results [validate]: {'test/accuracy_overall': 27.17, 'test/accuracy_QuestionType_one': 24.7, 'test/accuracy_QuestionType_eight': 24.77, 'test/accuracy_QuestionType_other': 29.76, 'test/accuracy_QuestionType_seven': 28.83, 'test/accuracy_QuestionType_four': 27.2, 'test/accuracy_QuestionType_five': 27.63, 'test/accuracy_QuestionType_three': 29.21, 'test/accuracy_QuestionType_nine': 26.9, 'test/accuracy_QuestionType_ten': 34.57, 'test/accuracy_QuestionType_two': 26.28, 'test/accuracy_QuestionType_six': 25.39, 'test/accuracy_AnswerType_other': 27.17, 'test/epoch': 8}[0m
Epoch 8, global step 2538: 'test/accuracy_overall' reached 27.17000 (best 27.17000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/NoDPR-flan-t5-large-lora.22571926/train/saved_model/model_8.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.t5_executor_with_prefix : Evaluation results [validate]: {'test/accuracy_overall': 27.15, 'test/accuracy_QuestionType_one': 24.65, 'test/accuracy_QuestionType_eight': 24.79, 'test/accuracy_QuestionType_other': 29.01, 'test/accuracy_QuestionType_seven': 29.44, 'test/accuracy_QuestionType_four': 27.55, 'test/accuracy_QuestionType_five': 28.12, 'test/accuracy_QuestionType_three': 28.36, 'test/accuracy_QuestionType_nine': 26.9, 'test/accuracy_QuestionType_ten': 34.88, 'test/accuracy_QuestionType_two': 25.47, 'test/accuracy_QuestionType_six': 24.96, 'test/accuracy_AnswerType_other': 27.15, 'test/epoch': 9}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.t5_executor_with_prefix : Evaluation results [validate]: {'test/accuracy_overall': 27.3, 'test/accuracy_QuestionType_one': 24.75, 'test/accuracy_QuestionType_eight': 24.91, 'test/accuracy_QuestionType_other': 29.53, 'test/accuracy_QuestionType_seven': 29.58, 'test/accuracy_QuestionType_four': 27.48, 'test/accuracy_QuestionType_five': 28.27, 'test/accuracy_QuestionType_three': 28.6, 'test/accuracy_QuestionType_nine': 26.9, 'test/accuracy_QuestionType_ten': 35.66, 'test/accuracy_QuestionType_two': 25.47, 'test/accuracy_QuestionType_six': 24.54, 'test/accuracy_AnswerType_other': 27.3, 'test/epoch': 9}[0m
Epoch 9, global step 2820: 'test/accuracy_overall' reached 27.30000 (best 27.30000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/NoDPR-flan-t5-large-lora.22571926/train/saved_model/model_9.ckpt' as top 1
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                     epoch ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████
wandb:            test/accuracy_AnswerType_other ▁▂▄▄▅▆▆▆▇▇▇▇██▇█████
wandb:   test/accuracy_AnswerType_other_auto_max ▁▄▅▆▇▇████
wandb:   test/accuracy_AnswerType_other_auto_min ▁▁▁▁▁▁▁▁▁▁
wandb:          test/accuracy_QuestionType_eight ▁▁▃▄▅▆▆▇▇▇▇▇█▇▇█▇███
wandb: test/accuracy_QuestionType_eight_auto_max ▁▃▆▆▇▇████
wandb: test/accuracy_QuestionType_eight_auto_min ▁▁▁▁▁▁▁▁▁▁
wandb:           test/accuracy_QuestionType_five ▁▃▄▅▅▅▆▆▅▇█▇▇▇▇▇█▇██
wandb:  test/accuracy_QuestionType_five_auto_max ▁▄▄▆▇▇▇▇██
wandb:  test/accuracy_QuestionType_five_auto_min ▁▁▁▁▁▁▁▁▁▁
wandb:           test/accuracy_QuestionType_four ▂▁▃▄▄▄▅▆▇▇▇██▇▇█▇▇█▇
wandb:  test/accuracy_QuestionType_four_auto_max ▁▃▄▆▇█████
wandb:  test/accuracy_QuestionType_four_auto_min ▁▁▁▁▁▁▁▁▁▁
wandb:           test/accuracy_QuestionType_nine ▁▂▂▂▂▄▃▂▅▄▅▃▂▃▄▅████
wandb:  test/accuracy_QuestionType_nine_auto_max ▁▁▃▃▄▄▄▄██
wandb:  test/accuracy_QuestionType_nine_auto_min ▁▁▁▁▁▁▁▁▁▁
wandb:            test/accuracy_QuestionType_one ▁▂▃▅▆▅▆▆▆▇▇▆█▇▇▇▇▇▇▇
wandb:   test/accuracy_QuestionType_one_auto_max ▁▄▅▆▇▇████
wandb:   test/accuracy_QuestionType_one_auto_min ▁▁▁▁▁▁▁▁▁▁
wandb:          test/accuracy_QuestionType_other ▁▂▄▅▆▆▇▆█▆▇▇▇▇▇█▇█▇█
wandb: test/accuracy_QuestionType_other_auto_max ▁▄▆▇██████
wandb: test/accuracy_QuestionType_other_auto_min ▁▁▁▁▁▁▁▁▁▁
wandb:          test/accuracy_QuestionType_seven ▁▃▄▅▆▆▆▆████▇███▇▇██
wandb: test/accuracy_QuestionType_seven_auto_max ▁▄▆▆██████
wandb: test/accuracy_QuestionType_seven_auto_min ▁▁▁▁▁▁▁▁▁▁
wandb:            test/accuracy_QuestionType_six ▁▂▄▁▃▃▅▇█▇▆▇▇▇▇▇▇█▇▇
wandb:   test/accuracy_QuestionType_six_auto_max ▁▂▂▇██████
wandb:   test/accuracy_QuestionType_six_auto_min ▁▁▁▁▁▁▁▁▁▁
wandb:            test/accuracy_QuestionType_ten ▁▄▃▃▅▆▄▅▆▅▅▆▄▇▅▄█▆▆▇
wandb:   test/accuracy_QuestionType_ten_auto_max ▁▁▄▄▅▅▆▆██
wandb:   test/accuracy_QuestionType_ten_auto_min ▁▁▁▁▁▁▁▁▁▁
wandb:          test/accuracy_QuestionType_three ▁▃▄▅▆▅▆▆▆▆▇▇▇▇▇▇██▇█
wandb: test/accuracy_QuestionType_three_auto_max ▁▃▅▆▆▇▇▇██
wandb: test/accuracy_QuestionType_three_auto_min ▁▁▁▁▁▁▁▁▁▁
wandb:            test/accuracy_QuestionType_two ▁▁▃▂▂▇▆▄█▆▇▇██▇▇▆█▇▇
wandb:   test/accuracy_QuestionType_two_auto_max ▁▂▇▇██████
wandb:   test/accuracy_QuestionType_two_auto_min ▁▁▁▁▁▁▁▁▁▁
wandb:                     test/accuracy_overall ▁▂▄▄▅▆▆▆▇▇▇▇██▇█████
wandb:            test/accuracy_overall_auto_max ▁▄▅▆▇▇████
wandb:            test/accuracy_overall_auto_min ▁▁▁▁▁▁▁▁▁▁
wandb:                                test/epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:                       test/epoch_auto_max ▁▂▃▃▄▅▆▆▇█
wandb:                       test/epoch_auto_min ▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss_epoch █▆▄▄▃▂▂▁▁▁
wandb:                 train/loss_epoch_auto_max ▁▁▁▁▁▁▁▁▁
wandb:                 train/loss_epoch_auto_min █▆▄▃▃▂▂▁▁
wandb:                           train/loss_step █▃▅▃▃▂▄▄▇▄▄▃▃▂▃▃▆▁▃▃▃▄▂▁▅▄▅▂▃▄▅▂▂▄▃▂▄▄▂▃
wandb:                  train/loss_step_auto_max ▁▁▁▁▁▁▁▁▁█
wandb:                  train/loss_step_auto_min █▂▂▂▂▂▂▂▂▁
wandb:                               train/lr[0] ███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:                      train/lr[0]_auto_max ▁▁▁▁▁▁▁▁▁▁
wandb:                      train/lr[0]_auto_min █▇▆▆▅▄▃▃▂▁
wandb:                       trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:                                     epoch 9
wandb:            test/accuracy_AnswerType_other 27.3
wandb:   test/accuracy_AnswerType_other_auto_max 27.3
wandb:   test/accuracy_AnswerType_other_auto_min 18.55
wandb:          test/accuracy_QuestionType_eight 24.91
wandb: test/accuracy_QuestionType_eight_auto_max 25.47
wandb: test/accuracy_QuestionType_eight_auto_min 15.6
wandb:           test/accuracy_QuestionType_five 28.27
wandb:  test/accuracy_QuestionType_five_auto_max 28.27
wandb:  test/accuracy_QuestionType_five_auto_min 20.35
wandb:           test/accuracy_QuestionType_four 27.48
wandb:  test/accuracy_QuestionType_four_auto_max 28.25
wandb:  test/accuracy_QuestionType_four_auto_min 18.2
wandb:           test/accuracy_QuestionType_nine 26.9
wandb:  test/accuracy_QuestionType_nine_auto_max 26.9
wandb:  test/accuracy_QuestionType_nine_auto_min 18.57
wandb:            test/accuracy_QuestionType_one 24.75
wandb:   test/accuracy_QuestionType_one_auto_max 25.71
wandb:   test/accuracy_QuestionType_one_auto_min 17.2
wandb:          test/accuracy_QuestionType_other 29.53
wandb: test/accuracy_QuestionType_other_auto_max 30.21
wandb: test/accuracy_QuestionType_other_auto_min 20.03
wandb:          test/accuracy_QuestionType_seven 29.58
wandb: test/accuracy_QuestionType_seven_auto_max 29.95
wandb: test/accuracy_QuestionType_seven_auto_min 18.69
wandb:            test/accuracy_QuestionType_six 24.54
wandb:   test/accuracy_QuestionType_six_auto_max 25.96
wandb:   test/accuracy_QuestionType_six_auto_min 15.04
wandb:            test/accuracy_QuestionType_ten 35.66
wandb:   test/accuracy_QuestionType_ten_auto_max 36.9
wandb:   test/accuracy_QuestionType_ten_auto_min 27.6
wandb:          test/accuracy_QuestionType_three 28.6
wandb: test/accuracy_QuestionType_three_auto_max 29.21
wandb: test/accuracy_QuestionType_three_auto_min 18.74
wandb:            test/accuracy_QuestionType_two 25.47
wandb:   test/accuracy_QuestionType_two_auto_max 26.74
wandb:   test/accuracy_QuestionType_two_auto_min 18.84
wandb:                     test/accuracy_overall 27.3
wandb:            test/accuracy_overall_auto_max 27.3
wandb:            test/accuracy_overall_auto_min 18.55
wandb:                                test/epoch 9.0
wandb:                       test/epoch_auto_max 9.0
wandb:                       test/epoch_auto_min 0.0
wandb:                          train/loss_epoch 1.22436
wandb:                 train/loss_epoch_auto_max 2.33991
wandb:                 train/loss_epoch_auto_min 1.25842
wandb:                           train/loss_step 0.00295
wandb:                  train/loss_step_auto_max 4.21988
wandb:                  train/loss_step_auto_min 0.00295
wandb:                               train/lr[0] 0.0
wandb:                      train/lr[0]_auto_max 6e-05
wandb:                      train/lr[0]_auto_min 0.0
wandb:                       trainer/global_step 2819
wandb: 
wandb: 🚀 View run NoDPR-flan-t5-large-lora.22571926 at: https://wandb.ai/xl544/RAVQA/runs/prmbb8bl
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230620_143214-prmbb8bl/logs
