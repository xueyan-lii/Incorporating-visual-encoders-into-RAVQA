Namespace(DATA_FOLDER='', EXPERIMENT_FOLDER='', accelerator='auto', accumulate_grad_batches=None, amp_backend='native', amp_level=None, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, benchmark=None, check_val_every_n_epoch=1, checkpoint_callback=None, config='../configs/okvqa/T5_NoDPR_prefix_only.jsonnet', default_root_dir=None, detect_anomaly=False, deterministic=None, devices='1', enable_checkpointing=True, enable_model_summary=True, enable_progress_bar=True, experiment_name='OKVQA_NoDPR-flanT5-qf.22980605', fast_dev_run=False, flush_logs_every_n_steps=None, gpus=None, gradient_clip_algorithm=None, gradient_clip_val=None, ipus=None, limit_predict_batches=None, limit_test_batches=None, limit_train_batches=None, limit_val_batches=None, log_every_n_steps=50, log_gpu_memory=None, log_prediction_tables=False, logger=True, max_epochs=None, max_steps=-1, max_time=None, min_epochs=None, min_steps=None, mode='train', modules=[], move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=None, num_sanity_val_steps=2, opts=['train.epochs=10', 'train.batch_size=2', 'valid.step_size=0.5', 'valid.batch_size=32', 'train.additional.gradient_accumulation_steps=16', 'train.lr=0.00006', 'train.MLP_lr=0.0003', 'train.scheduler=linear', 'train.load_model_path=/home/xl544/rds/hpc-work/MLMI8_2022_VQA/Experiments/Pretrain_qformer_mlp_con_cap.22972651/train/saved_model/model_09.ckpt', 'model_config.UsePrefixEmb=0.5', 'model_config.UseQformerEmb=1', 'model_config.LoadPretrainedMLP=1', 'model_config.TokenizerModelVersion=google/flan-t5-large', 'model_config.ModelVersion=google/flan-t5-large'], overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=None, process_position=0, profiler=None, progress_bar_refresh_rate=None, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, reset=False, resume_from_checkpoint=None, stochastic_weight_avg=False, strategy=None, sync_batchnorm=False, tags=[], terminate_on_nan=None, test_batch_size=-1, test_evaluation_name='', tpu_cores=None, track_grad_norm=-1, val_check_interval=None, weights_save_path=None, weights_summary='top')
input value {} is not a number, parse to string.
input value {} is not a number, parse to string.
input value {} is not a number, parse to string.
input value {} is not a number, parse to string.
{'DATA_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data', 'EXPERIMENT_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments', 'TENSORBOARD_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data_TB/tb_logs', 'WANDB': {'CACHE_DIR': '', 'entity': 'xl544', 'project': 'RAVQA', 'tags': ['OKVQA']}, 'cache': {'default_folder': '../data/ok-vqa/cache', 'regenerate': {'clip_embeddings': 0, 'ocr_feature_preprocessed': 0, 'qformer_embeddings': 0, 'test_data_preprocessed': 0, 'train_data_preprocessed': 0, 'vinvl_feature_preprocessed': 0}}, 'cuda': 0, 'data_loader': {'additional': {'max_decoder_source_length': 512, 'max_source_length': 512, 'max_target_length': 10}, 'dataset_modules': {'module_dict': {'LoadClipEmbeddings': {'config': {'clip_embeddings': {'train': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_train2014.pkl', 'val': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_val2014.pkl'}, 'qformer_embeddings': {'train': '../data/ok-vqa/pre-extracted_features/blip2_head_embeddings/coco_hgface_qformer_train2014.pkl', 'val': '../data/ok-vqa/pre-extracted_features/blip2_head_embeddings/coco_hgface_qformer_val2014.pkl'}, 'raw_pixels': {'train': '../data/ok-vqa/pre-extracted_features/raw-pixels/okvqa_raw_pixels_train2014.pkl', 'val': '../data/ok-vqa/pre-extracted_features/raw-pixels/okvqa_raw_pixels_val2014.pkl'}}, 'option': 'default', 'type': 'EmbeddingInput'}, 'LoadGoogleOCRFeatures': {'config': {'combine_with_vinvl': True, 'test': '../data/ok-vqa/pre-extracted_features/OCR/valid', 'train': '../data/ok-vqa/pre-extracted_features/OCR/train'}, 'option': 'default', 'type': 'LoadGoogleOCRFeatures'}, 'LoadGoogleSearchAnnotations': {'config': {'annotations_path': {'test': '../data/ok-vqa/pre-extracted_features/passages/retriever_test.json', 'train': '../data/ok-vqa/pre-extracted_features/passages/retriever_train.json', 'valid': '../data/ok-vqa/pre-extracted_features/passages/retriever_testdev.json'}}, 'option': 'default', 'type': 'LoadGoogleSearchAnnotations'}, 'LoadGoogleSearchPassageData': {'config': {'passage_data_path': {'full': '../data/ok-vqa/pre-extracted_features/passages/okvqa_full_corpus.csv', 'train': '../data/ok-vqa/pre-extracted_features/passages/okvqa_train_corpus.csv'}, 'use_full_split': True}, 'option': 'default', 'type': 'LoadGoogleSearchPassageData'}, 'LoadOKVQAData': {'config': {'image_data_path': {'test': '../data/ok-vqa/val2014', 'train': '../data/ok-vqa/train2014'}, 'vqa_data_path': {'annotation_files': {'test': '../data/ok-vqa/mscoco_val2014_annotations.json', 'train': '../data/ok-vqa/mscoco_train2014_annotations.json'}, 'question_files': {'test': '../data/ok-vqa/OpenEnded_mscoco_val2014_questions.json', 'train': '../data/ok-vqa/OpenEnded_mscoco_train2014_questions.json'}}}, 'option': 'default', 'type': 'LoadOKVQAData'}, 'LoadOscarCaptionFeatures': {'config': {'test': '../data/ok-vqa/pre-extracted_features/captions/test_predictions.json', 'train': '../data/ok-vqa/pre-extracted_features/captions/train_predictions.json', 'valid': '../data/ok-vqa/pre-extracted_features/captions/valid_predictions.json'}, 'option': 'default', 'type': 'LoadOscarCaptionFeatures'}, 'LoadPretrainedDPROutputForGoogleSearchPassage': {'config': {'pretrained_dpr_outputs': {'test': '../Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/test/test_evaluation/test_predictions.json', 'train': '../Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/test/test_evaluation/train_predictions.json'}}, 'option': 'none', 'type': 'LoadPretrainedDPROutputForGoogleSearchPassage'}, 'LoadVinVLFeatures': {'config': {'test': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_testset_full/inference/vinvl_vg_x152c4/predictions.tsv', 'train': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_trainset_full/inference/vinvl_vg_x152c4/predictions.tsv'}, 'option': 'default', 'type': 'LoadVinVLFeatures'}}, 'module_list': ['LoadVinVLFeatures', 'LoadGoogleOCRFeatures', 'LoadOscarCaptionFeatures', 'LoadOKVQAData', 'LoadClipEmbeddings']}, 'dataset_type': 'OKVQADataset', 'dummy_dataloader': 0, 'type': 'DataLoaderOKVQA'}, 'experiment_name': 'OKVQA_NoDPR-flanT5-qf.22980605', 'gpu_device': 0, 'ignore_pretrained_weights': [], 'metrics': [{'name': 'compute_okvqa_scores'}], 'model_config': {'ConfigClass': 'T5Config', 'GeneratorModelClass': 'T5ForConditionalGeneration', 'LoadPretrainMLP': 0, 'ModelClass': 'PrefixModel', 'ModelVersion': 'google/flan-t5-large', 'SPECIAL_TOKENS': {'additional_special_tokens': ['<BOQ>', '<EOQ>'], 'bos_token': '<PAD>', 'pad_token': '<PAD>'}, 'TokenizerClass': 'T5Tokenizer', 'TokenizerModelVersion': 'google/flan-t5-large', 'UsePrefixEmb': 0.5, 'UseQformerEmb': 1, 'UseRawPixels': 0, 'base_model': 'T5', 'decoder_input_modules': {'module_list': [], 'postprocess_module_list': []}, 'input_modules': {'module_list': [{'option': 'default', 'separation_tokens': {'end': '<EOQ>', 'start': '<BOQ>'}, 'type': 'QuestionInput'}, {'option': 'default', 'type': 'EmbeddingInput'}], 'postprocess_module_list': [{'option': 'default', 'type': 'PostProcessInputTokenization'}, {'option': 'default', 'type': 'PostProcessClipEmbeddings'}]}, 'loss_ratio': {'additional_loss': 0, 'nll_loss': 1, 'rag_loss': 0}, 'modules': [], 'output_modules': {'module_list': [{'option': 'default', 'type': 'GenerationOutput'}], 'postprocess_module_list': [{'option': 'default', 'type': 'PostProcessOutputTokenization'}]}, 'pretrained': 1, 'rag_modules': {'module_list': []}, 'LoadPretrainedMLP': 1}, 'platform_type': 'pytorch', 'seed': 2021, 'test': {'additional': {'multiprocessing': 4}, 'batch_size': 32, 'evaluation_name': 'test_evaluation', 'load_best_model': 0, 'load_epoch': -1, 'load_model_path': '', 'num_evaluation': 0}, 'train': {'MLP_lr': 0.0003, 'adam_epsilon': 1e-08, 'additional': {'gradient_accumulation_steps': 16, 'gradient_clipping': 0, 'plugins': [], 'save_top_k': 1, 'save_top_k_metric': 'test/accuracy_overall', 'save_top_k_mode': 'max', 'warmup_steps': 0}, 'batch_size': 2, 'epochs': 10, 'load_best_model': 0, 'load_epoch': -1, 'load_model_path': '/home/xl544/rds/hpc-work/MLMI8_2022_VQA/Experiments/Pretrain_qformer_mlp_con_cap.22972651/train/saved_model/model_09.ckpt', 'lr': 6e-05, 'retriever_lr': 1e-05, 'save_interval': 1, 'scheduler': 'linear', 'type': 'T5ExecutorWithPrefix'}, 'valid': {'additional': {}, 'batch_size': 32, 'break_interval': 3000, 'step_size': 0.5}, 'reset': False, 'mode': 'train', 'log_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_NoDPR-flanT5-qf.22980605/train', 'experiment_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_NoDPR-flanT5-qf.22980605', 'saved_model_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_NoDPR-flanT5-qf.22980605/train/saved_model', 'imgs_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_NoDPR-flanT5-qf.22980605/train/imgs', 'tensorboard_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data_TB/tb_logs/OKVQA_NoDPR-flanT5-qf.22980605', 'args': {'config': '../configs/okvqa/T5_NoDPR_prefix_only.jsonnet', 'DATA_FOLDER': '', 'EXPERIMENT_FOLDER': '', 'mode': 'train', 'reset': False, 'experiment_name': 'OKVQA_NoDPR-flanT5-qf.22980605', 'tags': [], 'modules': [], 'log_prediction_tables': False, 'test_batch_size': -1, 'test_evaluation_name': '', 'logger': True, 'checkpoint_callback': None, 'enable_checkpointing': True, 'default_root_dir': None, 'gradient_clip_val': None, 'gradient_clip_algorithm': None, 'process_position': 0, 'num_nodes': 1, 'num_processes': None, 'devices': '1', 'gpus': None, 'auto_select_gpus': False, 'tpu_cores': None, 'ipus': None, 'log_gpu_memory': None, 'progress_bar_refresh_rate': None, 'enable_progress_bar': True, 'overfit_batches': 0.0, 'track_grad_norm': -1, 'check_val_every_n_epoch': 1, 'fast_dev_run': False, 'accumulate_grad_batches': None, 'max_epochs': None, 'min_epochs': None, 'max_steps': -1, 'min_steps': None, 'max_time': None, 'limit_train_batches': None, 'limit_val_batches': None, 'limit_test_batches': None, 'limit_predict_batches': None, 'val_check_interval': None, 'flush_logs_every_n_steps': None, 'log_every_n_steps': 50, 'accelerator': 'auto', 'strategy': None, 'sync_batchnorm': False, 'precision': 32, 'enable_model_summary': True, 'weights_summary': 'top', 'weights_save_path': None, 'num_sanity_val_steps': 2, 'resume_from_checkpoint': None, 'profiler': None, 'benchmark': None, 'deterministic': None, 'reload_dataloaders_every_n_epochs': 0, 'auto_lr_find': False, 'replace_sampler_ddp': True, 'detect_anomaly': False, 'auto_scale_batch_size': False, 'prepare_data_per_node': None, 'plugins': None, 'amp_backend': 'native', 'amp_level': None, 'move_metrics_to_cpu': False, 'multiple_trainloader_mode': 'max_size_cycle', 'stochastic_weight_avg': False, 'terminate_on_nan': None, 'opts': ['train.epochs=10', 'train.batch_size=2', 'valid.step_size=0.5', 'valid.batch_size=32', 'train.additional.gradient_accumulation_steps=16', 'train.lr=0.00006', 'train.MLP_lr=0.0003', 'train.scheduler=linear', 'train.load_model_path=/home/xl544/rds/hpc-work/MLMI8_2022_VQA/Experiments/Pretrain_qformer_mlp_con_cap.22972651/train/saved_model/model_09.ckpt', 'model_config.UsePrefixEmb=0.5', 'model_config.UseQformerEmb=1', 'model_config.LoadPretrainedMLP=1', 'model_config.TokenizerModelVersion=google/flan-t5-large', 'model_config.ModelVersion=google/flan-t5-large']}}
['/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_NoDPR-flanT5-qf.22980605/train', '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_NoDPR-flanT5-qf.22980605/train/saved_model', '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_NoDPR-flanT5-qf.22980605/train/imgs', '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data_TB/tb_logs/OKVQA_NoDPR-flanT5-qf.22980605']
{'DATA_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data',
 'EXPERIMENT_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments',
 'TENSORBOARD_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data_TB/tb_logs',
 'WANDB': {'entity': 'xl544',
           'name': 'OKVQA_NoDPR-flanT5-qf.22980605',
           'project': 'RAVQA',
           'tags': ['OKVQA', 'T5']},
 'args': {'DATA_FOLDER': '',
          'EXPERIMENT_FOLDER': '',
          'accelerator': 'auto',
          'accumulate_grad_batches': None,
          'amp_backend': 'native',
          'amp_level': None,
          'auto_lr_find': False,
          'auto_scale_batch_size': False,
          'auto_select_gpus': False,
          'benchmark': None,
          'check_val_every_n_epoch': 1,
          'checkpoint_callback': None,
          'config': '../configs/okvqa/T5_NoDPR_prefix_only.jsonnet',
          'default_root_dir': None,
          'detect_anomaly': False,
          'deterministic': None,
          'devices': '1',
          'enable_checkpointing': True,
          'enable_model_summary': True,
          'enable_progress_bar': True,
          'experiment_name': 'OKVQA_NoDPR-flanT5-qf.22980605',
          'fast_dev_run': False,
          'flush_logs_every_n_steps': None,
          'gpus': None,
          'gradient_clip_algorithm': None,
          'gradient_clip_val': None,
          'ipus': None,
          'limit_predict_batches': None,
          'limit_test_batches': None,
          'limit_train_batches': None,
          'limit_val_batches': None,
          'log_every_n_steps': 50,
          'log_gpu_memory': None,
          'log_prediction_tables': False,
          'logger': True,
          'max_epochs': None,
          'max_steps': -1,
          'max_time': None,
          'min_epochs': None,
          'min_steps': None,
          'mode': 'train',
          'modules': [],
          'move_metrics_to_cpu': False,
          'multiple_trainloader_mode': 'max_size_cycle',
          'num_nodes': 1,
          'num_processes': None,
          'num_sanity_val_steps': 2,
          'opts': ['train.epochs=10',
                   'train.batch_size=2',
                   'valid.step_size=0.5',
                   'valid.batch_size=32',
                   'train.additional.gradient_accumulation_steps=16',
                   'train.lr=0.00006',
                   'train.MLP_lr=0.0003',
                   'train.scheduler=linear',
                   'train.load_model_path=/home/xl544/rds/hpc-work/MLMI8_2022_VQA/Experiments/Pretrain_qformer_mlp_con_cap.22972651/train/saved_model/model_09.ckpt',
                   'model_config.UsePrefixEmb=0.5',
                   'model_config.UseQformerEmb=1',
                   'model_config.LoadPretrainedMLP=1',
                   'model_config.TokenizerModelVersion=google/flan-t5-large',
                   'model_config.ModelVersion=google/flan-t5-large'],
          'overfit_batches': 0.0,
          'plugins': None,
          'precision': 32,
          'prepare_data_per_node': None,
          'process_position': 0,
          'profiler': None,
          'progress_bar_refresh_rate': None,
          'reload_dataloaders_every_n_epochs': 0,
          'replace_sampler_ddp': True,
          'reset': False,
          'resume_from_checkpoint': None,
          'stochastic_weight_avg': False,
          'strategy': None,
          'sync_batchnorm': False,
          'tags': [],
          'terminate_on_nan': None,
          'test_batch_size': -1,
          'test_evaluation_name': '',
          'tpu_cores': None,
          'track_grad_norm': -1,
          'val_check_interval': None,
          'weights_save_path': None,
          'weights_summary': 'top'},
 'cache': {'default_folder': '../data/ok-vqa/cache',
           'regenerate': {'clip_embeddings': 0,
                          'ocr_feature_preprocessed': 0,
                          'qformer_embeddings': 0,
                          'test_data_preprocessed': 0,
                          'train_data_preprocessed': 0,
                          'vinvl_feature_preprocessed': 0}},
 'cuda': 0,
 'data_loader': {'additional': {'max_decoder_source_length': 512,
                                'max_source_length': 512,
                                'max_target_length': 10},
                 'dataset_modules': {'module_dict': {'LoadClipEmbeddings': {'config': {'clip_embeddings': {'train': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_train2014.pkl',
                                                                                                           'val': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_val2014.pkl'},
                                                                                       'qformer_embeddings': {'train': '../data/ok-vqa/pre-extracted_features/blip2_head_embeddings/coco_hgface_qformer_train2014.pkl',
                                                                                                              'val': '../data/ok-vqa/pre-extracted_features/blip2_head_embeddings/coco_hgface_qformer_val2014.pkl'},
                                                                                       'raw_pixels': {'train': '../data/ok-vqa/pre-extracted_features/raw-pixels/okvqa_raw_pixels_train2014.pkl',
                                                                                                      'val': '../data/ok-vqa/pre-extracted_features/raw-pixels/okvqa_raw_pixels_val2014.pkl'}},
                                                                            'option': 'default',
                                                                            'type': 'EmbeddingInput'},
                                                     'LoadGoogleOCRFeatures': {'config': {'combine_with_vinvl': True,
                                                                                          'test': '../data/ok-vqa/pre-extracted_features/OCR/valid',
                                                                                          'train': '../data/ok-vqa/pre-extracted_features/OCR/train'},
                                                                               'option': 'default',
                                                                               'type': 'LoadGoogleOCRFeatures'},
                                                     'LoadGoogleSearchAnnotations': {'config': {'annotations_path': {'test': '../data/ok-vqa/pre-extracted_features/passages/retriever_test.json',
                                                                                                                     'train': '../data/ok-vqa/pre-extracted_features/passages/retriever_train.json',
                                                                                                                     'valid': '../data/ok-vqa/pre-extracted_features/passages/retriever_testdev.json'}},
                                                                                     'option': 'default',
                                                                                     'type': 'LoadGoogleSearchAnnotations'},
                                                     'LoadGoogleSearchPassageData': {'config': {'passage_data_path': {'full': '../data/ok-vqa/pre-extracted_features/passages/okvqa_full_corpus.csv',
                                                                                                                      'train': '../data/ok-vqa/pre-extracted_features/passages/okvqa_train_corpus.csv'},
                                                                                                'use_full_split': True},
                                                                                     'option': 'default',
                                                                                     'type': 'LoadGoogleSearchPassageData'},
                                                     'LoadOKVQAData': {'config': {'image_data_path': {'test': '../data/ok-vqa/val2014',
                                                                                                      'train': '../data/ok-vqa/train2014'},
                                                                                  'vqa_data_path': {'annotation_files': {'test': '../data/ok-vqa/mscoco_val2014_annotations.json',
                                                                                                                         'train': '../data/ok-vqa/mscoco_train2014_annotations.json'},
                                                                                                    'question_files': {'test': '../data/ok-vqa/OpenEnded_mscoco_val2014_questions.json',
                                                                                                                       'train': '../data/ok-vqa/OpenEnded_mscoco_train2014_questions.json'}}},
                                                                       'option': 'default',
                                                                       'type': 'LoadOKVQAData'},
                                                     'LoadOscarCaptionFeatures': {'config': {'test': '../data/ok-vqa/pre-extracted_features/captions/test_predictions.json',
                                                                                             'train': '../data/ok-vqa/pre-extracted_features/captions/train_predictions.json',
                                                                                             'valid': '../data/ok-vqa/pre-extracted_features/captions/valid_predictions.json'},
                                                                                  'option': 'default',
                                                                                  'type': 'LoadOscarCaptionFeatures'},
                                                     'LoadPretrainedDPROutputForGoogleSearchPassage': {'config': {'pretrained_dpr_outputs': {'test': '../Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/test/test_evaluation/test_predictions.json',
                                                                                                                                             'train': '../Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/test/test_evaluation/train_predictions.json'}},
                                                                                                       'option': 'none',
                                                                                                       'type': 'LoadPretrainedDPROutputForGoogleSearchPassage'},
                                                     'LoadVinVLFeatures': {'config': {'test': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_testset_full/inference/vinvl_vg_x152c4/predictions.tsv',
                                                                                      'train': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_trainset_full/inference/vinvl_vg_x152c4/predictions.tsv'},
                                                                           'option': 'default',
                                                                           'type': 'LoadVinVLFeatures'}},
                                     'module_list': ['LoadVinVLFeatures',
                                                     'LoadGoogleOCRFeatures',
                                                     'LoadOscarCaptionFeatures',
                                                     'LoadOKVQAData',
                                                     'LoadClipEmbeddings']},
                 'dataset_type': 'OKVQADataset',
                 'dummy_dataloader': 0,
                 'type': 'DataLoaderOKVQA'},
 'experiment_name': 'OKVQA_NoDPR-flanT5-qf.22980605',
 'experiment_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_NoDPR-flanT5-qf.22980605',
 'gpu_device': 0,
 'ignore_pretrained_weights': [],
 'imgs_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_NoDPR-flanT5-qf.22980605/train/imgs',
 'log_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_NoDPR-flanT5-qf.22980605/train',
 'metrics': [{'name': 'compute_okvqa_scores'}],
 'mode': 'train',
 'model_config': {'ConfigClass': 'T5Config',
                  'GeneratorModelClass': 'T5ForConditionalGeneration',
                  'LoadPretrainMLP': 0,
                  'LoadPretrainedMLP': 1,
                  'ModelClass': 'PrefixModel',
                  'ModelVersion': 'google/flan-t5-large',
                  'SPECIAL_TOKENS': {'additional_special_tokens': ['<BOQ>',
                                                                   '<EOQ>'],
                                     'bos_token': '<PAD>',
                                     'pad_token': '<PAD>'},
                  'TokenizerClass': 'T5Tokenizer',
                  'TokenizerModelVersion': 'google/flan-t5-large',
                  'UsePrefixEmb': 0.5,
                  'UseQformerEmb': 1,
                  'UseRawPixels': 0,
                  'base_model': 'T5',
                  'decoder_input_modules': {'module_list': [],
                                            'postprocess_module_list': []},
                  'input_modules': {'module_list': [{'option': 'default',
                                                     'separation_tokens': {'end': '<EOQ>',
                                                                           'start': '<BOQ>'},
                                                     'type': 'QuestionInput'},
                                                    {'option': 'default',
                                                     'type': 'EmbeddingInput'}],
                                    'postprocess_module_list': [{'option': 'default',
                                                                 'type': 'PostProcessInputTokenization'},
                                                                {'option': 'default',
                                                                 'type': 'PostProcessClipEmbeddings'}]},
                  'loss_ratio': {'additional_loss': 0,
                                 'nll_loss': 1,
                                 'rag_loss': 0},
                  'modules': [],
                  'output_modules': {'module_list': [{'option': 'default',
                                                      'type': 'GenerationOutput'}],
                                     'postprocess_module_list': [{'option': 'default',
                                                                  'type': 'PostProcessOutputTokenization'}]},
                  'pretrained': 1,
                  'rag_modules': {'module_list': []}},
 'platform_type': 'pytorch',
 'reset': False,
 'saved_model_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_NoDPR-flanT5-qf.22980605/train/saved_model',
 'seed': 2021,
 'tensorboard_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data_TB/tb_logs/OKVQA_NoDPR-flanT5-qf.22980605',
 'test': {'additional': {'multiprocessing': 4},
          'batch_size': 32,
          'evaluation_name': 'test_evaluation',
          'load_best_model': 0,
          'load_epoch': -1,
          'load_model_path': '',
          'num_evaluation': 0},
 'train': {'MLP_lr': 0.0003,
           'adam_epsilon': 1e-08,
           'additional': {'gradient_accumulation_steps': 16,
                          'gradient_clipping': 0,
                          'plugins': [],
                          'save_top_k': 1,
                          'save_top_k_metric': 'test/accuracy_overall',
                          'save_top_k_mode': 'max',
                          'warmup_steps': 0},
           'batch_size': 2,
           'epochs': 10,
           'load_best_model': 0,
           'load_epoch': -1,
           'load_model_path': '/home/xl544/rds/hpc-work/MLMI8_2022_VQA/Experiments/Pretrain_qformer_mlp_con_cap.22972651/train/saved_model/model_09.ckpt',
           'lr': 6e-05,
           'retriever_lr': 1e-05,
           'save_interval': 1,
           'scheduler': 'linear',
           'type': 'T5ExecutorWithPrefix'},
 'valid': {'additional': {},
           'batch_size': 32,
           'break_interval': 3000,
           'step_size': 0.5}}
data columns: dict_keys(['vinvl_features'])
data columns: dict_keys(['vinvl_features', 'ocr_features'])
[Data Statistics] Caption features 123287
data columns: dict_keys(['vinvl_features', 'ocr_features', 'caption_features'])
loading VQA annotations and questions into memory...
time elpased:  0:00:00.209574
creating index...
index created!
loading VQA annotations and questions into memory...
time elpased:  0:00:00.127875
creating index...
index created!
creating index...
index created!
year: 2019
version: 1.0
description: This is v1.0 of the OK-VQA dataset.
creating index...
index created!
year: 2019
version: 1.0
description: This is v1.0 of the OK-VQA dataset.
data columns: dict_keys(['vinvl_features', 'ocr_features', 'caption_features', 'okvqa_data', 'vqa_data'])
data columns: dict_keys(['vinvl_features', 'ocr_features', 'caption_features', 'okvqa_data', 'vqa_data', 'clip_embeddings'])

 Using MLP for Qformer
MLP input  768  Output dim  1024
