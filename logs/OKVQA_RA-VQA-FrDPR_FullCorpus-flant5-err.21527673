/home/xl544/.conda/envs/RAVQA/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning
  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')
[38;20m[INFO] - __main__ : Initialization done with the config: {'DATA_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data', 'EXPERIMENT_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments', 'TENSORBOARD_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data_TB/tb_logs', 'WANDB': {'entity': 'xl544', 'project': 'RAVQA', 'tags': ['OKVQA', 'RAG', 'freeze_question_encoder', 'force_existence'], 'name': 'OKVQA_RA-VQA-FrDPR_FullCorpus-flant5.21527673'}, 'cache': {'default_folder': '../data/ok-vqa/cache', 'regenerate': {'clip_embeddings': 0, 'ocr_feature_preprocessed': 0, 'test_data_preprocessed': 0, 'train_data_preprocessed': 0, 'vinvl_feature_preprocessed': 0}}, 'cuda': 0, 'data_loader': {'additional': {'max_decoder_source_length': 512, 'max_source_length': 512, 'max_target_length': 10, 'num_knowledge_passages': 5}, 'dataset_modules': {'module_dict': {'LoadClipEmbeddings': {'config': {'train': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_train2014.pkl', 'val': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_val2014.pkl'}, 'option': 'default', 'type': 'EmbeddingInput'}, 'LoadGoogleOCRFeatures': {'config': {'combine_with_vinvl': True, 'test': '../data/ok-vqa/pre-extracted_features/OCR/valid', 'train': '../data/ok-vqa/pre-extracted_features/OCR/train'}, 'option': 'default', 'type': 'LoadGoogleOCRFeatures'}, 'LoadGoogleSearchAnnotations': {'config': {'annotations_path': {'test': '../data/ok-vqa/pre-extracted_features/passages/retriever_test.json', 'train': '../data/ok-vqa/pre-extracted_features/passages/retriever_train.json', 'valid': '../data/ok-vqa/pre-extracted_features/passages/retriever_testdev.json'}}, 'option': 'default', 'type': 'LoadGoogleSearchAnnotations'}, 'LoadGoogleSearchPassageData': {'config': {'passage_data_path': {'full': '../data/ok-vqa/pre-extracted_features/passages/okvqa_full_corpus.csv', 'train': '../data/ok-vqa/pre-extracted_features/passages/okvqa_train_corpus.csv'}, 'use_full_split': True}, 'option': 'default', 'type': 'LoadGoogleSearchPassageData'}, 'LoadOKVQAData': {'config': {'image_data_path': {'test': '../data/ok-vqa/val2014', 'train': '../data/ok-vqa/train2014'}, 'vqa_data_path': {'annotation_files': {'test': '../data/ok-vqa/mscoco_val2014_annotations.json', 'train': '../data/ok-vqa/mscoco_train2014_annotations.json'}, 'question_files': {'test': '../data/ok-vqa/OpenEnded_mscoco_val2014_questions.json', 'train': '../data/ok-vqa/OpenEnded_mscoco_train2014_questions.json'}}}, 'option': 'default', 'type': 'LoadOKVQAData'}, 'LoadOscarCaptionFeatures': {'config': {'test': '../data/ok-vqa/pre-extracted_features/captions/test_predictions.json', 'train': '../data/ok-vqa/pre-extracted_features/captions/train_predictions.json', 'valid': '../data/ok-vqa/pre-extracted_features/captions/valid_predictions.json'}, 'option': 'default', 'type': 'LoadOscarCaptionFeatures'}, 'LoadPretrainedDPROutputForGoogleSearchPassage': {'config': {'pretrained_dpr_outputs': {'test': '../Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/test/test_evaluation/test_predictions.json', 'train': '../Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/test/test_evaluation/train_predictions.json'}}, 'option': 'none', 'type': 'LoadPretrainedDPROutputForGoogleSearchPassage'}, 'LoadVinVLFeatures': {'config': {'test': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_testset_full/inference/vinvl_vg_x152c4/predictions.tsv', 'train': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_trainset_full/inference/vinvl_vg_x152c4/predictions.tsv'}, 'option': 'default', 'type': 'LoadVinVLFeatures'}}, 'module_list': ['LoadVinVLFeatures', 'LoadGoogleOCRFeatures', 'LoadOscarCaptionFeatures', 'LoadOKVQAData', 'LoadGoogleSearchPassageData', 'LoadClipEmbeddings']}, 'dataset_type': 'OKVQADataset', 'dummy_dataloader': 0, 'index_files': {'index_passages_path': '../data/ok-vqa/pre-extracted_features/faiss/ok-vqa-passages-full-caption-pretrained-NewRun/my_knowledge_dataset', 'index_path': '../data/ok-vqa/pre-extracted_features/faiss/ok-vqa-passages-full-caption-pretrained-NewRun/my_knowledge_dataset_hnsw_index.faiss'}, 'type': 'DataLoaderOKVQAWithKnowledge'}, 'experiment_name': 'OKVQA_RA-VQA-FrDPR_FullCorpus-flant5.21527673', 'gpu_device': 0, 'ignore_pretrained_weights': [], 'metrics': [{'name': 'compute_exact_match'}, {'name': 'compute_retrieval_metrics'}, {'name': 'compute_okvqa_scores'}], 'model_config': {'DECODER_SPECIAL_TOKENS': {'additional_special_tokens': ['<BOV>', '<SOV>', '<EOV>', '<BOQ>', '<EOQ>', '<BOC>', '<EOC>', '<BOK>', '<EOK>'], 'bos_token': '<PAD>', 'pad_token': '<PAD>'}, 'DecoderTokenizerClass': 'T5Tokenizer', 'DecoderTokenizerModelVersion': 'google/flan-t5-large', 'GeneratorConfigClass': 'T5Config', 'GeneratorModelClass': 'T5ForConditionalGeneration', 'GeneratorModelVersion': 'google/flan-t5-large', 'LoadPretrainedMLPWeights': 0, 'ModelClass': 'RagModel', 'PretrainedMLPPath': '', 'QueryEncoderConfigClass': 'DPRConfig', 'QueryEncoderModelClass': 'DPRQuestionEncoder', 'QueryEncoderModelVersion': '../Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/train/saved_model/epoch6/query_encoder', 'RAVQA_loss_type': 'Approach6', 'SPECIAL_TOKENS': {'additional_special_tokens': ['<BOV>', '<SOV>', '<EOV>', '<BOQ>', '<EOQ>', '<BOC>', '<EOC>', '<BOK>', '<EOK>']}, 'TokenizerClass': 'DPRQuestionEncoderTokenizer', 'TokenizerModelVersion': 'facebook/dpr-question_encoder-single-nq-base', 'UsePrefixEmb': 0, 'base_model': 'RAG', 'decoder_input_modules': {'module_list': [], 'postprocess_module_list': []}, 'input_modules': {'module_list': [{'option': 'default', 'separation_tokens': {'end': '<EOQ>', 'start': '<BOQ>'}, 'type': 'QuestionInput'}, {'option': 'caption', 'separation_tokens': {'end': '<EOC>', 'start': '<BOC>'}, 'type': 'TextBasedVisionInput'}, {'attribute_max': 3, 'attribute_thres': 0.05, 'object_max': 40, 'ocr': 1, 'option': 'object', 'separation_tokens': {'end': '<EOV>', 'sep': '<SOV>', 'start': '<BOV>'}, 'type': 'TextBasedVisionInput'}, {'option': 'default', 'type': 'EmbeddingInput'}], 'postprocess_module_list': [{'option': 'default', 'type': 'PostProcessInputTokenization'}, {'option': 'default', 'type': 'PostProcessClipEmbeddings'}]}, 'loss_ratio': {'additional_loss': 0, 'nll_loss': 1, 'rag_loss': 0, 'retrieval_pseudo_loss': 0}, 'modules': ['freeze_question_encoder', 'force_existence'], 'output_modules': {'module_list': [{'option': 'default', 'type': 'GenerationOutput'}], 'postprocess_module_list': [{'option': 'default', 'type': 'PostProcessOutputTokenization'}]}, 'pretrained': 1, 'rag_modules': {'module_list': []}}, 'platform_type': 'pytorch', 'seed': 2021, 'test': {'additional': {'multiprocessing': 4}, 'batch_size': 32, 'evaluation_name': 'test_evaluation', 'load_best_model': 0, 'load_epoch': -1, 'load_model_path': '', 'num_evaluation': 0}, 'train': {'adam_epsilon': 1e-08, 'additional': {'gradient_accumulation_steps': 16, 'gradient_clipping': 0, 'plugins': [], 'save_top_k': 1, 'save_top_k_metric': 'test/accuracy_overall', 'save_top_k_mode': 'max', 'warmup_steps': 0}, 'batch_size': 2, 'epochs': 10, 'load_best_model': 0, 'load_epoch': -1, 'load_model_path': '', 'lr': 6e-05, 'retriever_lr': 1e-05, 'save_interval': 1, 'scheduler': 'linear', 'type': 'RagExecutor'}, 'valid': {'additional': {}, 'batch_size': 32, 'break_interval': 3000, 'step_size': 1}, 'reset': False, 'mode': 'train', 'log_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-FrDPR_FullCorpus-flant5.21527673/train', 'experiment_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-FrDPR_FullCorpus-flant5.21527673', 'saved_model_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-FrDPR_FullCorpus-flant5.21527673/train/saved_model', 'imgs_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-FrDPR_FullCorpus-flant5.21527673/train/imgs', 'tensorboard_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data_TB/tb_logs/OKVQA_RA-VQA-FrDPR_FullCorpus-flant5.21527673', 'args': {'config': '../configs/okvqa/RAVQA.jsonnet', 'DATA_FOLDER': '', 'EXPERIMENT_FOLDER': '', 'mode': 'train', 'reset': False, 'experiment_name': 'OKVQA_RA-VQA-FrDPR_FullCorpus-flant5.21527673', 'tags': [], 'modules': ['freeze_question_encoder', 'force_existence'], 'log_prediction_tables': False, 'test_batch_size': -1, 'test_evaluation_name': '', 'logger': True, 'checkpoint_callback': None, 'enable_checkpointing': True, 'default_root_dir': None, 'gradient_clip_val': None, 'gradient_clip_algorithm': None, 'process_position': 0, 'num_nodes': 1, 'num_processes': None, 'devices': '1', 'gpus': None, 'auto_select_gpus': False, 'tpu_cores': None, 'ipus': None, 'log_gpu_memory': None, 'progress_bar_refresh_rate': None, 'enable_progress_bar': True, 'overfit_batches': 0.0, 'track_grad_norm': -1, 'check_val_every_n_epoch': 1, 'fast_dev_run': False, 'accumulate_grad_batches': None, 'max_epochs': None, 'min_epochs': None, 'max_steps': -1, 'min_steps': None, 'max_time': None, 'limit_train_batches': None, 'limit_val_batches': None, 'limit_test_batches': None, 'limit_predict_batches': None, 'val_check_interval': None, 'flush_logs_every_n_steps': None, 'log_every_n_steps': 50, 'accelerator': 'auto', 'strategy': None, 'sync_batchnorm': False, 'precision': 32, 'enable_model_summary': True, 'weights_summary': 'top', 'weights_save_path': None, 'num_sanity_val_steps': 2, 'resume_from_checkpoint': None, 'profiler': None, 'benchmark': None, 'deterministic': None, 'reload_dataloaders_every_n_epochs': 0, 'auto_lr_find': False, 'replace_sampler_ddp': True, 'detect_anomaly': False, 'auto_scale_batch_size': False, 'prepare_data_per_node': None, 'plugins': None, 'amp_backend': 'native', 'amp_level': None, 'move_metrics_to_cpu': False, 'multiple_trainloader_mode': 'max_size_cycle', 'stochastic_weight_avg': False, 'terminate_on_nan': None, 'opts': ['train.epochs=10', 'train.batch_size=2', 'valid.step_size=1', 'valid.batch_size=32', 'train.additional.gradient_accumulation_steps=16', 'train.lr=0.00006', 'train.retriever_lr=0.00001', 'train.scheduler=linear', 'data_loader.additional.num_knowledge_passages=5', 'model_config.DecoderTokenizerModelVersion=google/flan-t5-large', 'model_config.GeneratorModelVersion=google/flan-t5-large', 'model_config.UsePrefixEmb=0']}}[0m
Global seed set to 2021
[38;20m[INFO] - __main__ : All seeds have been set to 2021[0m
[38;20m[INFO] - __main__ : init wandb logger with the following settings: {'entity': 'xl544', 'project': 'RAVQA', 'tags': ['OKVQA', 'RAG', 'freeze_question_encoder', 'force_existence'], 'name': 'OKVQA_RA-VQA-FrDPR_FullCorpus-flant5.21527673'}[0m
wandb: Currently logged in as: xl544. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.2
wandb: Run data is saved locally in /rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/src/wandb/run-20230531_203421-siz7pkyl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run OKVQA_RA-VQA-FrDPR_FullCorpus-flant5.21527673
wandb: ⭐️ View project at https://wandb.ai/xl544/RAVQA
wandb: 🚀 View run at https://wandb.ai/xl544/RAVQA/runs/siz7pkyl
Multiprocessing is handled by SLURM.
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[38;20m[INFO] - __main__ : arguments passed to trainer: Namespace(DATA_FOLDER='', EXPERIMENT_FOLDER='', accelerator='auto', accumulate_grad_batches=None, amp_backend='native', amp_level=None, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, benchmark=None, check_val_every_n_epoch=1, checkpoint_callback=None, config='../configs/okvqa/RAVQA.jsonnet', default_root_dir=None, detect_anomaly=False, deterministic=None, devices='1', enable_checkpointing=True, enable_model_summary=True, enable_progress_bar=True, experiment_name='OKVQA_RA-VQA-FrDPR_FullCorpus-flant5.21527673', fast_dev_run=False, flush_logs_every_n_steps=None, gpus=None, gradient_clip_algorithm=None, gradient_clip_val=None, ipus=None, limit_predict_batches=None, limit_test_batches=None, limit_train_batches=None, limit_val_batches=None, log_every_n_steps=50, log_gpu_memory=None, log_prediction_tables=False, logger=True, max_epochs=None, max_steps=-1, max_time=None, min_epochs=None, min_steps=None, mode='train', modules=['freeze_question_encoder', 'force_existence'], move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=None, num_sanity_val_steps=2, opts=['train.epochs=10', 'train.batch_size=2', 'valid.step_size=1', 'valid.batch_size=32', 'train.additional.gradient_accumulation_steps=16', 'train.lr=0.00006', 'train.retriever_lr=0.00001', 'train.scheduler=linear', 'data_loader.additional.num_knowledge_passages=5', 'model_config.DecoderTokenizerModelVersion=google/flan-t5-large', 'model_config.GeneratorModelVersion=google/flan-t5-large', 'model_config.UsePrefixEmb=0'], overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=None, process_position=0, profiler=None, progress_bar_refresh_rate=None, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, reset=False, resume_from_checkpoint=None, stochastic_weight_avg=False, strategy=None, sync_batchnorm=False, tags=[], terminate_on_nan=None, test_batch_size=-1, test_evaluation_name='', tpu_cores=None, track_grad_norm=-1, val_check_interval=None, weights_save_path=None, weights_summary='top')[0m
[38;20m[INFO] - __main__ : additional arguments passed to trainer: {'accumulate_grad_batches': 16, 'default_root_dir': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-FrDPR_FullCorpus-flant5.21527673/train/saved_model', 'max_epochs': 10, 'logger': [<pytorch_lightning.loggers.tensorboard.TensorBoardLogger object at 0x146b6a85b880>, <pytorch_lightning.loggers.wandb.WandbLogger object at 0x146a90a51340>, <utils.metrics_log_callback.MetricsHistoryLogger object at 0x146a9006e1f0>], 'callbacks': [<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x146a90a5ea90>, <pytorch_lightning.callbacks.progress.tqdm_progress.TQDMProgressBar object at 0x146a9009b880>, <pytorch_lightning.callbacks.model_summary.ModelSummary object at 0x146a9009b1f0>, <pytorch_lightning.callbacks.gradient_accumulation_scheduler.GradientAccumulationScheduler object at 0x146a9009bc70>], 'plugins': [], 'log_every_n_steps': 10}[0m
[33;20m[WARNING] - __main__ : No checkpoint exists from '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-FrDPR_FullCorpus-flant5.21527673/train/saved_model/last.ckpt'. Skipping...[0m
[38;20m[INFO] - __main__ : **First time to train**[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'test': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_testset_full/inference/vinvl_vg_x152c4/predictions.tsv', 'train': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_trainset_full/inference/vinvl_vg_x152c4/predictions.tsv'}, 'option': 'default', 'type': 'LoadVinVLFeatures'}[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/vinvl_feature_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data Statistics] VinVL features 14031[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'combine_with_vinvl': True, 'test': '../data/ok-vqa/pre-extracted_features/OCR/valid', 'train': '../data/ok-vqa/pre-extracted_features/OCR/train'}, 'option': 'default', 'type': 'LoadGoogleOCRFeatures'}[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/ocr_feature_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data Statistics] OCR features 14031, 5462 has annotations.[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : OCR feature detected in VinVL feature dict...skipping..[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'test': '../data/ok-vqa/pre-extracted_features/captions/test_predictions.json', 'train': '../data/ok-vqa/pre-extracted_features/captions/train_predictions.json', 'valid': '../data/ok-vqa/pre-extracted_features/captions/valid_predictions.json'}, 'option': 'default', 'type': 'LoadOscarCaptionFeatures'}[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'image_data_path': {'test': '../data/ok-vqa/val2014', 'train': '../data/ok-vqa/train2014'}, 'vqa_data_path': {'annotation_files': {'test': '../data/ok-vqa/mscoco_val2014_annotations.json', 'train': '../data/ok-vqa/mscoco_train2014_annotations.json'}, 'question_files': {'test': '../data/ok-vqa/OpenEnded_mscoco_val2014_questions.json', 'train': '../data/ok-vqa/OpenEnded_mscoco_train2014_questions.json'}}}, 'option': 'default', 'type': 'LoadOKVQAData'}[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/train_data_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data statistics] split: train  entries: 9009[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/test_data_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data statistics] split: test  entries: 5046[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'passage_data_path': {'full': '../data/ok-vqa/pre-extracted_features/passages/okvqa_full_corpus.csv', 'train': '../data/ok-vqa/pre-extracted_features/passages/okvqa_train_corpus.csv'}, 'use_full_split': True}, 'option': 'default', 'type': 'LoadGoogleSearchPassageData'}[0m
  0%|          | 0/168380 [00:00<?, ?it/s] 10%|▉         | 16723/168380 [00:00<00:00, 167214.21it/s] 20%|█▉        | 33445/168380 [00:00<00:00, 163907.87it/s] 30%|██▉       | 49890/168380 [00:00<00:00, 164141.66it/s] 39%|███▉      | 66307/168380 [00:00<00:00, 164035.20it/s] 49%|████▉     | 82792/168380 [00:00<00:00, 164321.51it/s] 59%|█████▉    | 99264/168380 [00:00<00:00, 164453.03it/s] 69%|██████▉   | 116369/168380 [00:00<00:00, 166602.86it/s] 80%|████████  | 135199/168380 [00:00<00:00, 173498.95it/s] 92%|█████████▏| 154251/168380 [00:00<00:00, 178813.46it/s]100%|█████████▉| 168307/168380 [00:00<00:00, 172320.03it/s]
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'train': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_train2014.pkl', 'val': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_val2014.pkl'}, 'option': 'default', 'type': 'EmbeddingInput'}[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/clip_embeddings.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data Statistics] CLIP embeddings 123287[0m
[38;20m[INFO] - data_loader_manager.datasets.okvqa_datasets : initialising OKVQADataset...[0m
[38;20m[INFO] - data_loader_manager.datasets.okvqa_datasets : initialising OKVQADataset...[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa_with_knowledge : [Data Statistics]: training data loader: 4505;  test data loader: 158[0m
[38;20m[INFO] - torch.distributed.nn.jit.instantiator : Created a temporary directory at /tmp/tmptbhiwz35[0m
[38;20m[INFO] - torch.distributed.nn.jit.instantiator : Writing /tmp/tmptbhiwz35/_remote_module_non_scriptable.py[0m
[38;20m[INFO] - trainers.base_executor : Initializing RagExecutor...[0m
[38;20m[INFO] - __main__ : config file was successfully saved to /rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-FrDPR_FullCorpus-flant5.21527673 for future use.[0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[38;20m[INFO] - trainers.rag_executor : using different learning rate for retriever[0m
[38;20m[INFO] - trainers.rag_executor : #params: 558   lr: 6e-05[0m
[38;20m[INFO] - trainers.rag_executor : #params: 0   lr: 1e-05[0m
Loading `train_dataloader` to estimate number of stepping batches.
/home/xl544/.conda/envs/RAVQA/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(

  | Name  | Type     | Params
-----------------------------------
0 | model | RagModel | 892 M 
-----------------------------------
783 M     Trainable params
109 M     Non-trainable params
892 M     Total params
3,570.409 Total estimated model params size (MB)
Missing logger folder: /rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data_TB/tb_logs/OKVQA_RA-VQA-FrDPR_FullCorpus-flant5.21527673/OKVQA_RA-VQA-FrDPR_FullCorpus-flant5.21527673
SLURM auto-requeueing enabled. Setting signal handlers.
/home/xl544/.conda/envs/RAVQA/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Failed to compute OKVQA scores: Results do not correspond to current VQA set. Either the results do not have predictions for all question ids in annotation file or there is atleast one question id that does not belong to the question ids in the annotation file.This could be due to the fact that OKVQA parser requires all questions to evaluatethe accuracy. Ignore this error if this is the sanity check.[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [sanity_check]: {'test/exact_match_at_1': 0.046875, 'test/exact_match_at_2': 0.078125, 'test/exact_match_at_3': 0.109375, 'test/exact_match_at_4': 0.109375, 'test/exact_match_at_5': 0.109375, 'test/recall_at_5': 0.796875, 'test/precision_at_5': 0.515625, 'test/gold_precision_at_5': 0.353125, 'test/gold_recall_at_5': 0.671875, 'test/successful_hit': 0.025, 'test/successful_no_hit': 0.003125, 'test/failed_hit': 0.5, 'test/failed_no_hit': 0.471875, 'test/selected_successful_hit': 0.046875, 'test/selected_successful_no_hit': 0.0, 'test/selected_failed_hit': 0.625, 'test/selected_failed_no_hit': 0.328125, 'test/n_retrieved_docs': 5, 'test/epoch': 0}[0m
[33;20m[WARNING] - root : Sanity check mode, not saving to loggers.[0m
/home/xl544/.conda/envs/RAVQA/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/home/xl544/.conda/envs/RAVQA/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5420134760206103, 'test/exact_match_at_2': 0.6692429647245343, 'test/exact_match_at_3': 0.7094728497820055, 'test/exact_match_at_4': 0.7219579865239794, 'test/exact_match_at_5': 0.724534284581847, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37292905271502186, 'test/gold_recall_at_5': 0.667063020214031, 'test/successful_hit': 0.36008719778042014, 'test/successful_no_hit': 0.1415378517637733, 'test/failed_hit': 0.33828775267538647, 'test/failed_no_hit': 0.16008719778042013, 'test/selected_successful_hit': 0.4349980182322632, 'test/selected_successful_no_hit': 0.1070154577883472, 'test/selected_failed_hit': 0.3640507332540626, 'test/selected_failed_no_hit': 0.093935790725327, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 49.74, 'test/accuracy_QuestionType_one': 46.47, 'test/accuracy_QuestionType_eight': 48.14, 'test/accuracy_QuestionType_other': 51.34, 'test/accuracy_QuestionType_seven': 49.44, 'test/accuracy_QuestionType_four': 55.27, 'test/accuracy_QuestionType_five': 50.78, 'test/accuracy_QuestionType_three': 48.79, 'test/accuracy_QuestionType_nine': 38.57, 'test/accuracy_QuestionType_ten': 49.46, 'test/accuracy_QuestionType_two': 52.09, 'test/accuracy_QuestionType_six': 50.92, 'test/accuracy_AnswerType_other': 49.74, 'test/epoch': 0}[0m
Epoch 0, global step 282: 'test/accuracy_overall' reached 49.74000 (best 49.74000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-FrDPR_FullCorpus-flant5.21527673/train/saved_model/model_0.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5640110978993262, 'test/exact_match_at_2': 0.6924296472453428, 'test/exact_match_at_3': 0.7294887039239001, 'test/exact_match_at_4': 0.7427665477606025, 'test/exact_match_at_5': 0.7457391993658343, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37292905271502186, 'test/gold_recall_at_5': 0.667063020214031, 'test/successful_hit': 0.3682917162108601, 'test/successful_no_hit': 0.152239397542608, 'test/failed_hit': 0.31969877130400315, 'test/failed_no_hit': 0.15977011494252874, 'test/selected_successful_hit': 0.44946492271105826, 'test/selected_successful_no_hit': 0.11454617518826793, 'test/selected_failed_hit': 0.34244946492271106, 'test/selected_failed_no_hit': 0.09353943717796274, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 51.8, 'test/accuracy_QuestionType_one': 49.25, 'test/accuracy_QuestionType_eight': 51.09, 'test/accuracy_QuestionType_other': 51.6, 'test/accuracy_QuestionType_seven': 51.54, 'test/accuracy_QuestionType_four': 58.02, 'test/accuracy_QuestionType_five': 51.53, 'test/accuracy_QuestionType_three': 52.48, 'test/accuracy_QuestionType_nine': 39.76, 'test/accuracy_QuestionType_ten': 52.09, 'test/accuracy_QuestionType_two': 55.47, 'test/accuracy_QuestionType_six': 49.22, 'test/accuracy_AnswerType_other': 51.8, 'test/epoch': 1}[0m
Epoch 1, global step 564: 'test/accuracy_overall' reached 51.80000 (best 51.80000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-FrDPR_FullCorpus-flant5.21527673/train/saved_model/model_1.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.56995640110979, 'test/exact_match_at_2': 0.6900515259611574, 'test/exact_match_at_3': 0.7275069361870788, 'test/exact_match_at_4': 0.7413793103448276, 'test/exact_match_at_5': 0.7429647245342846, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37292905271502186, 'test/gold_recall_at_5': 0.667063020214031, 'test/successful_hit': 0.3629409433214427, 'test/successful_no_hit': 0.16436781609195403, 'test/failed_hit': 0.29056678557273086, 'test/failed_no_hit': 0.18212445501387237, 'test/selected_successful_hit': 0.4425287356321839, 'test/selected_successful_no_hit': 0.12742766547760603, 'test/selected_failed_hit': 0.323226317875545, 'test/selected_failed_no_hit': 0.10681728101466507, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 52.37, 'test/accuracy_QuestionType_one': 48.93, 'test/accuracy_QuestionType_eight': 50.02, 'test/accuracy_QuestionType_other': 53.54, 'test/accuracy_QuestionType_seven': 52.43, 'test/accuracy_QuestionType_four': 59.19, 'test/accuracy_QuestionType_five': 53.32, 'test/accuracy_QuestionType_three': 53.41, 'test/accuracy_QuestionType_nine': 40.48, 'test/accuracy_QuestionType_ten': 53.18, 'test/accuracy_QuestionType_two': 53.6, 'test/accuracy_QuestionType_six': 49.93, 'test/accuracy_AnswerType_other': 52.37, 'test/epoch': 2}[0m
Epoch 2, global step 846: 'test/accuracy_overall' reached 52.37000 (best 52.37000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-FrDPR_FullCorpus-flant5.21527673/train/saved_model/model_2.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5786761791518034, 'test/exact_match_at_2': 0.6991676575505351, 'test/exact_match_at_3': 0.7302814110186286, 'test/exact_match_at_4': 0.7401902497027348, 'test/exact_match_at_5': 0.7411811335711455, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37292905271502186, 'test/gold_recall_at_5': 0.667063020214031, 'test/successful_hit': 0.364367816091954, 'test/successful_no_hit': 0.18145065398335314, 'test/failed_hit': 0.26143479984145856, 'test/failed_no_hit': 0.19274673008323426, 'test/selected_successful_hit': 0.4353943717796274, 'test/selected_successful_no_hit': 0.14328180737217597, 'test/selected_failed_hit': 0.2972651605231867, 'test/selected_failed_no_hit': 0.12405866032500991, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.05, 'test/accuracy_QuestionType_one': 48.14, 'test/accuracy_QuestionType_eight': 51.84, 'test/accuracy_QuestionType_other': 53.83, 'test/accuracy_QuestionType_seven': 52.34, 'test/accuracy_QuestionType_four': 60.21, 'test/accuracy_QuestionType_five': 54.71, 'test/accuracy_QuestionType_three': 54.67, 'test/accuracy_QuestionType_nine': 38.57, 'test/accuracy_QuestionType_ten': 53.02, 'test/accuracy_QuestionType_two': 53.84, 'test/accuracy_QuestionType_six': 51.91, 'test/accuracy_AnswerType_other': 53.05, 'test/epoch': 3}[0m
Epoch 3, global step 1128: 'test/accuracy_overall' reached 53.05000 (best 53.05000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-FrDPR_FullCorpus-flant5.21527673/train/saved_model/model_3.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5862068965517241, 'test/exact_match_at_2': 0.7045184304399524, 'test/exact_match_at_3': 0.730875941339675, 'test/exact_match_at_4': 0.7399920729290527, 'test/exact_match_at_5': 0.7417756638921918, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37292905271502186, 'test/gold_recall_at_5': 0.667063020214031, 'test/successful_hit': 0.36424891002774473, 'test/successful_no_hit': 0.18688069758224335, 'test/failed_hit': 0.24879112168053905, 'test/failed_no_hit': 0.20007927070947285, 'test/selected_successful_hit': 0.4383670233848593, 'test/selected_successful_no_hit': 0.14783987316686484, 'test/selected_failed_hit': 0.27764565992865636, 'test/selected_failed_no_hit': 0.1361474435196195, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.75, 'test/accuracy_QuestionType_one': 48.86, 'test/accuracy_QuestionType_eight': 52.33, 'test/accuracy_QuestionType_other': 55.19, 'test/accuracy_QuestionType_seven': 50.61, 'test/accuracy_QuestionType_four': 61.52, 'test/accuracy_QuestionType_five': 55.4, 'test/accuracy_QuestionType_three': 55.37, 'test/accuracy_QuestionType_nine': 42.86, 'test/accuracy_QuestionType_ten': 54.42, 'test/accuracy_QuestionType_two': 55.7, 'test/accuracy_QuestionType_six': 51.63, 'test/accuracy_AnswerType_other': 53.75, 'test/epoch': 4}[0m
Epoch 4, global step 1410: 'test/accuracy_overall' reached 53.75000 (best 53.75000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-FrDPR_FullCorpus-flant5.21527673/train/saved_model/model_4.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5891795481569561, 'test/exact_match_at_2': 0.7035275465715418, 'test/exact_match_at_3': 0.7328577090764963, 'test/exact_match_at_4': 0.7413793103448276, 'test/exact_match_at_5': 0.7427665477606025, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37292905271502186, 'test/gold_recall_at_5': 0.667063020214031, 'test/successful_hit': 0.3622275069361871, 'test/successful_no_hit': 0.1863258026159334, 'test/failed_hit': 0.25422116527942923, 'test/failed_no_hit': 0.19722552516845027, 'test/selected_successful_hit': 0.43737613951644866, 'test/selected_successful_no_hit': 0.15180340864050734, 'test/selected_failed_hit': 0.2790328973444312, 'test/selected_failed_no_hit': 0.13178755449861276, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 54.14, 'test/accuracy_QuestionType_one': 48.83, 'test/accuracy_QuestionType_eight': 52.67, 'test/accuracy_QuestionType_other': 56.54, 'test/accuracy_QuestionType_seven': 51.96, 'test/accuracy_QuestionType_four': 61.87, 'test/accuracy_QuestionType_five': 55.48, 'test/accuracy_QuestionType_three': 55.19, 'test/accuracy_QuestionType_nine': 45.95, 'test/accuracy_QuestionType_ten': 50.54, 'test/accuracy_QuestionType_two': 55.58, 'test/accuracy_QuestionType_six': 54.18, 'test/accuracy_AnswerType_other': 54.14, 'test/epoch': 5}[0m
Epoch 5, global step 1692: 'test/accuracy_overall' reached 54.14000 (best 54.14000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-FrDPR_FullCorpus-flant5.21527673/train/saved_model/model_5.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5911613158937773, 'test/exact_match_at_2': 0.6999603646452636, 'test/exact_match_at_3': 0.7284978200554895, 'test/exact_match_at_4': 0.7350376535869996, 'test/exact_match_at_5': 0.7358303606817281, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37292905271502186, 'test/gold_recall_at_5': 0.667063020214031, 'test/successful_hit': 0.36087990487514865, 'test/successful_no_hit': 0.19674990091161315, 'test/failed_hit': 0.2273880301228696, 'test/failed_no_hit': 0.2149821640903686, 'test/selected_successful_hit': 0.43143083630598494, 'test/selected_successful_no_hit': 0.15973047958779232, 'test/selected_failed_hit': 0.2600079270709473, 'test/selected_failed_no_hit': 0.14883075703527546, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 54.23, 'test/accuracy_QuestionType_one': 48.86, 'test/accuracy_QuestionType_eight': 51.95, 'test/accuracy_QuestionType_other': 55.64, 'test/accuracy_QuestionType_seven': 52.8, 'test/accuracy_QuestionType_four': 62.61, 'test/accuracy_QuestionType_five': 56.25, 'test/accuracy_QuestionType_three': 55.33, 'test/accuracy_QuestionType_nine': 44.05, 'test/accuracy_QuestionType_ten': 53.02, 'test/accuracy_QuestionType_two': 54.88, 'test/accuracy_QuestionType_six': 54.89, 'test/accuracy_AnswerType_other': 54.23, 'test/epoch': 6}[0m
Epoch 6, global step 1974: 'test/accuracy_overall' reached 54.23000 (best 54.23000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-FrDPR_FullCorpus-flant5.21527673/train/saved_model/model_6.ckpt' as top 1
