/home/xl544/.conda/envs/RAVQA/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning
  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')
[38;20m[INFO] - __main__ : Initialization done with the config: {'DATA_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data', 'EXPERIMENT_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments', 'TENSORBOARD_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data_TB/tb_logs', 'WANDB': {'entity': 'xl544', 'project': 'RAVQA', 'tags': ['OKVQA', 'RAG', 'freeze_question_encoder', 'force_existence'], 'name': 'OKVQA_RA-VQA-T5-CLIP-32.21804773'}, 'cache': {'default_folder': '../data/ok-vqa/cache', 'regenerate': {'clip_embeddings': 0, 'ocr_feature_preprocessed': 0, 'test_data_preprocessed': 0, 'train_data_preprocessed': 0, 'vinvl_feature_preprocessed': 0}}, 'cuda': 0, 'data_loader': {'additional': {'max_decoder_source_length': 512, 'max_source_length': 512, 'max_target_length': 10, 'num_knowledge_passages': 5}, 'dataset_modules': {'module_dict': {'LoadClipEmbeddings': {'config': {'train': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_train2014.pkl', 'val': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_val2014.pkl'}, 'option': 'default', 'type': 'EmbeddingInput'}, 'LoadGoogleOCRFeatures': {'config': {'combine_with_vinvl': True, 'test': '../data/ok-vqa/pre-extracted_features/OCR/valid', 'train': '../data/ok-vqa/pre-extracted_features/OCR/train'}, 'option': 'default', 'type': 'LoadGoogleOCRFeatures'}, 'LoadGoogleSearchAnnotations': {'config': {'annotations_path': {'test': '../data/ok-vqa/pre-extracted_features/passages/retriever_test.json', 'train': '../data/ok-vqa/pre-extracted_features/passages/retriever_train.json', 'valid': '../data/ok-vqa/pre-extracted_features/passages/retriever_testdev.json'}}, 'option': 'default', 'type': 'LoadGoogleSearchAnnotations'}, 'LoadGoogleSearchPassageData': {'config': {'passage_data_path': {'full': '../data/ok-vqa/pre-extracted_features/passages/okvqa_full_corpus.csv', 'train': '../data/ok-vqa/pre-extracted_features/passages/okvqa_train_corpus.csv'}, 'use_full_split': True}, 'option': 'default', 'type': 'LoadGoogleSearchPassageData'}, 'LoadOKVQAData': {'config': {'image_data_path': {'test': '../data/ok-vqa/val2014', 'train': '../data/ok-vqa/train2014'}, 'vqa_data_path': {'annotation_files': {'test': '../data/ok-vqa/mscoco_val2014_annotations.json', 'train': '../data/ok-vqa/mscoco_train2014_annotations.json'}, 'question_files': {'test': '../data/ok-vqa/OpenEnded_mscoco_val2014_questions.json', 'train': '../data/ok-vqa/OpenEnded_mscoco_train2014_questions.json'}}}, 'option': 'default', 'type': 'LoadOKVQAData'}, 'LoadOscarCaptionFeatures': {'config': {'test': '../data/ok-vqa/pre-extracted_features/captions/test_predictions.json', 'train': '../data/ok-vqa/pre-extracted_features/captions/train_predictions.json', 'valid': '../data/ok-vqa/pre-extracted_features/captions/valid_predictions.json'}, 'option': 'default', 'type': 'LoadOscarCaptionFeatures'}, 'LoadPretrainedDPROutputForGoogleSearchPassage': {'config': {'pretrained_dpr_outputs': {'test': '../Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/test/test_evaluation/test_predictions.json', 'train': '../Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/test/test_evaluation/train_predictions.json'}}, 'option': 'none', 'type': 'LoadPretrainedDPROutputForGoogleSearchPassage'}, 'LoadVinVLFeatures': {'config': {'test': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_testset_full/inference/vinvl_vg_x152c4/predictions.tsv', 'train': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_trainset_full/inference/vinvl_vg_x152c4/predictions.tsv'}, 'option': 'default', 'type': 'LoadVinVLFeatures'}}, 'module_list': ['LoadVinVLFeatures', 'LoadGoogleOCRFeatures', 'LoadOscarCaptionFeatures', 'LoadOKVQAData', 'LoadGoogleSearchPassageData', 'LoadClipEmbeddings']}, 'dataset_type': 'OKVQADataset', 'dummy_dataloader': 0, 'index_files': {'index_passages_path': '../data/ok-vqa/pre-extracted_features/faiss/ok-vqa-passages-full-caption-pretrained-NewRun/my_knowledge_dataset', 'index_path': '../data/ok-vqa/pre-extracted_features/faiss/ok-vqa-passages-full-caption-pretrained-NewRun/my_knowledge_dataset_hnsw_index.faiss'}, 'type': 'DataLoaderOKVQAWithKnowledge'}, 'experiment_name': 'OKVQA_RA-VQA-T5-CLIP-32.21804773', 'gpu_device': 0, 'ignore_pretrained_weights': [], 'metrics': [{'name': 'compute_exact_match'}, {'name': 'compute_retrieval_metrics'}, {'name': 'compute_okvqa_scores'}], 'model_config': {'DECODER_SPECIAL_TOKENS': {'additional_special_tokens': ['<BOV>', '<SOV>', '<EOV>', '<BOQ>', '<EOQ>', '<BOC>', '<EOC>', '<BOK>', '<EOK>'], 'bos_token': '<PAD>', 'pad_token': '<PAD>'}, 'DecoderTokenizerClass': 'T5Tokenizer', 'DecoderTokenizerModelVersion': 'google/flan-t5-large', 'GeneratorConfigClass': 'T5Config', 'GeneratorModelClass': 'T5ForConditionalGeneration', 'GeneratorModelVersion': 'google/flan-t5-large', 'LoadPretrainedMLPWeights': 0, 'ModelClass': 'RagModel', 'PretrainedMLPPath': '', 'QueryEncoderConfigClass': 'DPRConfig', 'QueryEncoderModelClass': 'DPRQuestionEncoder', 'QueryEncoderModelVersion': '../Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/train/saved_model/epoch6/query_encoder', 'RAVQA_loss_type': 'Approach6', 'SPECIAL_TOKENS': {'additional_special_tokens': ['<BOV>', '<SOV>', '<EOV>', '<BOQ>', '<EOQ>', '<BOC>', '<EOC>', '<BOK>', '<EOK>']}, 'TokenizerClass': 'DPRQuestionEncoderTokenizer', 'TokenizerModelVersion': 'facebook/dpr-question_encoder-single-nq-base', 'UsePrefixEmb': 1, 'base_model': 'RAG', 'decoder_input_modules': {'module_list': [], 'postprocess_module_list': []}, 'input_modules': {'module_list': [{'option': 'default', 'separation_tokens': {'end': '<EOQ>', 'start': '<BOQ>'}, 'type': 'QuestionInput'}, {'option': 'caption', 'separation_tokens': {'end': '<EOC>', 'start': '<BOC>'}, 'type': 'TextBasedVisionInput'}, {'attribute_max': 3, 'attribute_thres': 0.05, 'object_max': 40, 'ocr': 1, 'option': 'object', 'separation_tokens': {'end': '<EOV>', 'sep': '<SOV>', 'start': '<BOV>'}, 'type': 'TextBasedVisionInput'}, {'option': 'default', 'type': 'EmbeddingInput'}], 'postprocess_module_list': [{'option': 'default', 'type': 'PostProcessInputTokenization'}, {'option': 'default', 'type': 'PostProcessClipEmbeddings'}]}, 'loss_ratio': {'additional_loss': 0, 'nll_loss': 1, 'rag_loss': 0, 'retrieval_pseudo_loss': 0}, 'modules': ['freeze_question_encoder', 'force_existence'], 'output_modules': {'module_list': [{'option': 'default', 'type': 'GenerationOutput'}], 'postprocess_module_list': [{'option': 'default', 'type': 'PostProcessOutputTokenization'}]}, 'pretrained': 1, 'rag_modules': {'module_list': []}}, 'platform_type': 'pytorch', 'seed': 2021, 'test': {'additional': {'multiprocessing': 4}, 'batch_size': 32, 'evaluation_name': 'test_evaluation', 'load_best_model': 0, 'load_epoch': -1, 'load_model_path': '', 'num_evaluation': 0}, 'train': {'adam_epsilon': 1e-08, 'additional': {'gradient_accumulation_steps': 16, 'gradient_clipping': 0, 'plugins': [], 'save_top_k': 1, 'save_top_k_metric': 'test/accuracy_overall', 'save_top_k_mode': 'max', 'warmup_steps': 0}, 'batch_size': 2, 'epochs': 8, 'load_best_model': 0, 'load_epoch': -1, 'load_model_path': '', 'lr': 6e-05, 'retriever_lr': 1e-05, 'save_interval': 1, 'scheduler': 'linear', 'type': 'RagExecutor'}, 'valid': {'additional': {}, 'batch_size': 32, 'break_interval': 3000, 'step_size': 0.2}, 'reset': False, 'mode': 'train', 'log_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-T5-CLIP-32.21804773/train', 'experiment_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-T5-CLIP-32.21804773', 'saved_model_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-T5-CLIP-32.21804773/train/saved_model', 'imgs_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-T5-CLIP-32.21804773/train/imgs', 'tensorboard_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data_TB/tb_logs/OKVQA_RA-VQA-T5-CLIP-32.21804773', 'args': {'config': '../configs/okvqa/RAVQA.jsonnet', 'DATA_FOLDER': '', 'EXPERIMENT_FOLDER': '', 'mode': 'train', 'reset': False, 'experiment_name': 'OKVQA_RA-VQA-T5-CLIP-32.21804773', 'tags': [], 'modules': ['freeze_question_encoder', 'force_existence'], 'log_prediction_tables': False, 'test_batch_size': -1, 'test_evaluation_name': '', 'logger': True, 'checkpoint_callback': None, 'enable_checkpointing': True, 'default_root_dir': None, 'gradient_clip_val': None, 'gradient_clip_algorithm': None, 'process_position': 0, 'num_nodes': 1, 'num_processes': None, 'devices': '1', 'gpus': None, 'auto_select_gpus': False, 'tpu_cores': None, 'ipus': None, 'log_gpu_memory': None, 'progress_bar_refresh_rate': None, 'enable_progress_bar': True, 'overfit_batches': 0.0, 'track_grad_norm': -1, 'check_val_every_n_epoch': 1, 'fast_dev_run': False, 'accumulate_grad_batches': None, 'max_epochs': None, 'min_epochs': None, 'max_steps': -1, 'min_steps': None, 'max_time': None, 'limit_train_batches': None, 'limit_val_batches': None, 'limit_test_batches': None, 'limit_predict_batches': None, 'val_check_interval': None, 'flush_logs_every_n_steps': None, 'log_every_n_steps': 50, 'accelerator': 'auto', 'strategy': None, 'sync_batchnorm': False, 'precision': 32, 'enable_model_summary': True, 'weights_summary': 'top', 'weights_save_path': None, 'num_sanity_val_steps': 2, 'resume_from_checkpoint': None, 'profiler': None, 'benchmark': None, 'deterministic': None, 'reload_dataloaders_every_n_epochs': 0, 'auto_lr_find': False, 'replace_sampler_ddp': True, 'detect_anomaly': False, 'auto_scale_batch_size': False, 'prepare_data_per_node': None, 'plugins': None, 'amp_backend': 'native', 'amp_level': None, 'move_metrics_to_cpu': False, 'multiple_trainloader_mode': 'max_size_cycle', 'stochastic_weight_avg': False, 'terminate_on_nan': None, 'opts': ['train.epochs=8', 'train.batch_size=2', 'valid.step_size=0.2', 'valid.batch_size=32', 'train.additional.gradient_accumulation_steps=16', 'train.lr=0.00006', 'train.retriever_lr=0.00001', 'train.scheduler=linear', 'data_loader.additional.num_knowledge_passages=5', 'model_config.UsePrefixEmb=1', 'model_config.DecoderTokenizerModelVersion=google/flan-t5-large', 'model_config.GeneratorModelVersion=google/flan-t5-large']}}[0m
Global seed set to 2021
[38;20m[INFO] - __main__ : All seeds have been set to 2021[0m
[38;20m[INFO] - __main__ : init wandb logger with the following settings: {'entity': 'xl544', 'project': 'RAVQA', 'tags': ['OKVQA', 'RAG', 'freeze_question_encoder', 'force_existence'], 'name': 'OKVQA_RA-VQA-T5-CLIP-32.21804773'}[0m
wandb: Currently logged in as: xl544. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.2
wandb: Run data is saved locally in /rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/src/wandb/run-20230606_022601-k3mvj2az
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run OKVQA_RA-VQA-T5-CLIP-32.21804773
wandb: ⭐️ View project at https://wandb.ai/xl544/RAVQA
wandb: 🚀 View run at https://wandb.ai/xl544/RAVQA/runs/k3mvj2az
Multiprocessing is handled by SLURM.
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[38;20m[INFO] - __main__ : arguments passed to trainer: Namespace(DATA_FOLDER='', EXPERIMENT_FOLDER='', accelerator='auto', accumulate_grad_batches=None, amp_backend='native', amp_level=None, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, benchmark=None, check_val_every_n_epoch=1, checkpoint_callback=None, config='../configs/okvqa/RAVQA.jsonnet', default_root_dir=None, detect_anomaly=False, deterministic=None, devices='1', enable_checkpointing=True, enable_model_summary=True, enable_progress_bar=True, experiment_name='OKVQA_RA-VQA-T5-CLIP-32.21804773', fast_dev_run=False, flush_logs_every_n_steps=None, gpus=None, gradient_clip_algorithm=None, gradient_clip_val=None, ipus=None, limit_predict_batches=None, limit_test_batches=None, limit_train_batches=None, limit_val_batches=None, log_every_n_steps=50, log_gpu_memory=None, log_prediction_tables=False, logger=True, max_epochs=None, max_steps=-1, max_time=None, min_epochs=None, min_steps=None, mode='train', modules=['freeze_question_encoder', 'force_existence'], move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=None, num_sanity_val_steps=2, opts=['train.epochs=8', 'train.batch_size=2', 'valid.step_size=0.2', 'valid.batch_size=32', 'train.additional.gradient_accumulation_steps=16', 'train.lr=0.00006', 'train.retriever_lr=0.00001', 'train.scheduler=linear', 'data_loader.additional.num_knowledge_passages=5', 'model_config.UsePrefixEmb=1', 'model_config.DecoderTokenizerModelVersion=google/flan-t5-large', 'model_config.GeneratorModelVersion=google/flan-t5-large'], overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=None, process_position=0, profiler=None, progress_bar_refresh_rate=None, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, reset=False, resume_from_checkpoint=None, stochastic_weight_avg=False, strategy=None, sync_batchnorm=False, tags=[], terminate_on_nan=None, test_batch_size=-1, test_evaluation_name='', tpu_cores=None, track_grad_norm=-1, val_check_interval=None, weights_save_path=None, weights_summary='top')[0m
[38;20m[INFO] - __main__ : additional arguments passed to trainer: {'accumulate_grad_batches': 16, 'default_root_dir': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-T5-CLIP-32.21804773/train/saved_model', 'max_epochs': 8, 'logger': [<pytorch_lightning.loggers.tensorboard.TensorBoardLogger object at 0x14ecc4cac520>, <pytorch_lightning.loggers.wandb.WandbLogger object at 0x14ead7f83f70>, <utils.metrics_log_callback.MetricsHistoryLogger object at 0x14ead7124f70>], 'callbacks': [<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x14ead7f83760>, <pytorch_lightning.callbacks.progress.tqdm_progress.TQDMProgressBar object at 0x14ead766b730>, <pytorch_lightning.callbacks.model_summary.ModelSummary object at 0x14ead7124430>, <pytorch_lightning.callbacks.gradient_accumulation_scheduler.GradientAccumulationScheduler object at 0x14ead71244c0>], 'plugins': [], 'log_every_n_steps': 100, 'val_check_interval': 0.2}[0m
[33;20m[WARNING] - __main__ : No checkpoint exists from '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-T5-CLIP-32.21804773/train/saved_model/last.ckpt'. Skipping...[0m
[38;20m[INFO] - __main__ : **First time to train**[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'test': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_testset_full/inference/vinvl_vg_x152c4/predictions.tsv', 'train': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_trainset_full/inference/vinvl_vg_x152c4/predictions.tsv'}, 'option': 'default', 'type': 'LoadVinVLFeatures'}[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/vinvl_feature_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data Statistics] VinVL features 14031[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'combine_with_vinvl': True, 'test': '../data/ok-vqa/pre-extracted_features/OCR/valid', 'train': '../data/ok-vqa/pre-extracted_features/OCR/train'}, 'option': 'default', 'type': 'LoadGoogleOCRFeatures'}[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/ocr_feature_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data Statistics] OCR features 14031, 5462 has annotations.[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : OCR feature detected in VinVL feature dict...skipping..[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'test': '../data/ok-vqa/pre-extracted_features/captions/test_predictions.json', 'train': '../data/ok-vqa/pre-extracted_features/captions/train_predictions.json', 'valid': '../data/ok-vqa/pre-extracted_features/captions/valid_predictions.json'}, 'option': 'default', 'type': 'LoadOscarCaptionFeatures'}[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'image_data_path': {'test': '../data/ok-vqa/val2014', 'train': '../data/ok-vqa/train2014'}, 'vqa_data_path': {'annotation_files': {'test': '../data/ok-vqa/mscoco_val2014_annotations.json', 'train': '../data/ok-vqa/mscoco_train2014_annotations.json'}, 'question_files': {'test': '../data/ok-vqa/OpenEnded_mscoco_val2014_questions.json', 'train': '../data/ok-vqa/OpenEnded_mscoco_train2014_questions.json'}}}, 'option': 'default', 'type': 'LoadOKVQAData'}[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/train_data_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data statistics] split: train  entries: 9009[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/test_data_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data statistics] split: test  entries: 5046[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'passage_data_path': {'full': '../data/ok-vqa/pre-extracted_features/passages/okvqa_full_corpus.csv', 'train': '../data/ok-vqa/pre-extracted_features/passages/okvqa_train_corpus.csv'}, 'use_full_split': True}, 'option': 'default', 'type': 'LoadGoogleSearchPassageData'}[0m
  0%|          | 0/168380 [00:00<?, ?it/s] 10%|▉         | 16776/168380 [00:00<00:00, 167751.36it/s] 20%|█▉        | 33552/168380 [00:00<00:00, 165072.94it/s] 30%|██▉       | 50072/168380 [00:00<00:00, 165128.39it/s] 40%|███▉      | 66724/168380 [00:00<00:00, 165673.05it/s] 50%|████▉     | 83462/168380 [00:00<00:00, 166285.11it/s] 59%|█████▉    | 100092/168380 [00:00<00:00, 165389.58it/s] 70%|██████▉   | 117500/168380 [00:00<00:00, 168214.07it/s] 81%|████████  | 136750/168380 [00:00<00:00, 175915.56it/s] 93%|█████████▎| 156550/168380 [00:00<00:00, 182800.21it/s]100%|█████████▉| 168307/168380 [00:00<00:00, 174598.50it/s]
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'train': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_train2014.pkl', 'val': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_val2014.pkl'}, 'option': 'default', 'type': 'EmbeddingInput'}[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/clip_embeddings.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data Statistics] CLIP embeddings 123287[0m
[38;20m[INFO] - data_loader_manager.datasets.okvqa_datasets : initialising OKVQADataset...[0m
[38;20m[INFO] - data_loader_manager.datasets.okvqa_datasets : initialising OKVQADataset...[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa_with_knowledge : [Data Statistics]: training data loader: 4505;  test data loader: 158[0m
[38;20m[INFO] - torch.distributed.nn.jit.instantiator : Created a temporary directory at /tmp/tmprasxw83i[0m
[38;20m[INFO] - torch.distributed.nn.jit.instantiator : Writing /tmp/tmprasxw83i/_remote_module_non_scriptable.py[0m
[38;20m[INFO] - trainers.base_executor : Initializing RagExecutor...[0m
[38;20m[INFO] - __main__ : config file was successfully saved to /rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-T5-CLIP-32.21804773 for future use.[0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[38;20m[INFO] - trainers.rag_executor : using different learning rate for retriever[0m
[38;20m[INFO] - trainers.rag_executor : #params: 558   lr: 6e-05[0m
[38;20m[INFO] - trainers.rag_executor : #params: 4   lr: 1e-05[0m
Loading `train_dataloader` to estimate number of stepping batches.
/home/xl544/.conda/envs/RAVQA/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(

  | Name  | Type     | Params
-----------------------------------
0 | model | RagModel | 1.4 B 
-----------------------------------
1.3 B     Trainable params
109 M     Non-trainable params
1.4 B     Total params
5,768.421 Total estimated model params size (MB)
Missing logger folder: /rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data_TB/tb_logs/OKVQA_RA-VQA-T5-CLIP-32.21804773/OKVQA_RA-VQA-T5-CLIP-32.21804773
SLURM auto-requeueing enabled. Setting signal handlers.
/home/xl544/.conda/envs/RAVQA/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Failed to compute OKVQA scores: Results do not correspond to current VQA set. Either the results do not have predictions for all question ids in annotation file or there is atleast one question id that does not belong to the question ids in the annotation file.This could be due to the fact that OKVQA parser requires all questions to evaluatethe accuracy. Ignore this error if this is the sanity check.[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [sanity_check]: {'test/exact_match_at_1': 0.0625, 'test/exact_match_at_2': 0.0625, 'test/exact_match_at_3': 0.0625, 'test/exact_match_at_4': 0.0625, 'test/exact_match_at_5': 0.0625, 'test/recall_at_5': 0.796875, 'test/precision_at_5': 0.515625, 'test/gold_precision_at_5': 0.353125, 'test/gold_recall_at_5': 0.671875, 'test/successful_hit': 0.01875, 'test/successful_no_hit': 0.0, 'test/failed_hit': 0.559375, 'test/failed_no_hit': 0.421875, 'test/selected_successful_hit': 0.0625, 'test/selected_successful_no_hit': 0.0, 'test/selected_failed_hit': 0.625, 'test/selected_failed_no_hit': 0.3125, 'test/n_retrieved_docs': 5, 'test/epoch': 0}[0m
[33;20m[WARNING] - root : Sanity check mode, not saving to loggers.[0m
/home/xl544/.conda/envs/RAVQA/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.4766151407055093, 'test/exact_match_at_2': 0.6000792707094729, 'test/exact_match_at_3': 0.6480380499405469, 'test/exact_match_at_4': 0.6621086008719778, 'test/exact_match_at_5': 0.6678557273087594, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37209671026555685, 'test/gold_recall_at_5': 0.6642885453824812, 'test/successful_hit': 0.3191042409829568, 'test/successful_no_hit': 0.11022592152199762, 'test/failed_hit': 0.3850574712643678, 'test/failed_no_hit': 0.18561236623067776, 'test/selected_successful_hit': 0.3850574712643678, 'test/selected_successful_no_hit': 0.09155766944114149, 'test/selected_failed_hit': 0.4239001189060642, 'test/selected_failed_no_hit': 0.09948474038842647, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 43.73, 'test/accuracy_QuestionType_one': 38.87, 'test/accuracy_QuestionType_eight': 41.3, 'test/accuracy_QuestionType_other': 44.17, 'test/accuracy_QuestionType_seven': 44.63, 'test/accuracy_QuestionType_four': 50.72, 'test/accuracy_QuestionType_five': 45.99, 'test/accuracy_QuestionType_three': 41.59, 'test/accuracy_QuestionType_nine': 33.57, 'test/accuracy_QuestionType_ten': 43.1, 'test/accuracy_QuestionType_two': 49.77, 'test/accuracy_QuestionType_six': 46.81, 'test/accuracy_AnswerType_other': 43.73, 'test/epoch': 0}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5085216012683313, 'test/exact_match_at_2': 0.6363456202933017, 'test/exact_match_at_3': 0.6797463337296868, 'test/exact_match_at_4': 0.6948077685295283, 'test/exact_match_at_5': 0.6991676575505351, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37209671026555685, 'test/gold_recall_at_5': 0.6642885453824812, 'test/successful_hit': 0.3418152992469283, 'test/successful_no_hit': 0.12275069361870788, 'test/failed_hit': 0.36975822433610783, 'test/failed_no_hit': 0.16567578279825604, 'test/selected_successful_hit': 0.4102259215219976, 'test/selected_successful_no_hit': 0.09829567974633373, 'test/selected_failed_hit': 0.39615537059056677, 'test/selected_failed_no_hit': 0.09532302814110186, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 46.67, 'test/accuracy_QuestionType_one': 41.45, 'test/accuracy_QuestionType_eight': 43.77, 'test/accuracy_QuestionType_other': 48.98, 'test/accuracy_QuestionType_seven': 47.76, 'test/accuracy_QuestionType_four': 53.09, 'test/accuracy_QuestionType_five': 49.37, 'test/accuracy_QuestionType_three': 44.16, 'test/accuracy_QuestionType_nine': 34.76, 'test/accuracy_QuestionType_ten': 44.34, 'test/accuracy_QuestionType_two': 54.19, 'test/accuracy_QuestionType_six': 47.52, 'test/accuracy_AnswerType_other': 46.67, 'test/epoch': 0}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5180340864050733, 'test/exact_match_at_2': 0.6516052318668252, 'test/exact_match_at_3': 0.6916369401506144, 'test/exact_match_at_4': 0.7061038446294094, 'test/exact_match_at_5': 0.7092746730083235, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37209671026555685, 'test/gold_recall_at_5': 0.6642885453824812, 'test/successful_hit': 0.35295283392786364, 'test/successful_no_hit': 0.1258026159334126, 'test/failed_hit': 0.37078874355925484, 'test/failed_no_hit': 0.1504558065794689, 'test/selected_successful_hit': 0.4205311137534681, 'test/selected_successful_no_hit': 0.09750297265160524, 'test/selected_failed_hit': 0.3912009512485137, 'test/selected_failed_no_hit': 0.090764962346413, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 47.48, 'test/accuracy_QuestionType_one': 42.98, 'test/accuracy_QuestionType_eight': 46.21, 'test/accuracy_QuestionType_other': 49.24, 'test/accuracy_QuestionType_seven': 48.04, 'test/accuracy_QuestionType_four': 52.49, 'test/accuracy_QuestionType_five': 49.24, 'test/accuracy_QuestionType_three': 46.36, 'test/accuracy_QuestionType_nine': 35.48, 'test/accuracy_QuestionType_ten': 45.27, 'test/accuracy_QuestionType_two': 52.44, 'test/accuracy_QuestionType_six': 48.09, 'test/accuracy_AnswerType_other': 47.48, 'test/epoch': 0}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5326991676575505, 'test/exact_match_at_2': 0.6619104240982957, 'test/exact_match_at_3': 0.7019421323820848, 'test/exact_match_at_4': 0.7152199762187872, 'test/exact_match_at_5': 0.7191835116924297, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37209671026555685, 'test/gold_recall_at_5': 0.6642885453824812, 'test/successful_hit': 0.35707491082045184, 'test/successful_no_hit': 0.12956797463337297, 'test/failed_hit': 0.3575108997225525, 'test/failed_no_hit': 0.15584621482362268, 'test/selected_successful_hit': 0.43242172017439556, 'test/selected_successful_no_hit': 0.10027744748315498, 'test/selected_failed_hit': 0.3799048751486326, 'test/selected_failed_no_hit': 0.08739595719381689, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 48.87, 'test/accuracy_QuestionType_one': 44.99, 'test/accuracy_QuestionType_eight': 47.37, 'test/accuracy_QuestionType_other': 49.56, 'test/accuracy_QuestionType_seven': 47.48, 'test/accuracy_QuestionType_four': 55.41, 'test/accuracy_QuestionType_five': 50.61, 'test/accuracy_QuestionType_three': 47.76, 'test/accuracy_QuestionType_nine': 40.24, 'test/accuracy_QuestionType_ten': 48.84, 'test/accuracy_QuestionType_two': 51.63, 'test/accuracy_QuestionType_six': 50.35, 'test/accuracy_AnswerType_other': 48.87, 'test/epoch': 0}[0m
/home/xl544/.conda/envs/RAVQA/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5406262386048355, 'test/exact_match_at_2': 0.6674593737613952, 'test/exact_match_at_3': 0.7072929052715021, 'test/exact_match_at_4': 0.7201743955608403, 'test/exact_match_at_5': 0.7253269916765755, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37209671026555685, 'test/gold_recall_at_5': 0.6642885453824812, 'test/successful_hit': 0.35850178359096313, 'test/successful_no_hit': 0.1372572334522394, 'test/failed_hit': 0.3387633769322235, 'test/failed_no_hit': 0.16547760602457393, 'test/selected_successful_hit': 0.4346016646848989, 'test/selected_successful_no_hit': 0.10602457391993658, 'test/selected_failed_hit': 0.3686087990487515, 'test/selected_failed_no_hit': 0.090764962346413, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 49.62, 'test/accuracy_QuestionType_one': 46.1, 'test/accuracy_QuestionType_eight': 47.23, 'test/accuracy_QuestionType_other': 51.05, 'test/accuracy_QuestionType_seven': 47.66, 'test/accuracy_QuestionType_four': 56.19, 'test/accuracy_QuestionType_five': 51.53, 'test/accuracy_QuestionType_three': 48.46, 'test/accuracy_QuestionType_nine': 39.76, 'test/accuracy_QuestionType_ten': 51.63, 'test/accuracy_QuestionType_two': 51.74, 'test/accuracy_QuestionType_six': 51.77, 'test/accuracy_AnswerType_other': 49.62, 'test/epoch': 0}[0m
Epoch 0, global step 282: 'test/accuracy_overall' reached 49.62000 (best 49.62000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-T5-CLIP-32.21804773/train/saved_model/model_0.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.549346016646849, 'test/exact_match_at_2': 0.6763773285770908, 'test/exact_match_at_3': 0.7173999207292905, 'test/exact_match_at_4': 0.7306777645659929, 'test/exact_match_at_5': 0.7330558858501783, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37209671026555685, 'test/gold_recall_at_5': 0.6642885453824812, 'test/successful_hit': 0.36298057867617917, 'test/successful_no_hit': 0.14641300039635355, 'test/failed_hit': 0.33285770907649626, 'test/failed_no_hit': 0.15774871185097106, 'test/selected_successful_hit': 0.43222354340071345, 'test/selected_successful_no_hit': 0.11712247324613555, 'test/selected_failed_hit': 0.3565200158541419, 'test/selected_failed_no_hit': 0.09413396749900911, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 50.56, 'test/accuracy_QuestionType_one': 46.72, 'test/accuracy_QuestionType_eight': 48.79, 'test/accuracy_QuestionType_other': 51.92, 'test/accuracy_QuestionType_seven': 49.39, 'test/accuracy_QuestionType_four': 57.85, 'test/accuracy_QuestionType_five': 52.4, 'test/accuracy_QuestionType_three': 49.44, 'test/accuracy_QuestionType_nine': 40.71, 'test/accuracy_QuestionType_ten': 51.16, 'test/accuracy_QuestionType_two': 51.05, 'test/accuracy_QuestionType_six': 49.5, 'test/accuracy_AnswerType_other': 50.56, 'test/epoch': 1}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5566785572730876, 'test/exact_match_at_2': 0.6761791518034086, 'test/exact_match_at_3': 0.713238208481966, 'test/exact_match_at_4': 0.7271105826397146, 'test/exact_match_at_5': 0.7290923503765359, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37209671026555685, 'test/gold_recall_at_5': 0.6642885453824812, 'test/successful_hit': 0.36139516448672215, 'test/successful_no_hit': 0.15009908838684105, 'test/failed_hit': 0.3202536662703131, 'test/failed_no_hit': 0.16825208085612367, 'test/selected_successful_hit': 0.4353943717796274, 'test/selected_successful_no_hit': 0.12128418549346016, 'test/selected_failed_hit': 0.3388822829964328, 'test/selected_failed_no_hit': 0.10443915973047958, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 51.13, 'test/accuracy_QuestionType_one': 47.06, 'test/accuracy_QuestionType_eight': 49.81, 'test/accuracy_QuestionType_other': 53.34, 'test/accuracy_QuestionType_seven': 49.86, 'test/accuracy_QuestionType_four': 56.97, 'test/accuracy_QuestionType_five': 53.09, 'test/accuracy_QuestionType_three': 48.5, 'test/accuracy_QuestionType_nine': 44.52, 'test/accuracy_QuestionType_ten': 52.87, 'test/accuracy_QuestionType_two': 51.98, 'test/accuracy_QuestionType_six': 51.21, 'test/accuracy_AnswerType_other': 51.13, 'test/epoch': 1}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5537059056678557, 'test/exact_match_at_2': 0.6801426872770512, 'test/exact_match_at_3': 0.7203725723345223, 'test/exact_match_at_4': 0.734046769718589, 'test/exact_match_at_5': 0.7368212445501388, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37209671026555685, 'test/gold_recall_at_5': 0.6642885453824812, 'test/successful_hit': 0.3642885453824812, 'test/successful_no_hit': 0.1469678953626635, 'test/failed_hit': 0.3349187475227903, 'test/failed_no_hit': 0.153824811732065, 'test/selected_successful_hit': 0.43697978596908443, 'test/selected_successful_no_hit': 0.11672611969877131, 'test/selected_failed_hit': 0.35453824811732065, 'test/selected_failed_no_hit': 0.09175584621482362, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 50.99, 'test/accuracy_QuestionType_one': 46.86, 'test/accuracy_QuestionType_eight': 48.91, 'test/accuracy_QuestionType_other': 52.08, 'test/accuracy_QuestionType_seven': 50.28, 'test/accuracy_QuestionType_four': 58.34, 'test/accuracy_QuestionType_five': 52.35, 'test/accuracy_QuestionType_three': 49.53, 'test/accuracy_QuestionType_nine': 45.0, 'test/accuracy_QuestionType_ten': 52.56, 'test/accuracy_QuestionType_two': 54.07, 'test/accuracy_QuestionType_six': 50.21, 'test/accuracy_AnswerType_other': 50.99, 'test/epoch': 1}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5689655172413793, 'test/exact_match_at_2': 0.6843043995243757, 'test/exact_match_at_3': 0.7223543400713437, 'test/exact_match_at_4': 0.7330558858501783, 'test/exact_match_at_5': 0.7358303606817281, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37209671026555685, 'test/gold_recall_at_5': 0.6642885453824812, 'test/successful_hit': 0.3639714625445898, 'test/successful_no_hit': 0.15973047958779232, 'test/failed_hit': 0.3051129607609988, 'test/failed_no_hit': 0.1711850971066191, 'test/selected_successful_hit': 0.4401506143479984, 'test/selected_successful_no_hit': 0.1288149028933809, 'test/selected_failed_hit': 0.3335315101070155, 'test/selected_failed_no_hit': 0.09750297265160524, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 52.23, 'test/accuracy_QuestionType_one': 48.73, 'test/accuracy_QuestionType_eight': 50.02, 'test/accuracy_QuestionType_other': 54.31, 'test/accuracy_QuestionType_seven': 51.21, 'test/accuracy_QuestionType_four': 57.88, 'test/accuracy_QuestionType_five': 55.16, 'test/accuracy_QuestionType_three': 50.79, 'test/accuracy_QuestionType_nine': 40.48, 'test/accuracy_QuestionType_ten': 50.85, 'test/accuracy_QuestionType_two': 53.02, 'test/accuracy_QuestionType_six': 51.91, 'test/accuracy_AnswerType_other': 52.23, 'test/epoch': 1}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5665873959571938, 'test/exact_match_at_2': 0.6934205311137535, 'test/exact_match_at_3': 0.7273087594133968, 'test/exact_match_at_4': 0.7395957193816884, 'test/exact_match_at_5': 0.7413793103448276, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37209671026555685, 'test/gold_recall_at_5': 0.6642885453824812, 'test/successful_hit': 0.3697978596908442, 'test/successful_no_hit': 0.1517637732857709, 'test/failed_hit': 0.320055489496631, 'test/failed_no_hit': 0.15838287752675387, 'test/selected_successful_hit': 0.44887039239001186, 'test/selected_successful_no_hit': 0.11771700356718193, 'test/selected_failed_hit': 0.34641300039635353, 'test/selected_failed_no_hit': 0.08699960364645264, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 52.06, 'test/accuracy_QuestionType_one': 49.18, 'test/accuracy_QuestionType_eight': 50.28, 'test/accuracy_QuestionType_other': 54.25, 'test/accuracy_QuestionType_seven': 52.52, 'test/accuracy_QuestionType_four': 57.18, 'test/accuracy_QuestionType_five': 53.76, 'test/accuracy_QuestionType_three': 50.09, 'test/accuracy_QuestionType_nine': 39.05, 'test/accuracy_QuestionType_ten': 53.49, 'test/accuracy_QuestionType_two': 52.56, 'test/accuracy_QuestionType_six': 50.07, 'test/accuracy_AnswerType_other': 52.06, 'test/epoch': 1}[0m
Epoch 1, global step 564: 'test/accuracy_overall' reached 52.06000 (best 52.06000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-T5-CLIP-32.21804773/train/saved_model/model_1.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5673801030519223, 'test/exact_match_at_2': 0.69124058660325, 'test/exact_match_at_3': 0.7251288149028934, 'test/exact_match_at_4': 0.7364248910027745, 'test/exact_match_at_5': 0.7386048355132778, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37209671026555685, 'test/gold_recall_at_5': 0.6642885453824812, 'test/successful_hit': 0.3648434403487911, 'test/successful_no_hit': 0.16246531906460562, 'test/failed_hit': 0.295243757431629, 'test/failed_no_hit': 0.17744748315497424, 'test/selected_successful_hit': 0.43777249306381294, 'test/selected_successful_no_hit': 0.12960760998810938, 'test/selected_failed_hit': 0.32441537851763774, 'test/selected_failed_no_hit': 0.10820451843043995, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 52.05, 'test/accuracy_QuestionType_one': 48.68, 'test/accuracy_QuestionType_eight': 51.3, 'test/accuracy_QuestionType_other': 53.38, 'test/accuracy_QuestionType_seven': 50.65, 'test/accuracy_QuestionType_four': 58.1, 'test/accuracy_QuestionType_five': 54.31, 'test/accuracy_QuestionType_three': 49.95, 'test/accuracy_QuestionType_nine': 43.81, 'test/accuracy_QuestionType_ten': 50.54, 'test/accuracy_QuestionType_two': 51.98, 'test/accuracy_QuestionType_six': 49.79, 'test/accuracy_AnswerType_other': 52.05, 'test/epoch': 2}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5705509314308363, 'test/exact_match_at_2': 0.69124058660325, 'test/exact_match_at_3': 0.727705112960761, 'test/exact_match_at_4': 0.7405866032500991, 'test/exact_match_at_5': 0.7425683709869203, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37209671026555685, 'test/gold_recall_at_5': 0.6642885453824812, 'test/successful_hit': 0.3676575505350773, 'test/successful_no_hit': 0.1555687673404677, 'test/failed_hit': 0.30919540229885056, 'test/failed_no_hit': 0.16757827982560444, 'test/selected_successful_hit': 0.44887039239001186, 'test/selected_successful_no_hit': 0.12168053904082442, 'test/selected_failed_hit': 0.33075703527546574, 'test/selected_failed_no_hit': 0.09869203329369798, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 52.54, 'test/accuracy_QuestionType_one': 48.0, 'test/accuracy_QuestionType_eight': 50.37, 'test/accuracy_QuestionType_other': 55.67, 'test/accuracy_QuestionType_seven': 52.01, 'test/accuracy_QuestionType_four': 58.91, 'test/accuracy_QuestionType_five': 53.69, 'test/accuracy_QuestionType_three': 52.2, 'test/accuracy_QuestionType_nine': 46.67, 'test/accuracy_QuestionType_ten': 53.64, 'test/accuracy_QuestionType_two': 51.51, 'test/accuracy_QuestionType_six': 52.62, 'test/accuracy_AnswerType_other': 52.54, 'test/epoch': 2}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5784780023781213, 'test/exact_match_at_2': 0.7003567181926278, 'test/exact_match_at_3': 0.7316686484344035, 'test/exact_match_at_4': 0.7401902497027348, 'test/exact_match_at_5': 0.7415774871185097, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37209671026555685, 'test/gold_recall_at_5': 0.6642885453824812, 'test/successful_hit': 0.36420927467300834, 'test/successful_no_hit': 0.17586206896551723, 'test/failed_hit': 0.27847800237812126, 'test/failed_no_hit': 0.18145065398335314, 'test/selected_successful_hit': 0.43856520015854145, 'test/selected_successful_no_hit': 0.13991280221957986, 'test/selected_failed_hit': 0.30578676179151804, 'test/selected_failed_no_hit': 0.11573523583036067, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.13, 'test/accuracy_QuestionType_one': 49.35, 'test/accuracy_QuestionType_eight': 52.19, 'test/accuracy_QuestionType_other': 55.09, 'test/accuracy_QuestionType_seven': 51.82, 'test/accuracy_QuestionType_four': 59.61, 'test/accuracy_QuestionType_five': 54.21, 'test/accuracy_QuestionType_three': 51.59, 'test/accuracy_QuestionType_nine': 47.14, 'test/accuracy_QuestionType_ten': 55.66, 'test/accuracy_QuestionType_two': 53.02, 'test/accuracy_QuestionType_six': 49.93, 'test/accuracy_AnswerType_other': 53.13, 'test/epoch': 2}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5731272294887039, 'test/exact_match_at_2': 0.6961950059453033, 'test/exact_match_at_3': 0.7279032897344431, 'test/exact_match_at_4': 0.7376139516448672, 'test/exact_match_at_5': 0.7399920729290527, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37209671026555685, 'test/gold_recall_at_5': 0.6642885453824812, 'test/successful_hit': 0.3670233848592945, 'test/successful_no_hit': 0.16864843440348792, 'test/failed_hit': 0.2900118906064209, 'test/failed_no_hit': 0.17431629013079666, 'test/selected_successful_hit': 0.4383670233848593, 'test/selected_successful_no_hit': 0.13476020610384462, 'test/selected_failed_hit': 0.3158937772493064, 'test/selected_failed_no_hit': 0.11097899326198969, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 52.64, 'test/accuracy_QuestionType_one': 48.19, 'test/accuracy_QuestionType_eight': 51.4, 'test/accuracy_QuestionType_other': 54.18, 'test/accuracy_QuestionType_seven': 52.15, 'test/accuracy_QuestionType_four': 59.79, 'test/accuracy_QuestionType_five': 54.53, 'test/accuracy_QuestionType_three': 50.47, 'test/accuracy_QuestionType_nine': 42.86, 'test/accuracy_QuestionType_ten': 55.81, 'test/accuracy_QuestionType_two': 53.49, 'test/accuracy_QuestionType_six': 49.36, 'test/accuracy_AnswerType_other': 52.64, 'test/epoch': 2}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5778834720570749, 'test/exact_match_at_2': 0.70055489496631, 'test/exact_match_at_3': 0.734046769718589, 'test/exact_match_at_4': 0.7459373761395165, 'test/exact_match_at_5': 0.7475227903289734, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37209671026555685, 'test/gold_recall_at_5': 0.6642885453824812, 'test/successful_hit': 0.36852952833927866, 'test/successful_no_hit': 0.1681331747919144, 'test/failed_hit': 0.2840665873959572, 'test/failed_no_hit': 0.1792707094728498, 'test/selected_successful_hit': 0.4456995640110979, 'test/selected_successful_no_hit': 0.13218390804597702, 'test/selected_failed_hit': 0.3151010701545779, 'test/selected_failed_no_hit': 0.1070154577883472, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.06, 'test/accuracy_QuestionType_one': 49.5, 'test/accuracy_QuestionType_eight': 51.44, 'test/accuracy_QuestionType_other': 55.19, 'test/accuracy_QuestionType_seven': 52.94, 'test/accuracy_QuestionType_four': 59.61, 'test/accuracy_QuestionType_five': 54.68, 'test/accuracy_QuestionType_three': 50.93, 'test/accuracy_QuestionType_nine': 42.14, 'test/accuracy_QuestionType_ten': 53.18, 'test/accuracy_QuestionType_two': 53.37, 'test/accuracy_QuestionType_six': 51.49, 'test/accuracy_AnswerType_other': 53.06, 'test/epoch': 2}[0m
Epoch 2, global step 846: 'test/accuracy_overall' reached 53.06000 (best 53.06000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-T5-CLIP-32.21804773/train/saved_model/model_2.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5802615933412604, 'test/exact_match_at_2': 0.6985731272294887, 'test/exact_match_at_3': 0.7350376535869996, 'test/exact_match_at_4': 0.7429647245342846, 'test/exact_match_at_5': 0.7445501387237415, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37209671026555685, 'test/gold_recall_at_5': 0.6642885453824812, 'test/successful_hit': 0.3681728101466508, 'test/successful_no_hit': 0.17384066587395958, 'test/failed_hit': 0.271264367816092, 'test/failed_no_hit': 0.18672215616329765, 'test/selected_successful_hit': 0.44351961950059454, 'test/selected_successful_no_hit': 0.1367419738406659, 'test/selected_failed_hit': 0.30439952437574314, 'test/selected_failed_no_hit': 0.11533888228299644, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.37, 'test/accuracy_QuestionType_one': 49.37, 'test/accuracy_QuestionType_eight': 50.79, 'test/accuracy_QuestionType_other': 55.32, 'test/accuracy_QuestionType_seven': 53.04, 'test/accuracy_QuestionType_four': 61.09, 'test/accuracy_QuestionType_five': 54.73, 'test/accuracy_QuestionType_three': 52.94, 'test/accuracy_QuestionType_nine': 45.24, 'test/accuracy_QuestionType_ten': 54.73, 'test/accuracy_QuestionType_two': 53.84, 'test/accuracy_QuestionType_six': 50.21, 'test/accuracy_AnswerType_other': 53.37, 'test/epoch': 3}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5862068965517241, 'test/exact_match_at_2': 0.702338485929449, 'test/exact_match_at_3': 0.7362267142290924, 'test/exact_match_at_4': 0.7431629013079667, 'test/exact_match_at_5': 0.7453428458184701, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37209671026555685, 'test/gold_recall_at_5': 0.6642885453824812, 'test/successful_hit': 0.368093539437178, 'test/successful_no_hit': 0.1794688862465319, 'test/failed_hit': 0.2612762584225129, 'test/failed_no_hit': 0.19116131589377724, 'test/selected_successful_hit': 0.441141498216409, 'test/selected_successful_no_hit': 0.1450653983353151, 'test/selected_failed_hit': 0.29429250891795483, 'test/selected_failed_no_hit': 0.11950059453032105, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.88, 'test/accuracy_QuestionType_one': 49.52, 'test/accuracy_QuestionType_eight': 51.74, 'test/accuracy_QuestionType_other': 57.06, 'test/accuracy_QuestionType_seven': 52.06, 'test/accuracy_QuestionType_four': 61.55, 'test/accuracy_QuestionType_five': 55.55, 'test/accuracy_QuestionType_three': 52.29, 'test/accuracy_QuestionType_nine': 47.62, 'test/accuracy_QuestionType_ten': 55.97, 'test/accuracy_QuestionType_two': 53.6, 'test/accuracy_QuestionType_six': 50.21, 'test/accuracy_AnswerType_other': 53.88, 'test/epoch': 3}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5846214823622672, 'test/exact_match_at_2': 0.702140309155767, 'test/exact_match_at_3': 0.7326595323028141, 'test/exact_match_at_4': 0.7415774871185097, 'test/exact_match_at_5': 0.7435592548553309, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37209671026555685, 'test/gold_recall_at_5': 0.6642885453824812, 'test/successful_hit': 0.3654776060245739, 'test/successful_no_hit': 0.1802615933412604, 'test/failed_hit': 0.26377328577090764, 'test/failed_no_hit': 0.19048751486325802, 'test/selected_successful_hit': 0.4391597304795878, 'test/selected_successful_no_hit': 0.14546175188267935, 'test/selected_failed_hit': 0.2958779231074118, 'test/selected_failed_no_hit': 0.11950059453032105, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.73, 'test/accuracy_QuestionType_one': 50.36, 'test/accuracy_QuestionType_eight': 51.3, 'test/accuracy_QuestionType_other': 55.61, 'test/accuracy_QuestionType_seven': 52.38, 'test/accuracy_QuestionType_four': 61.52, 'test/accuracy_QuestionType_five': 54.63, 'test/accuracy_QuestionType_three': 52.8, 'test/accuracy_QuestionType_nine': 47.14, 'test/accuracy_QuestionType_ten': 54.73, 'test/accuracy_QuestionType_two': 54.53, 'test/accuracy_QuestionType_six': 52.2, 'test/accuracy_AnswerType_other': 53.73, 'test/epoch': 3}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5871977804201347, 'test/exact_match_at_2': 0.7080856123662307, 'test/exact_match_at_3': 0.7399920729290527, 'test/exact_match_at_4': 0.750099088386841, 'test/exact_match_at_5': 0.7522790328973444, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37209671026555685, 'test/gold_recall_at_5': 0.6642885453824812, 'test/successful_hit': 0.369124058660325, 'test/successful_no_hit': 0.17827982560443917, 'test/failed_hit': 0.27027348394768136, 'test/failed_no_hit': 0.1823226317875545, 'test/selected_successful_hit': 0.44451050336900516, 'test/selected_successful_no_hit': 0.1426872770511296, 'test/selected_failed_hit': 0.29964328180737215, 'test/selected_failed_no_hit': 0.11315893777249307, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.99, 'test/accuracy_QuestionType_one': 50.5, 'test/accuracy_QuestionType_eight': 51.58, 'test/accuracy_QuestionType_other': 56.25, 'test/accuracy_QuestionType_seven': 51.73, 'test/accuracy_QuestionType_four': 60.74, 'test/accuracy_QuestionType_five': 56.17, 'test/accuracy_QuestionType_three': 53.79, 'test/accuracy_QuestionType_nine': 44.52, 'test/accuracy_QuestionType_ten': 55.35, 'test/accuracy_QuestionType_two': 54.77, 'test/accuracy_QuestionType_six': 50.07, 'test/accuracy_AnswerType_other': 53.99, 'test/epoch': 3}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.584423305588585, 'test/exact_match_at_2': 0.7074910820451843, 'test/exact_match_at_3': 0.7397938961553706, 'test/exact_match_at_4': 0.749900911613159, 'test/exact_match_at_5': 0.751486325802616, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37209671026555685, 'test/gold_recall_at_5': 0.6642885453824812, 'test/successful_hit': 0.3711454617518827, 'test/successful_no_hit': 0.17606024573919937, 'test/failed_hit': 0.2772096710265557, 'test/failed_no_hit': 0.17558462148236226, 'test/selected_successful_hit': 0.44431232659532305, 'test/selected_successful_no_hit': 0.140110978993262, 'test/selected_failed_hit': 0.3036068172810147, 'test/selected_failed_no_hit': 0.11196987713040031, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.74, 'test/accuracy_QuestionType_one': 49.08, 'test/accuracy_QuestionType_eight': 51.07, 'test/accuracy_QuestionType_other': 57.58, 'test/accuracy_QuestionType_seven': 51.82, 'test/accuracy_QuestionType_four': 59.4, 'test/accuracy_QuestionType_five': 56.07, 'test/accuracy_QuestionType_three': 53.36, 'test/accuracy_QuestionType_nine': 44.05, 'test/accuracy_QuestionType_ten': 56.59, 'test/accuracy_QuestionType_two': 57.21, 'test/accuracy_QuestionType_six': 49.93, 'test/accuracy_AnswerType_other': 53.74, 'test/epoch': 3}[0m
Epoch 3, global step 1128: 'test/accuracy_overall' reached 53.74000 (best 53.74000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-T5-CLIP-32.21804773/train/saved_model/model_3.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5854141894569956, 'test/exact_match_at_2': 0.7041220768925882, 'test/exact_match_at_3': 0.7320650019817677, 'test/exact_match_at_4': 0.7407847800237812, 'test/exact_match_at_5': 0.7429647245342846, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37209671026555685, 'test/gold_recall_at_5': 0.6642885453824812, 'test/successful_hit': 0.36741973840665876, 'test/successful_no_hit': 0.18196591359492667, 'test/failed_hit': 0.26072136345620295, 'test/failed_no_hit': 0.18989298454221165, 'test/selected_successful_hit': 0.44193420531113753, 'test/selected_successful_no_hit': 0.1434799841458581, 'test/selected_failed_hit': 0.2911216805390408, 'test/selected_failed_no_hit': 0.12346413000396353, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.72, 'test/accuracy_QuestionType_one': 49.74, 'test/accuracy_QuestionType_eight': 51.19, 'test/accuracy_QuestionType_other': 57.22, 'test/accuracy_QuestionType_seven': 51.68, 'test/accuracy_QuestionType_four': 59.61, 'test/accuracy_QuestionType_five': 55.43, 'test/accuracy_QuestionType_three': 54.11, 'test/accuracy_QuestionType_nine': 44.52, 'test/accuracy_QuestionType_ten': 58.6, 'test/accuracy_QuestionType_two': 53.84, 'test/accuracy_QuestionType_six': 49.22, 'test/accuracy_AnswerType_other': 53.72, 'test/epoch': 4}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5893777249306381, 'test/exact_match_at_2': 0.7013476020610384, 'test/exact_match_at_3': 0.7281014665081252, 'test/exact_match_at_4': 0.7366230677764566, 'test/exact_match_at_5': 0.7374157748711851, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37209671026555685, 'test/gold_recall_at_5': 0.6642885453824812, 'test/successful_hit': 0.3637732857709077, 'test/successful_no_hit': 0.187871581450654, 'test/failed_hit': 0.25105033690051526, 'test/failed_no_hit': 0.1973047958779231, 'test/selected_successful_hit': 0.43717796274276655, 'test/selected_successful_no_hit': 0.15219976218787157, 'test/selected_failed_hit': 0.28160919540229884, 'test/selected_failed_no_hit': 0.12901307966706302, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 54.04, 'test/accuracy_QuestionType_one': 50.16, 'test/accuracy_QuestionType_eight': 51.79, 'test/accuracy_QuestionType_other': 57.03, 'test/accuracy_QuestionType_seven': 52.1, 'test/accuracy_QuestionType_four': 59.96, 'test/accuracy_QuestionType_five': 54.86, 'test/accuracy_QuestionType_three': 54.67, 'test/accuracy_QuestionType_nine': 44.76, 'test/accuracy_QuestionType_ten': 56.9, 'test/accuracy_QuestionType_two': 56.98, 'test/accuracy_QuestionType_six': 51.77, 'test/accuracy_AnswerType_other': 54.04, 'test/epoch': 4}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5887831946095917, 'test/exact_match_at_2': 0.702338485929449, 'test/exact_match_at_3': 0.7318668252080857, 'test/exact_match_at_4': 0.73880301228696, 'test/exact_match_at_5': 0.7395957193816884, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37209671026555685, 'test/gold_recall_at_5': 0.6642885453824812, 'test/successful_hit': 0.36563614744351963, 'test/successful_no_hit': 0.18755449861276258, 'test/failed_hit': 0.2537059056678557, 'test/failed_no_hit': 0.19310344827586207, 'test/selected_successful_hit': 0.4361870788743559, 'test/selected_successful_no_hit': 0.15259611573523582, 'test/selected_failed_hit': 0.28240190249702735, 'test/selected_failed_no_hit': 0.1288149028933809, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 54.06, 'test/accuracy_QuestionType_one': 48.91, 'test/accuracy_QuestionType_eight': 51.95, 'test/accuracy_QuestionType_other': 56.64, 'test/accuracy_QuestionType_seven': 52.57, 'test/accuracy_QuestionType_four': 61.27, 'test/accuracy_QuestionType_five': 55.68, 'test/accuracy_QuestionType_three': 53.55, 'test/accuracy_QuestionType_nine': 45.95, 'test/accuracy_QuestionType_ten': 57.05, 'test/accuracy_QuestionType_two': 56.51, 'test/accuracy_QuestionType_six': 52.2, 'test/accuracy_AnswerType_other': 54.06, 'test/epoch': 4}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5848196591359492, 'test/exact_match_at_2': 0.7053111375346809, 'test/exact_match_at_3': 0.7378121284185494, 'test/exact_match_at_4': 0.7473246135552913, 'test/exact_match_at_5': 0.748513674197384, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37209671026555685, 'test/gold_recall_at_5': 0.6642885453824812, 'test/successful_hit': 0.3693222354340071, 'test/successful_no_hit': 0.1785176377328577, 'test/failed_hit': 0.2697185889813714, 'test/failed_no_hit': 0.18244153785176379, 'test/selected_successful_hit': 0.4403487911216805, 'test/selected_successful_no_hit': 0.14447086801426873, 'test/selected_failed_hit': 0.2978596908442331, 'test/selected_failed_no_hit': 0.11732065001981767, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.7, 'test/accuracy_QuestionType_one': 49.42, 'test/accuracy_QuestionType_eight': 51.49, 'test/accuracy_QuestionType_other': 55.8, 'test/accuracy_QuestionType_seven': 52.1, 'test/accuracy_QuestionType_four': 61.52, 'test/accuracy_QuestionType_five': 55.68, 'test/accuracy_QuestionType_three': 51.92, 'test/accuracy_QuestionType_nine': 45.24, 'test/accuracy_QuestionType_ten': 54.42, 'test/accuracy_QuestionType_two': 56.86, 'test/accuracy_QuestionType_six': 50.78, 'test/accuracy_AnswerType_other': 53.7, 'test/epoch': 4}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5869996036464527, 'test/exact_match_at_2': 0.7053111375346809, 'test/exact_match_at_3': 0.7346413000396353, 'test/exact_match_at_4': 0.7441537851763773, 'test/exact_match_at_5': 0.7457391993658343, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37209671026555685, 'test/gold_recall_at_5': 0.6642885453824812, 'test/successful_hit': 0.36940150614348, 'test/successful_no_hit': 0.1817281014665081, 'test/failed_hit': 0.26143479984145856, 'test/failed_no_hit': 0.1874355925485533, 'test/selected_successful_hit': 0.44272691240586604, 'test/selected_successful_no_hit': 0.14427269124058661, 'test/selected_failed_hit': 0.2923107411811336, 'test/selected_failed_no_hit': 0.1206896551724138, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.85, 'test/accuracy_QuestionType_one': 49.99, 'test/accuracy_QuestionType_eight': 52.21, 'test/accuracy_QuestionType_other': 56.12, 'test/accuracy_QuestionType_seven': 52.29, 'test/accuracy_QuestionType_four': 60.95, 'test/accuracy_QuestionType_five': 54.58, 'test/accuracy_QuestionType_three': 53.83, 'test/accuracy_QuestionType_nine': 45.24, 'test/accuracy_QuestionType_ten': 53.18, 'test/accuracy_QuestionType_two': 55.7, 'test/accuracy_QuestionType_six': 51.63, 'test/accuracy_AnswerType_other': 53.85, 'test/epoch': 4}[0m
Epoch 4, global step 1410: 'test/accuracy_overall' reached 53.85000 (best 53.85000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-T5-CLIP-32.21804773/train/saved_model/model_4.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5903686087990487, 'test/exact_match_at_2': 0.7045184304399524, 'test/exact_match_at_3': 0.734046769718589, 'test/exact_match_at_4': 0.7423701942132382, 'test/exact_match_at_5': 0.7439556084026953, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37209671026555685, 'test/gold_recall_at_5': 0.6642885453824812, 'test/successful_hit': 0.36741973840665876, 'test/successful_no_hit': 0.18430439952437574, 'test/failed_hit': 0.2568767340467697, 'test/failed_no_hit': 0.1913991280221958, 'test/selected_successful_hit': 0.44272691240586604, 'test/selected_successful_no_hit': 0.14764169639318273, 'test/selected_failed_hit': 0.28398731668648436, 'test/selected_failed_no_hit': 0.1256440745144669, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 54.11, 'test/accuracy_QuestionType_one': 49.47, 'test/accuracy_QuestionType_eight': 52.23, 'test/accuracy_QuestionType_other': 56.9, 'test/accuracy_QuestionType_seven': 51.45, 'test/accuracy_QuestionType_four': 60.92, 'test/accuracy_QuestionType_five': 55.85, 'test/accuracy_QuestionType_three': 54.16, 'test/accuracy_QuestionType_nine': 44.05, 'test/accuracy_QuestionType_ten': 54.42, 'test/accuracy_QuestionType_two': 56.86, 'test/accuracy_QuestionType_six': 52.91, 'test/accuracy_AnswerType_other': 54.11, 'test/epoch': 5}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5871977804201347, 'test/exact_match_at_2': 0.7035275465715418, 'test/exact_match_at_3': 0.7306777645659929, 'test/exact_match_at_4': 0.7423701942132382, 'test/exact_match_at_5': 0.7445501387237415, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.37209671026555685, 'test/gold_recall_at_5': 0.6642885453824812, 'test/successful_hit': 0.36662703131193025, 'test/successful_no_hit': 0.18045977011494252, 'test/failed_hit': 0.2613158937772493, 'test/failed_no_hit': 0.19159730479587792, 'test/selected_successful_hit': 0.4423305588585018, 'test/selected_successful_no_hit': 0.14486722156163298, 'test/selected_failed_hit': 0.28973444312326596, 'test/selected_failed_no_hit': 0.12306777645659929, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.88, 'test/accuracy_QuestionType_one': 50.26, 'test/accuracy_QuestionType_eight': 52.0, 'test/accuracy_QuestionType_other': 57.16, 'test/accuracy_QuestionType_seven': 51.07, 'test/accuracy_QuestionType_four': 61.02, 'test/accuracy_QuestionType_five': 54.53, 'test/accuracy_QuestionType_three': 52.9, 'test/accuracy_QuestionType_nine': 45.24, 'test/accuracy_QuestionType_ten': 55.97, 'test/accuracy_QuestionType_two': 56.16, 'test/accuracy_QuestionType_six': 51.49, 'test/accuracy_AnswerType_other': 53.88, 'test/epoch': 5}[0m
