/home/xl544/.conda/envs/RAVQA/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning
  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')
[38;20m[INFO] - __main__ : Initialization done with the config: {'DATA_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data', 'EXPERIMENT_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments', 'TENSORBOARD_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data_TB/tb_logs', 'WANDB': {'entity': 'xl544', 'project': 'RAVQA', 'tags': ['OKVQA', 'RAG', 'freeze_question_encoder', 'force_existence'], 'name': 'OKVQA_RA-VQA-T5-CLIP-32.21804959'}, 'cache': {'default_folder': '../data/ok-vqa/cache', 'regenerate': {'clip_embeddings': 0, 'ocr_feature_preprocessed': 0, 'test_data_preprocessed': 0, 'train_data_preprocessed': 0, 'vinvl_feature_preprocessed': 0}}, 'cuda': 0, 'data_loader': {'additional': {'max_decoder_source_length': 512, 'max_source_length': 512, 'max_target_length': 10, 'num_knowledge_passages': 5}, 'dataset_modules': {'module_dict': {'LoadClipEmbeddings': {'config': {'train': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_train2014.pkl', 'val': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_val2014.pkl'}, 'option': 'default', 'type': 'EmbeddingInput'}, 'LoadGoogleOCRFeatures': {'config': {'combine_with_vinvl': True, 'test': '../data/ok-vqa/pre-extracted_features/OCR/valid', 'train': '../data/ok-vqa/pre-extracted_features/OCR/train'}, 'option': 'default', 'type': 'LoadGoogleOCRFeatures'}, 'LoadGoogleSearchAnnotations': {'config': {'annotations_path': {'test': '../data/ok-vqa/pre-extracted_features/passages/retriever_test.json', 'train': '../data/ok-vqa/pre-extracted_features/passages/retriever_train.json', 'valid': '../data/ok-vqa/pre-extracted_features/passages/retriever_testdev.json'}}, 'option': 'default', 'type': 'LoadGoogleSearchAnnotations'}, 'LoadGoogleSearchPassageData': {'config': {'passage_data_path': {'full': '../data/ok-vqa/pre-extracted_features/passages/okvqa_full_corpus.csv', 'train': '../data/ok-vqa/pre-extracted_features/passages/okvqa_train_corpus.csv'}, 'use_full_split': True}, 'option': 'default', 'type': 'LoadGoogleSearchPassageData'}, 'LoadOKVQAData': {'config': {'image_data_path': {'test': '../data/ok-vqa/val2014', 'train': '../data/ok-vqa/train2014'}, 'vqa_data_path': {'annotation_files': {'test': '../data/ok-vqa/mscoco_val2014_annotations.json', 'train': '../data/ok-vqa/mscoco_train2014_annotations.json'}, 'question_files': {'test': '../data/ok-vqa/OpenEnded_mscoco_val2014_questions.json', 'train': '../data/ok-vqa/OpenEnded_mscoco_train2014_questions.json'}}}, 'option': 'default', 'type': 'LoadOKVQAData'}, 'LoadOscarCaptionFeatures': {'config': {'test': '../data/ok-vqa/pre-extracted_features/captions/test_predictions.json', 'train': '../data/ok-vqa/pre-extracted_features/captions/train_predictions.json', 'valid': '../data/ok-vqa/pre-extracted_features/captions/valid_predictions.json'}, 'option': 'default', 'type': 'LoadOscarCaptionFeatures'}, 'LoadPretrainedDPROutputForGoogleSearchPassage': {'config': {'pretrained_dpr_outputs': {'test': '../Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/test/test_evaluation/test_predictions.json', 'train': '../Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/test/test_evaluation/train_predictions.json'}}, 'option': 'none', 'type': 'LoadPretrainedDPROutputForGoogleSearchPassage'}, 'LoadVinVLFeatures': {'config': {'test': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_testset_full/inference/vinvl_vg_x152c4/predictions.tsv', 'train': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_trainset_full/inference/vinvl_vg_x152c4/predictions.tsv'}, 'option': 'default', 'type': 'LoadVinVLFeatures'}}, 'module_list': ['LoadVinVLFeatures', 'LoadGoogleOCRFeatures', 'LoadOscarCaptionFeatures', 'LoadOKVQAData', 'LoadGoogleSearchPassageData', 'LoadClipEmbeddings']}, 'dataset_type': 'OKVQADataset', 'dummy_dataloader': 0, 'index_files': {'index_passages_path': '../data/ok-vqa/pre-extracted_features/faiss/ok-vqa-passages-full-caption-pretrained-NewRun/my_knowledge_dataset', 'index_path': '../data/ok-vqa/pre-extracted_features/faiss/ok-vqa-passages-full-caption-pretrained-NewRun/my_knowledge_dataset_hnsw_index.faiss'}, 'type': 'DataLoaderOKVQAWithKnowledge'}, 'experiment_name': 'OKVQA_RA-VQA-T5-CLIP-32.21804959', 'gpu_device': 0, 'ignore_pretrained_weights': [], 'metrics': [{'name': 'compute_exact_match'}, {'name': 'compute_retrieval_metrics'}, {'name': 'compute_okvqa_scores'}], 'model_config': {'DECODER_SPECIAL_TOKENS': {'additional_special_tokens': ['<BOV>', '<SOV>', '<EOV>', '<BOQ>', '<EOQ>', '<BOC>', '<EOC>', '<BOK>', '<EOK>'], 'bos_token': '<PAD>', 'pad_token': '<PAD>'}, 'DecoderTokenizerClass': 'T5Tokenizer', 'DecoderTokenizerModelVersion': 't5-large', 'GeneratorConfigClass': 'T5Config', 'GeneratorModelClass': 'T5ForConditionalGeneration', 'GeneratorModelVersion': 't5-large', 'LoadPretrainedMLPWeights': 0, 'ModelClass': 'RagModel', 'PretrainedMLPPath': '', 'QueryEncoderConfigClass': 'DPRConfig', 'QueryEncoderModelClass': 'DPRQuestionEncoder', 'QueryEncoderModelVersion': '../Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/train/saved_model/epoch6/query_encoder', 'RAVQA_loss_type': 'Approach6', 'SPECIAL_TOKENS': {'additional_special_tokens': ['<BOV>', '<SOV>', '<EOV>', '<BOQ>', '<EOQ>', '<BOC>', '<EOC>', '<BOK>', '<EOK>']}, 'TokenizerClass': 'DPRQuestionEncoderTokenizer', 'TokenizerModelVersion': 'facebook/dpr-question_encoder-single-nq-base', 'UsePrefixEmb': 1, 'base_model': 'RAG', 'decoder_input_modules': {'module_list': [], 'postprocess_module_list': []}, 'input_modules': {'module_list': [{'option': 'default', 'separation_tokens': {'end': '<EOQ>', 'start': '<BOQ>'}, 'type': 'QuestionInput'}, {'option': 'caption', 'separation_tokens': {'end': '<EOC>', 'start': '<BOC>'}, 'type': 'TextBasedVisionInput'}, {'attribute_max': 3, 'attribute_thres': 0.05, 'object_max': 40, 'ocr': 1, 'option': 'object', 'separation_tokens': {'end': '<EOV>', 'sep': '<SOV>', 'start': '<BOV>'}, 'type': 'TextBasedVisionInput'}, {'option': 'default', 'type': 'EmbeddingInput'}], 'postprocess_module_list': [{'option': 'default', 'type': 'PostProcessInputTokenization'}, {'option': 'default', 'type': 'PostProcessClipEmbeddings'}]}, 'loss_ratio': {'additional_loss': 0, 'nll_loss': 1, 'rag_loss': 0, 'retrieval_pseudo_loss': 0}, 'modules': ['freeze_question_encoder', 'force_existence'], 'output_modules': {'module_list': [{'option': 'default', 'type': 'GenerationOutput'}], 'postprocess_module_list': [{'option': 'default', 'type': 'PostProcessOutputTokenization'}]}, 'pretrained': 1, 'rag_modules': {'module_list': []}}, 'platform_type': 'pytorch', 'seed': 2021, 'test': {'additional': {'multiprocessing': 4}, 'batch_size': 32, 'evaluation_name': 'test_evaluation', 'load_best_model': 0, 'load_epoch': -1, 'load_model_path': '', 'num_evaluation': 0}, 'train': {'adam_epsilon': 1e-08, 'additional': {'gradient_accumulation_steps': 16, 'gradient_clipping': 0, 'plugins': [], 'save_top_k': 1, 'save_top_k_metric': 'test/accuracy_overall', 'save_top_k_mode': 'max', 'warmup_steps': 0}, 'batch_size': 2, 'epochs': 8, 'load_best_model': 0, 'load_epoch': -1, 'load_model_path': '', 'lr': 6e-05, 'retriever_lr': 1e-05, 'save_interval': 1, 'scheduler': 'linear', 'type': 'RagExecutor'}, 'valid': {'additional': {}, 'batch_size': 32, 'break_interval': 3000, 'step_size': 0.2}, 'reset': False, 'mode': 'train', 'log_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-T5-CLIP-32.21804959/train', 'experiment_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-T5-CLIP-32.21804959', 'saved_model_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-T5-CLIP-32.21804959/train/saved_model', 'imgs_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-T5-CLIP-32.21804959/train/imgs', 'tensorboard_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data_TB/tb_logs/OKVQA_RA-VQA-T5-CLIP-32.21804959', 'args': {'config': '../configs/okvqa/RAVQA.jsonnet', 'DATA_FOLDER': '', 'EXPERIMENT_FOLDER': '', 'mode': 'train', 'reset': False, 'experiment_name': 'OKVQA_RA-VQA-T5-CLIP-32.21804959', 'tags': [], 'modules': ['freeze_question_encoder', 'force_existence'], 'log_prediction_tables': False, 'test_batch_size': -1, 'test_evaluation_name': '', 'logger': True, 'checkpoint_callback': None, 'enable_checkpointing': True, 'default_root_dir': None, 'gradient_clip_val': None, 'gradient_clip_algorithm': None, 'process_position': 0, 'num_nodes': 1, 'num_processes': None, 'devices': '1', 'gpus': None, 'auto_select_gpus': False, 'tpu_cores': None, 'ipus': None, 'log_gpu_memory': None, 'progress_bar_refresh_rate': None, 'enable_progress_bar': True, 'overfit_batches': 0.0, 'track_grad_norm': -1, 'check_val_every_n_epoch': 1, 'fast_dev_run': False, 'accumulate_grad_batches': None, 'max_epochs': None, 'min_epochs': None, 'max_steps': -1, 'min_steps': None, 'max_time': None, 'limit_train_batches': None, 'limit_val_batches': None, 'limit_test_batches': None, 'limit_predict_batches': None, 'val_check_interval': None, 'flush_logs_every_n_steps': None, 'log_every_n_steps': 50, 'accelerator': 'auto', 'strategy': None, 'sync_batchnorm': False, 'precision': 32, 'enable_model_summary': True, 'weights_summary': 'top', 'weights_save_path': None, 'num_sanity_val_steps': 2, 'resume_from_checkpoint': None, 'profiler': None, 'benchmark': None, 'deterministic': None, 'reload_dataloaders_every_n_epochs': 0, 'auto_lr_find': False, 'replace_sampler_ddp': True, 'detect_anomaly': False, 'auto_scale_batch_size': False, 'prepare_data_per_node': None, 'plugins': None, 'amp_backend': 'native', 'amp_level': None, 'move_metrics_to_cpu': False, 'multiple_trainloader_mode': 'max_size_cycle', 'stochastic_weight_avg': False, 'terminate_on_nan': None, 'opts': ['train.epochs=8', 'train.batch_size=2', 'valid.step_size=0.2', 'valid.batch_size=32', 'train.additional.gradient_accumulation_steps=16', 'train.lr=0.00006', 'train.retriever_lr=0.00001', 'train.scheduler=linear', 'data_loader.additional.num_knowledge_passages=5', 'model_config.UsePrefixEmb=1', 'model_config.DecoderTokenizerModelVersion=t5-large', 'model_config.GeneratorModelVersion=t5-large']}}[0m
Global seed set to 2021
[38;20m[INFO] - __main__ : All seeds have been set to 2021[0m
[38;20m[INFO] - __main__ : init wandb logger with the following settings: {'entity': 'xl544', 'project': 'RAVQA', 'tags': ['OKVQA', 'RAG', 'freeze_question_encoder', 'force_existence'], 'name': 'OKVQA_RA-VQA-T5-CLIP-32.21804959'}[0m
wandb: Currently logged in as: xl544. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.2
wandb: Run data is saved locally in /rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/src/wandb/run-20230606_022853-e80wxw1s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run OKVQA_RA-VQA-T5-CLIP-32.21804959
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xl544/RAVQA
wandb: üöÄ View run at https://wandb.ai/xl544/RAVQA/runs/e80wxw1s
Multiprocessing is handled by SLURM.
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[38;20m[INFO] - __main__ : arguments passed to trainer: Namespace(DATA_FOLDER='', EXPERIMENT_FOLDER='', accelerator='auto', accumulate_grad_batches=None, amp_backend='native', amp_level=None, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, benchmark=None, check_val_every_n_epoch=1, checkpoint_callback=None, config='../configs/okvqa/RAVQA.jsonnet', default_root_dir=None, detect_anomaly=False, deterministic=None, devices='1', enable_checkpointing=True, enable_model_summary=True, enable_progress_bar=True, experiment_name='OKVQA_RA-VQA-T5-CLIP-32.21804959', fast_dev_run=False, flush_logs_every_n_steps=None, gpus=None, gradient_clip_algorithm=None, gradient_clip_val=None, ipus=None, limit_predict_batches=None, limit_test_batches=None, limit_train_batches=None, limit_val_batches=None, log_every_n_steps=50, log_gpu_memory=None, log_prediction_tables=False, logger=True, max_epochs=None, max_steps=-1, max_time=None, min_epochs=None, min_steps=None, mode='train', modules=['freeze_question_encoder', 'force_existence'], move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=None, num_sanity_val_steps=2, opts=['train.epochs=8', 'train.batch_size=2', 'valid.step_size=0.2', 'valid.batch_size=32', 'train.additional.gradient_accumulation_steps=16', 'train.lr=0.00006', 'train.retriever_lr=0.00001', 'train.scheduler=linear', 'data_loader.additional.num_knowledge_passages=5', 'model_config.UsePrefixEmb=1', 'model_config.DecoderTokenizerModelVersion=t5-large', 'model_config.GeneratorModelVersion=t5-large'], overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=None, process_position=0, profiler=None, progress_bar_refresh_rate=None, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, reset=False, resume_from_checkpoint=None, stochastic_weight_avg=False, strategy=None, sync_batchnorm=False, tags=[], terminate_on_nan=None, test_batch_size=-1, test_evaluation_name='', tpu_cores=None, track_grad_norm=-1, val_check_interval=None, weights_save_path=None, weights_summary='top')[0m
[38;20m[INFO] - __main__ : additional arguments passed to trainer: {'accumulate_grad_batches': 16, 'default_root_dir': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-T5-CLIP-32.21804959/train/saved_model', 'max_epochs': 8, 'logger': [<pytorch_lightning.loggers.tensorboard.TensorBoardLogger object at 0x14bbaf5f7fd0>, <pytorch_lightning.loggers.wandb.WandbLogger object at 0x14bbaf580d30>, <utils.metrics_log_callback.MetricsHistoryLogger object at 0x14bc300dedc0>], 'callbacks': [<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x14bbaf580700>, <pytorch_lightning.callbacks.progress.tqdm_progress.TQDMProgressBar object at 0x14bbaec655b0>, <pytorch_lightning.callbacks.model_summary.ModelSummary object at 0x14bbaec65a30>, <pytorch_lightning.callbacks.gradient_accumulation_scheduler.GradientAccumulationScheduler object at 0x14bbaec65a90>], 'plugins': [], 'log_every_n_steps': 100, 'val_check_interval': 0.2}[0m
[33;20m[WARNING] - __main__ : No checkpoint exists from '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-T5-CLIP-32.21804959/train/saved_model/last.ckpt'. Skipping...[0m
[38;20m[INFO] - __main__ : **First time to train**[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'test': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_testset_full/inference/vinvl_vg_x152c4/predictions.tsv', 'train': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_trainset_full/inference/vinvl_vg_x152c4/predictions.tsv'}, 'option': 'default', 'type': 'LoadVinVLFeatures'}[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/vinvl_feature_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data Statistics] VinVL features 14031[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'combine_with_vinvl': True, 'test': '../data/ok-vqa/pre-extracted_features/OCR/valid', 'train': '../data/ok-vqa/pre-extracted_features/OCR/train'}, 'option': 'default', 'type': 'LoadGoogleOCRFeatures'}[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/ocr_feature_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data Statistics] OCR features 14031, 5462 has annotations.[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : OCR feature detected in VinVL feature dict...skipping..[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'test': '../data/ok-vqa/pre-extracted_features/captions/test_predictions.json', 'train': '../data/ok-vqa/pre-extracted_features/captions/train_predictions.json', 'valid': '../data/ok-vqa/pre-extracted_features/captions/valid_predictions.json'}, 'option': 'default', 'type': 'LoadOscarCaptionFeatures'}[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'image_data_path': {'test': '../data/ok-vqa/val2014', 'train': '../data/ok-vqa/train2014'}, 'vqa_data_path': {'annotation_files': {'test': '../data/ok-vqa/mscoco_val2014_annotations.json', 'train': '../data/ok-vqa/mscoco_train2014_annotations.json'}, 'question_files': {'test': '../data/ok-vqa/OpenEnded_mscoco_val2014_questions.json', 'train': '../data/ok-vqa/OpenEnded_mscoco_train2014_questions.json'}}}, 'option': 'default', 'type': 'LoadOKVQAData'}[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/train_data_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data statistics] split: train  entries: 9009[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/test_data_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data statistics] split: test  entries: 5046[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'passage_data_path': {'full': '../data/ok-vqa/pre-extracted_features/passages/okvqa_full_corpus.csv', 'train': '../data/ok-vqa/pre-extracted_features/passages/okvqa_train_corpus.csv'}, 'use_full_split': True}, 'option': 'default', 'type': 'LoadGoogleSearchPassageData'}[0m
  0%|          | 0/168380 [00:00<?, ?it/s]  9%|‚ñâ         | 15989/168380 [00:00<00:00, 159878.34it/s] 19%|‚ñà‚ñâ        | 32301/168380 [00:00<00:00, 161782.73it/s] 29%|‚ñà‚ñà‚ñâ       | 48803/168380 [00:00<00:00, 163258.48it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 65550/168380 [00:00<00:00, 164917.91it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 82229/168380 [00:00<00:00, 165589.33it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 98788/168380 [00:00<00:00, 165135.83it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 115453/168380 [00:00<00:00, 165627.40it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 134410/168380 [00:00<00:00, 173232.75it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 154182/168380 [00:00<00:00, 180877.82it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 168307/168380 [00:00<00:00, 173473.14it/s]
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'train': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_train2014.pkl', 'val': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_val2014.pkl'}, 'option': 'default', 'type': 'EmbeddingInput'}[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/clip_embeddings.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data Statistics] CLIP embeddings 123287[0m
[38;20m[INFO] - data_loader_manager.datasets.okvqa_datasets : initialising OKVQADataset...[0m
[38;20m[INFO] - data_loader_manager.datasets.okvqa_datasets : initialising OKVQADataset...[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa_with_knowledge : [Data Statistics]: training data loader: 4505;  test data loader: 158[0m
[38;20m[INFO] - torch.distributed.nn.jit.instantiator : Created a temporary directory at /tmp/tmp6dyzo891[0m
[38;20m[INFO] - torch.distributed.nn.jit.instantiator : Writing /tmp/tmp6dyzo891/_remote_module_non_scriptable.py[0m
[38;20m[INFO] - trainers.base_executor : Initializing RagExecutor...[0m
[38;20m[INFO] - __main__ : config file was successfully saved to /rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-T5-CLIP-32.21804959 for future use.[0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[38;20m[INFO] - trainers.rag_executor : using different learning rate for retriever[0m
[38;20m[INFO] - trainers.rag_executor : #params: 509   lr: 6e-05[0m
[38;20m[INFO] - trainers.rag_executor : #params: 4   lr: 1e-05[0m
Loading `train_dataloader` to estimate number of stepping batches.
/home/xl544/.conda/envs/RAVQA/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(

  | Name  | Type     | Params
-----------------------------------
0 | model | RagModel | 1.4 B 
-----------------------------------
1.3 B     Trainable params
109 M     Non-trainable params
1.4 B     Total params
5,586.567 Total estimated model params size (MB)
Missing logger folder: /rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data_TB/tb_logs/OKVQA_RA-VQA-T5-CLIP-32.21804959/OKVQA_RA-VQA-T5-CLIP-32.21804959
SLURM auto-requeueing enabled. Setting signal handlers.
/home/xl544/.conda/envs/RAVQA/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Failed to compute OKVQA scores: Results do not correspond to current VQA set. Either the results do not have predictions for all question ids in annotation file or there is atleast one question id that does not belong to the question ids in the annotation file.This could be due to the fact that OKVQA parser requires all questions to evaluatethe accuracy. Ignore this error if this is the sanity check.[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [sanity_check]: {'test/exact_match_at_1': 0.015625, 'test/exact_match_at_2': 0.015625, 'test/exact_match_at_3': 0.015625, 'test/exact_match_at_4': 0.015625, 'test/exact_match_at_5': 0.015625, 'test/recall_at_5': 0.796875, 'test/precision_at_5': 0.515625, 'test/gold_precision_at_5': 0.33125000000000004, 'test/gold_recall_at_5': 0.640625, 'test/successful_hit': 0.00625, 'test/successful_no_hit': 0.0, 'test/failed_hit': 0.575, 'test/failed_no_hit': 0.41875, 'test/selected_successful_hit': 0.015625, 'test/selected_successful_no_hit': 0.0, 'test/selected_failed_hit': 0.359375, 'test/selected_failed_no_hit': 0.625, 'test/n_retrieved_docs': 5, 'test/epoch': 0}[0m
[33;20m[WARNING] - root : Sanity check mode, not saving to loggers.[0m
/home/xl544/.conda/envs/RAVQA/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.2812128418549346, 'test/exact_match_at_2': 0.3937772493063813, 'test/exact_match_at_3': 0.4455013872374158, 'test/exact_match_at_4': 0.46650812524772095, 'test/exact_match_at_5': 0.47344431232659534, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3717003567181927, 'test/gold_recall_at_5': 0.6644867221561633, 'test/successful_hit': 0.21985731272294887, 'test/successful_no_hit': 0.031787554498612765, 'test/failed_hit': 0.6101862861672612, 'test/failed_no_hit': 0.13816884661117718, 'test/selected_successful_hit': 0.25485533095521207, 'test/selected_successful_no_hit': 0.026357510899722554, 'test/selected_failed_hit': 0.629013079667063, 'test/selected_failed_no_hit': 0.08977407847800238, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 25.41, 'test/accuracy_QuestionType_one': 22.71, 'test/accuracy_QuestionType_eight': 24.26, 'test/accuracy_QuestionType_other': 25.75, 'test/accuracy_QuestionType_seven': 26.03, 'test/accuracy_QuestionType_four': 32.52, 'test/accuracy_QuestionType_five': 24.17, 'test/accuracy_QuestionType_three': 24.49, 'test/accuracy_QuestionType_nine': 16.19, 'test/accuracy_QuestionType_ten': 30.39, 'test/accuracy_QuestionType_two': 25.47, 'test/accuracy_QuestionType_six': 26.81, 'test/accuracy_AnswerType_other': 25.41, 'test/epoch': 0}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.42489100277447484, 'test/exact_match_at_2': 0.5507332540626239, 'test/exact_match_at_3': 0.6026555687673405, 'test/exact_match_at_4': 0.6206896551724138, 'test/exact_match_at_5': 0.6244550138723741, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3717003567181927, 'test/gold_recall_at_5': 0.6644867221561633, 'test/successful_hit': 0.30122869599682917, 'test/successful_no_hit': 0.08723741577487118, 'test/failed_hit': 0.45326991676575507, 'test/failed_no_hit': 0.1582639714625446, 'test/selected_successful_hit': 0.35414189456995643, 'test/selected_successful_no_hit': 0.07074910820451843, 'test/selected_failed_hit': 0.4659135949266746, 'test/selected_failed_no_hit': 0.10919540229885058, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 38.43, 'test/accuracy_QuestionType_one': 35.52, 'test/accuracy_QuestionType_eight': 37.19, 'test/accuracy_QuestionType_other': 40.36, 'test/accuracy_QuestionType_seven': 38.79, 'test/accuracy_QuestionType_four': 41.87, 'test/accuracy_QuestionType_five': 37.29, 'test/accuracy_QuestionType_three': 37.1, 'test/accuracy_QuestionType_nine': 26.67, 'test/accuracy_QuestionType_ten': 42.33, 'test/accuracy_QuestionType_two': 49.3, 'test/accuracy_QuestionType_six': 40.14, 'test/accuracy_AnswerType_other': 38.43, 'test/epoch': 0}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.4708680142687277, 'test/exact_match_at_2': 0.593935790725327, 'test/exact_match_at_3': 0.643479984145858, 'test/exact_match_at_4': 0.6573523583036068, 'test/exact_match_at_5': 0.6621086008719778, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3717003567181927, 'test/gold_recall_at_5': 0.6644867221561633, 'test/successful_hit': 0.32318668252080857, 'test/successful_no_hit': 0.10606420927467301, 'test/failed_hit': 0.403170828378914, 'test/failed_no_hit': 0.16757827982560444, 'test/selected_successful_hit': 0.3856520015854142, 'test/selected_successful_no_hit': 0.08521601268331351, 'test/selected_failed_hit': 0.41518034086405076, 'test/selected_failed_no_hit': 0.11395164486722156, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 42.7, 'test/accuracy_QuestionType_one': 39.7, 'test/accuracy_QuestionType_eight': 41.19, 'test/accuracy_QuestionType_other': 45.43, 'test/accuracy_QuestionType_seven': 42.29, 'test/accuracy_QuestionType_four': 45.47, 'test/accuracy_QuestionType_five': 42.11, 'test/accuracy_QuestionType_three': 41.96, 'test/accuracy_QuestionType_nine': 33.1, 'test/accuracy_QuestionType_ten': 47.29, 'test/accuracy_QuestionType_two': 50.47, 'test/accuracy_QuestionType_six': 44.82, 'test/accuracy_AnswerType_other': 42.7, 'test/epoch': 0}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.4839476813317479, 'test/exact_match_at_2': 0.6042409829567975, 'test/exact_match_at_3': 0.6553705905667856, 'test/exact_match_at_4': 0.6712247324613555, 'test/exact_match_at_5': 0.6745937376139517, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3717003567181927, 'test/gold_recall_at_5': 0.6644867221561633, 'test/successful_hit': 0.3301228695996829, 'test/successful_no_hit': 0.11300039635354736, 'test/failed_hit': 0.3918747522790329, 'test/failed_no_hit': 0.1650019817677368, 'test/selected_successful_hit': 0.3915973047958779, 'test/selected_successful_no_hit': 0.09235037653587, 'test/selected_failed_hit': 0.41418945699564014, 'test/selected_failed_no_hit': 0.10186286167261197, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 44.17, 'test/accuracy_QuestionType_one': 40.2, 'test/accuracy_QuestionType_eight': 42.16, 'test/accuracy_QuestionType_other': 45.78, 'test/accuracy_QuestionType_seven': 42.2, 'test/accuracy_QuestionType_four': 48.96, 'test/accuracy_QuestionType_five': 45.32, 'test/accuracy_QuestionType_three': 44.86, 'test/accuracy_QuestionType_nine': 34.05, 'test/accuracy_QuestionType_ten': 50.7, 'test/accuracy_QuestionType_two': 50.0, 'test/accuracy_QuestionType_six': 43.26, 'test/accuracy_AnswerType_other': 44.17, 'test/epoch': 0}[0m
/home/xl544/.conda/envs/RAVQA/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5025762980578676, 'test/exact_match_at_2': 0.6276258422512881, 'test/exact_match_at_3': 0.6712247324613555, 'test/exact_match_at_4': 0.6872770511296076, 'test/exact_match_at_5': 0.6918351169242964, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3717003567181927, 'test/gold_recall_at_5': 0.6644867221561633, 'test/successful_hit': 0.3411018628616726, 'test/successful_no_hit': 0.11553705905667856, 'test/failed_hit': 0.3825208085612366, 'test/failed_no_hit': 0.16084026952041222, 'test/selected_successful_hit': 0.40824415378517637, 'test/selected_successful_no_hit': 0.09433214427269124, 'test/selected_failed_hit': 0.39100277447483156, 'test/selected_failed_no_hit': 0.10642092746730084, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 45.83, 'test/accuracy_QuestionType_one': 41.13, 'test/accuracy_QuestionType_eight': 42.72, 'test/accuracy_QuestionType_other': 48.01, 'test/accuracy_QuestionType_seven': 45.61, 'test/accuracy_QuestionType_four': 49.77, 'test/accuracy_QuestionType_five': 47.4, 'test/accuracy_QuestionType_three': 45.75, 'test/accuracy_QuestionType_nine': 40.71, 'test/accuracy_QuestionType_ten': 50.54, 'test/accuracy_QuestionType_two': 53.95, 'test/accuracy_QuestionType_six': 47.09, 'test/accuracy_AnswerType_other': 45.83, 'test/epoch': 0}[0m
Epoch 0, global step 282: 'test/accuracy_overall' reached 45.83000 (best 45.83000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-T5-CLIP-32.21804959/train/saved_model/model_0.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5120887831946096, 'test/exact_match_at_2': 0.6327784383670234, 'test/exact_match_at_3': 0.6753864447086801, 'test/exact_match_at_4': 0.6914387633769322, 'test/exact_match_at_5': 0.6963931827189853, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3717003567181927, 'test/gold_recall_at_5': 0.6644867221561633, 'test/successful_hit': 0.3434799841458581, 'test/successful_no_hit': 0.1246928260007927, 'test/failed_hit': 0.35774871185097107, 'test/failed_no_hit': 0.17407847800237813, 'test/selected_successful_hit': 0.409235037653587, 'test/selected_successful_no_hit': 0.1028537455410226, 'test/selected_failed_hit': 0.37177962742766546, 'test/selected_failed_no_hit': 0.11613158937772493, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 46.81, 'test/accuracy_QuestionType_one': 43.76, 'test/accuracy_QuestionType_eight': 43.81, 'test/accuracy_QuestionType_other': 49.76, 'test/accuracy_QuestionType_seven': 44.44, 'test/accuracy_QuestionType_four': 51.29, 'test/accuracy_QuestionType_five': 48.2, 'test/accuracy_QuestionType_three': 46.68, 'test/accuracy_QuestionType_nine': 37.62, 'test/accuracy_QuestionType_ten': 51.16, 'test/accuracy_QuestionType_two': 51.86, 'test/accuracy_QuestionType_six': 46.67, 'test/accuracy_AnswerType_other': 46.81, 'test/epoch': 1}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5204122076892588, 'test/exact_match_at_2': 0.640110978993262, 'test/exact_match_at_3': 0.6779627427665478, 'test/exact_match_at_4': 0.6918351169242964, 'test/exact_match_at_5': 0.6938168846611177, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3717003567181927, 'test/gold_recall_at_5': 0.6644867221561633, 'test/successful_hit': 0.34387633769322234, 'test/successful_no_hit': 0.13012286959968292, 'test/failed_hit': 0.3461355529131986, 'test/failed_no_hit': 0.17986523979389615, 'test/selected_successful_hit': 0.415576694411415, 'test/selected_successful_no_hit': 0.10483551327784384, 'test/selected_failed_hit': 0.36642885453824814, 'test/selected_failed_no_hit': 0.11315893777249307, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 47.44, 'test/accuracy_QuestionType_one': 44.53, 'test/accuracy_QuestionType_eight': 44.4, 'test/accuracy_QuestionType_other': 50.05, 'test/accuracy_QuestionType_seven': 46.5, 'test/accuracy_QuestionType_four': 51.25, 'test/accuracy_QuestionType_five': 48.75, 'test/accuracy_QuestionType_three': 47.76, 'test/accuracy_QuestionType_nine': 36.67, 'test/accuracy_QuestionType_ten': 51.94, 'test/accuracy_QuestionType_two': 53.37, 'test/accuracy_QuestionType_six': 45.67, 'test/accuracy_AnswerType_other': 47.44, 'test/epoch': 1}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5227903289734444, 'test/exact_match_at_2': 0.6492271105826397, 'test/exact_match_at_3': 0.6872770511296076, 'test/exact_match_at_4': 0.7013476020610384, 'test/exact_match_at_5': 0.703923900118906, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3717003567181927, 'test/gold_recall_at_5': 0.6644867221561633, 'test/successful_hit': 0.34910820451843044, 'test/successful_no_hit': 0.13396749900911614, 'test/failed_hit': 0.34419342053111374, 'test/failed_no_hit': 0.17273087594133968, 'test/selected_successful_hit': 0.4149821640903686, 'test/selected_successful_no_hit': 0.10780816488307571, 'test/selected_failed_hit': 0.3634562029330162, 'test/selected_failed_no_hit': 0.11375346809353944, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 47.85, 'test/accuracy_QuestionType_one': 44.94, 'test/accuracy_QuestionType_eight': 45.86, 'test/accuracy_QuestionType_other': 50.34, 'test/accuracy_QuestionType_seven': 47.52, 'test/accuracy_QuestionType_four': 50.93, 'test/accuracy_QuestionType_five': 48.97, 'test/accuracy_QuestionType_three': 47.8, 'test/accuracy_QuestionType_nine': 39.05, 'test/accuracy_QuestionType_ten': 50.85, 'test/accuracy_QuestionType_two': 51.4, 'test/accuracy_QuestionType_six': 46.24, 'test/accuracy_AnswerType_other': 47.85, 'test/epoch': 1}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5303210463733651, 'test/exact_match_at_2': 0.6555687673404677, 'test/exact_match_at_3': 0.6975822433610781, 'test/exact_match_at_4': 0.7100673801030519, 'test/exact_match_at_5': 0.7128418549346016, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3717003567181927, 'test/gold_recall_at_5': 0.6644867221561633, 'test/successful_hit': 0.3501783590963139, 'test/successful_no_hit': 0.14149821640903687, 'test/failed_hit': 0.32956797463337295, 'test/failed_no_hit': 0.17875544986127626, 'test/selected_successful_hit': 0.4181529924692826, 'test/selected_successful_no_hit': 0.11216805390408244, 'test/selected_failed_hit': 0.34898929845422116, 'test/selected_failed_no_hit': 0.1206896551724138, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 48.59, 'test/accuracy_QuestionType_one': 44.53, 'test/accuracy_QuestionType_eight': 46.88, 'test/accuracy_QuestionType_other': 49.82, 'test/accuracy_QuestionType_seven': 46.36, 'test/accuracy_QuestionType_four': 53.26, 'test/accuracy_QuestionType_five': 50.61, 'test/accuracy_QuestionType_three': 48.46, 'test/accuracy_QuestionType_nine': 39.52, 'test/accuracy_QuestionType_ten': 51.63, 'test/accuracy_QuestionType_two': 56.51, 'test/accuracy_QuestionType_six': 46.67, 'test/accuracy_AnswerType_other': 48.59, 'test/epoch': 1}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5287356321839081, 'test/exact_match_at_2': 0.6595323028141102, 'test/exact_match_at_3': 0.6979785969084423, 'test/exact_match_at_4': 0.7128418549346016, 'test/exact_match_at_5': 0.716607213634562, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3717003567181927, 'test/gold_recall_at_5': 0.6644867221561633, 'test/successful_hit': 0.35089179548156957, 'test/successful_no_hit': 0.13868410622275068, 'test/failed_hit': 0.3456995640110979, 'test/failed_no_hit': 0.16472453428458184, 'test/selected_successful_hit': 0.42092746730083236, 'test/selected_successful_no_hit': 0.10780816488307571, 'test/selected_failed_hit': 0.36821244550138726, 'test/selected_failed_no_hit': 0.10305192231470471, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 48.51, 'test/accuracy_QuestionType_one': 45.31, 'test/accuracy_QuestionType_eight': 45.67, 'test/accuracy_QuestionType_other': 49.98, 'test/accuracy_QuestionType_seven': 47.15, 'test/accuracy_QuestionType_four': 53.9, 'test/accuracy_QuestionType_five': 50.29, 'test/accuracy_QuestionType_three': 47.52, 'test/accuracy_QuestionType_nine': 40.71, 'test/accuracy_QuestionType_ten': 49.92, 'test/accuracy_QuestionType_two': 54.65, 'test/accuracy_QuestionType_six': 48.79, 'test/accuracy_AnswerType_other': 48.51, 'test/epoch': 1}[0m
Epoch 1, global step 564: 'test/accuracy_overall' reached 48.51000 (best 48.51000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-T5-CLIP-32.21804959/train/saved_model/model_1.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5336900515259612, 'test/exact_match_at_2': 0.6607213634562029, 'test/exact_match_at_3': 0.6985731272294887, 'test/exact_match_at_4': 0.7110582639714625, 'test/exact_match_at_5': 0.7124455013872374, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3717003567181927, 'test/gold_recall_at_5': 0.6644867221561633, 'test/successful_hit': 0.34863258026159333, 'test/successful_no_hit': 0.14962346413000396, 'test/failed_hit': 0.3200158541418946, 'test/failed_no_hit': 0.1817281014665081, 'test/selected_successful_hit': 0.41339674990091163, 'test/selected_successful_no_hit': 0.12029330162504955, 'test/selected_failed_hit': 0.34958382877526756, 'test/selected_failed_no_hit': 0.11672611969877131, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 48.94, 'test/accuracy_QuestionType_one': 45.29, 'test/accuracy_QuestionType_eight': 47.19, 'test/accuracy_QuestionType_other': 50.95, 'test/accuracy_QuestionType_seven': 47.34, 'test/accuracy_QuestionType_four': 53.47, 'test/accuracy_QuestionType_five': 51.08, 'test/accuracy_QuestionType_three': 47.29, 'test/accuracy_QuestionType_nine': 37.86, 'test/accuracy_QuestionType_ten': 50.7, 'test/accuracy_QuestionType_two': 55.81, 'test/accuracy_QuestionType_six': 47.8, 'test/accuracy_AnswerType_other': 48.94, 'test/epoch': 2}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5356718192627824, 'test/exact_match_at_2': 0.6630994847403884, 'test/exact_match_at_3': 0.7025366627031312, 'test/exact_match_at_4': 0.7152199762187872, 'test/exact_match_at_5': 0.718192627824019, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3717003567181927, 'test/gold_recall_at_5': 0.6644867221561633, 'test/successful_hit': 0.3536662703131193, 'test/successful_no_hit': 0.14867221561632976, 'test/failed_hit': 0.316607213634562, 'test/failed_no_hit': 0.1810543004359889, 'test/selected_successful_hit': 0.41973840665873957, 'test/selected_successful_no_hit': 0.1159334126040428, 'test/selected_failed_hit': 0.3493856520015854, 'test/selected_failed_no_hit': 0.11494252873563218, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 49.15, 'test/accuracy_QuestionType_one': 45.81, 'test/accuracy_QuestionType_eight': 46.95, 'test/accuracy_QuestionType_other': 50.5, 'test/accuracy_QuestionType_seven': 47.15, 'test/accuracy_QuestionType_four': 53.47, 'test/accuracy_QuestionType_five': 51.88, 'test/accuracy_QuestionType_three': 48.46, 'test/accuracy_QuestionType_nine': 38.33, 'test/accuracy_QuestionType_ten': 52.56, 'test/accuracy_QuestionType_two': 54.19, 'test/accuracy_QuestionType_six': 48.37, 'test/accuracy_AnswerType_other': 49.15, 'test/epoch': 2}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5416171224732461, 'test/exact_match_at_2': 0.6674593737613952, 'test/exact_match_at_3': 0.7088783194609591, 'test/exact_match_at_4': 0.7215616329766151, 'test/exact_match_at_5': 0.7241379310344828, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3717003567181927, 'test/gold_recall_at_5': 0.6644867221561633, 'test/successful_hit': 0.3549346016646849, 'test/successful_no_hit': 0.1456599286563615, 'test/failed_hit': 0.3223147047166072, 'test/failed_no_hit': 0.1770907649623464, 'test/selected_successful_hit': 0.42409829567974633, 'test/selected_successful_no_hit': 0.1175188267934998, 'test/selected_failed_hit': 0.3442330558858502, 'test/selected_failed_no_hit': 0.11414982164090369, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 49.71, 'test/accuracy_QuestionType_one': 46.52, 'test/accuracy_QuestionType_eight': 46.93, 'test/accuracy_QuestionType_other': 52.44, 'test/accuracy_QuestionType_seven': 46.96, 'test/accuracy_QuestionType_four': 55.31, 'test/accuracy_QuestionType_five': 50.31, 'test/accuracy_QuestionType_three': 49.49, 'test/accuracy_QuestionType_nine': 40.48, 'test/accuracy_QuestionType_ten': 52.56, 'test/accuracy_QuestionType_two': 57.44, 'test/accuracy_QuestionType_six': 49.79, 'test/accuracy_AnswerType_other': 49.71, 'test/epoch': 2}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5471660721363456, 'test/exact_match_at_2': 0.6730083234244947, 'test/exact_match_at_3': 0.711850971066191, 'test/exact_match_at_4': 0.7225525168450257, 'test/exact_match_at_5': 0.7247324613555292, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3717003567181927, 'test/gold_recall_at_5': 0.6644867221561633, 'test/successful_hit': 0.35612366230677767, 'test/successful_no_hit': 0.15311137534680935, 'test/failed_hit': 0.31565596512088784, 'test/failed_no_hit': 0.17510899722552517, 'test/selected_successful_hit': 0.4246928260007927, 'test/selected_successful_no_hit': 0.12247324613555291, 'test/selected_failed_hit': 0.34185493460166466, 'test/selected_failed_no_hit': 0.11097899326198969, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 50.12, 'test/accuracy_QuestionType_one': 46.47, 'test/accuracy_QuestionType_eight': 47.53, 'test/accuracy_QuestionType_other': 52.86, 'test/accuracy_QuestionType_seven': 48.79, 'test/accuracy_QuestionType_four': 55.98, 'test/accuracy_QuestionType_five': 51.38, 'test/accuracy_QuestionType_three': 47.38, 'test/accuracy_QuestionType_nine': 42.38, 'test/accuracy_QuestionType_ten': 53.49, 'test/accuracy_QuestionType_two': 55.7, 'test/accuracy_QuestionType_six': 51.21, 'test/accuracy_AnswerType_other': 50.12, 'test/epoch': 2}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.543004359889021, 'test/exact_match_at_2': 0.6708283789139913, 'test/exact_match_at_3': 0.7124455013872374, 'test/exact_match_at_4': 0.7235434007134364, 'test/exact_match_at_5': 0.7259215219976218, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3717003567181927, 'test/gold_recall_at_5': 0.6644867221561633, 'test/successful_hit': 0.35465715418152993, 'test/successful_no_hit': 0.15326991676575505, 'test/failed_hit': 0.31898533491874753, 'test/failed_no_hit': 0.1730875941339675, 'test/selected_successful_hit': 0.4233055885850178, 'test/selected_successful_no_hit': 0.11969877130400317, 'test/selected_failed_hit': 0.3434403487911217, 'test/selected_failed_no_hit': 0.11355529131985731, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 49.75, 'test/accuracy_QuestionType_one': 47.18, 'test/accuracy_QuestionType_eight': 47.81, 'test/accuracy_QuestionType_other': 51.6, 'test/accuracy_QuestionType_seven': 49.86, 'test/accuracy_QuestionType_four': 53.93, 'test/accuracy_QuestionType_five': 50.41, 'test/accuracy_QuestionType_three': 48.32, 'test/accuracy_QuestionType_nine': 41.9, 'test/accuracy_QuestionType_ten': 49.15, 'test/accuracy_QuestionType_two': 54.77, 'test/accuracy_QuestionType_six': 50.78, 'test/accuracy_AnswerType_other': 49.75, 'test/epoch': 2}[0m
Epoch 2, global step 846: 'test/accuracy_overall' reached 49.75000 (best 49.75000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-T5-CLIP-32.21804959/train/saved_model/model_2.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5521204914783987, 'test/exact_match_at_2': 0.6773682124455014, 'test/exact_match_at_3': 0.7108600871977804, 'test/exact_match_at_4': 0.7195798652397939, 'test/exact_match_at_5': 0.7203725723345223, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3717003567181927, 'test/gold_recall_at_5': 0.6644867221561633, 'test/successful_hit': 0.3514466904478795, 'test/successful_no_hit': 0.16904478795085215, 'test/failed_hit': 0.2797859690844233, 'test/failed_no_hit': 0.19972255251684504, 'test/selected_successful_hit': 0.4175584621482362, 'test/selected_successful_no_hit': 0.1345620293301625, 'test/selected_failed_hit': 0.31391200951248516, 'test/selected_failed_no_hit': 0.13396749900911614, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 50.71, 'test/accuracy_QuestionType_one': 46.94, 'test/accuracy_QuestionType_eight': 47.74, 'test/accuracy_QuestionType_other': 53.12, 'test/accuracy_QuestionType_seven': 50.93, 'test/accuracy_QuestionType_four': 56.19, 'test/accuracy_QuestionType_five': 52.7, 'test/accuracy_QuestionType_three': 49.44, 'test/accuracy_QuestionType_nine': 38.33, 'test/accuracy_QuestionType_ten': 52.87, 'test/accuracy_QuestionType_two': 53.37, 'test/accuracy_QuestionType_six': 51.91, 'test/accuracy_AnswerType_other': 50.71, 'test/epoch': 3}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.549346016646849, 'test/exact_match_at_2': 0.6739992072929053, 'test/exact_match_at_3': 0.7096710265556877, 'test/exact_match_at_4': 0.7215616329766151, 'test/exact_match_at_5': 0.7235434007134364, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3717003567181927, 'test/gold_recall_at_5': 0.6644867221561633, 'test/successful_hit': 0.3565200158541419, 'test/successful_no_hit': 0.16107808164883075, 'test/failed_hit': 0.29496630994847406, 'test/failed_no_hit': 0.1874355925485533, 'test/selected_successful_hit': 0.42350376535869994, 'test/selected_successful_no_hit': 0.12584225128814902, 'test/selected_failed_hit': 0.3208481965913595, 'test/selected_failed_no_hit': 0.12980578676179153, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 50.46, 'test/accuracy_QuestionType_one': 46.62, 'test/accuracy_QuestionType_eight': 48.84, 'test/accuracy_QuestionType_other': 53.31, 'test/accuracy_QuestionType_seven': 50.28, 'test/accuracy_QuestionType_four': 55.24, 'test/accuracy_QuestionType_five': 51.5, 'test/accuracy_QuestionType_three': 48.22, 'test/accuracy_QuestionType_nine': 37.62, 'test/accuracy_QuestionType_ten': 52.87, 'test/accuracy_QuestionType_two': 53.95, 'test/accuracy_QuestionType_six': 53.48, 'test/accuracy_AnswerType_other': 50.46, 'test/epoch': 3}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5451843043995244, 'test/exact_match_at_2': 0.6785572730875942, 'test/exact_match_at_3': 0.7146254458977408, 'test/exact_match_at_4': 0.7257233452239398, 'test/exact_match_at_5': 0.7290923503765359, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3717003567181927, 'test/gold_recall_at_5': 0.6644867221561633, 'test/successful_hit': 0.35838287752675385, 'test/successful_no_hit': 0.15965120887831946, 'test/failed_hit': 0.3009512485136742, 'test/failed_no_hit': 0.18101466508125247, 'test/selected_successful_hit': 0.41954022988505746, 'test/selected_successful_no_hit': 0.1256440745144669, 'test/selected_failed_hit': 0.33729686880697585, 'test/selected_failed_no_hit': 0.1175188267934998, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 50.19, 'test/accuracy_QuestionType_one': 47.11, 'test/accuracy_QuestionType_eight': 47.58, 'test/accuracy_QuestionType_other': 53.09, 'test/accuracy_QuestionType_seven': 48.46, 'test/accuracy_QuestionType_four': 55.27, 'test/accuracy_QuestionType_five': 50.91, 'test/accuracy_QuestionType_three': 49.39, 'test/accuracy_QuestionType_nine': 38.57, 'test/accuracy_QuestionType_ten': 50.85, 'test/accuracy_QuestionType_two': 57.56, 'test/accuracy_QuestionType_six': 51.49, 'test/accuracy_AnswerType_other': 50.19, 'test/epoch': 3}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5535077288941737, 'test/exact_match_at_2': 0.6755846214823623, 'test/exact_match_at_3': 0.711850971066191, 'test/exact_match_at_4': 0.7243361078081649, 'test/exact_match_at_5': 0.7259215219976218, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3717003567181927, 'test/gold_recall_at_5': 0.6644867221561633, 'test/successful_hit': 0.35834324217201746, 'test/successful_no_hit': 0.15842251288149028, 'test/failed_hit': 0.30103051922314705, 'test/failed_no_hit': 0.18220372572334523, 'test/selected_successful_hit': 0.4225128814902893, 'test/selected_successful_no_hit': 0.13099484740388426, 'test/selected_failed_hit': 0.32996432818073723, 'test/selected_failed_no_hit': 0.11652794292508918, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 50.84, 'test/accuracy_QuestionType_one': 47.04, 'test/accuracy_QuestionType_eight': 48.21, 'test/accuracy_QuestionType_other': 53.05, 'test/accuracy_QuestionType_seven': 51.21, 'test/accuracy_QuestionType_four': 56.08, 'test/accuracy_QuestionType_five': 53.29, 'test/accuracy_QuestionType_three': 48.79, 'test/accuracy_QuestionType_nine': 39.29, 'test/accuracy_QuestionType_ten': 51.94, 'test/accuracy_QuestionType_two': 53.26, 'test/accuracy_QuestionType_six': 52.06, 'test/accuracy_AnswerType_other': 50.84, 'test/epoch': 3}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.547760602457392, 'test/exact_match_at_2': 0.6757827982560444, 'test/exact_match_at_3': 0.7110582639714625, 'test/exact_match_at_4': 0.7255251684502576, 'test/exact_match_at_5': 0.7275069361870788, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3717003567181927, 'test/gold_recall_at_5': 0.6644867221561633, 'test/successful_hit': 0.3562029330162505, 'test/successful_no_hit': 0.15675782798256044, 'test/failed_hit': 0.30154577883472056, 'test/failed_no_hit': 0.18549346016646848, 'test/selected_successful_hit': 0.4221165279429251, 'test/selected_successful_no_hit': 0.1256440745144669, 'test/selected_failed_hit': 0.3337296868806976, 'test/selected_failed_no_hit': 0.11850971066191042, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 50.3, 'test/accuracy_QuestionType_one': 47.75, 'test/accuracy_QuestionType_eight': 48.12, 'test/accuracy_QuestionType_other': 51.83, 'test/accuracy_QuestionType_seven': 49.07, 'test/accuracy_QuestionType_four': 55.56, 'test/accuracy_QuestionType_five': 51.5, 'test/accuracy_QuestionType_three': 49.58, 'test/accuracy_QuestionType_nine': 42.86, 'test/accuracy_QuestionType_ten': 50.08, 'test/accuracy_QuestionType_two': 53.02, 'test/accuracy_QuestionType_six': 50.78, 'test/accuracy_AnswerType_other': 50.3, 'test/epoch': 3}[0m
Epoch 3, global step 1128: 'test/accuracy_overall' reached 50.30000 (best 50.30000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-T5-CLIP-32.21804959/train/saved_model/model_3.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5566785572730876, 'test/exact_match_at_2': 0.6837098692033293, 'test/exact_match_at_3': 0.7168053904082442, 'test/exact_match_at_4': 0.7251288149028934, 'test/exact_match_at_5': 0.7267142290923504, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3717003567181927, 'test/gold_recall_at_5': 0.6644867221561633, 'test/successful_hit': 0.3560840269520412, 'test/successful_no_hit': 0.1673801030519223, 'test/failed_hit': 0.2810543004359889, 'test/failed_no_hit': 0.19548156956004756, 'test/selected_successful_hit': 0.41914387633769323, 'test/selected_successful_no_hit': 0.13753468093539437, 'test/selected_failed_hit': 0.3113357114546175, 'test/selected_failed_no_hit': 0.13198573127229488, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 51.04, 'test/accuracy_QuestionType_one': 48.24, 'test/accuracy_QuestionType_eight': 48.49, 'test/accuracy_QuestionType_other': 52.83, 'test/accuracy_QuestionType_seven': 50.37, 'test/accuracy_QuestionType_four': 57.35, 'test/accuracy_QuestionType_five': 52.4, 'test/accuracy_QuestionType_three': 48.69, 'test/accuracy_QuestionType_nine': 41.19, 'test/accuracy_QuestionType_ten': 51.16, 'test/accuracy_QuestionType_two': 54.53, 'test/accuracy_QuestionType_six': 52.34, 'test/accuracy_AnswerType_other': 51.04, 'test/epoch': 4}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5558858501783591, 'test/exact_match_at_2': 0.6825208085612366, 'test/exact_match_at_3': 0.7158145065398336, 'test/exact_match_at_4': 0.7269124058660325, 'test/exact_match_at_5': 0.729290527150218, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3717003567181927, 'test/gold_recall_at_5': 0.6644867221561633, 'test/successful_hit': 0.3579865239793896, 'test/successful_no_hit': 0.16527942925089179, 'test/failed_hit': 0.28339278636543797, 'test/failed_no_hit': 0.19334126040428062, 'test/selected_successful_hit': 0.4233055885850178, 'test/selected_successful_no_hit': 0.13258026159334127, 'test/selected_failed_hit': 0.31232659532302814, 'test/selected_failed_no_hit': 0.13178755449861276, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 50.99, 'test/accuracy_QuestionType_one': 47.18, 'test/accuracy_QuestionType_eight': 48.3, 'test/accuracy_QuestionType_other': 52.41, 'test/accuracy_QuestionType_seven': 50.89, 'test/accuracy_QuestionType_four': 58.59, 'test/accuracy_QuestionType_five': 52.15, 'test/accuracy_QuestionType_three': 49.07, 'test/accuracy_QuestionType_nine': 40.0, 'test/accuracy_QuestionType_ten': 51.63, 'test/accuracy_QuestionType_two': 55.0, 'test/accuracy_QuestionType_six': 53.19, 'test/accuracy_AnswerType_other': 50.99, 'test/epoch': 4}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5578676179151804, 'test/exact_match_at_2': 0.6823226317875545, 'test/exact_match_at_3': 0.7160126833135156, 'test/exact_match_at_4': 0.7302814110186286, 'test/exact_match_at_5': 0.7324613555291319, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3717003567181927, 'test/gold_recall_at_5': 0.6644867221561633, 'test/successful_hit': 0.36087990487514865, 'test/successful_no_hit': 0.16345620293301624, 'test/failed_hit': 0.2923900118906064, 'test/failed_no_hit': 0.1832738803012287, 'test/selected_successful_hit': 0.4262782401902497, 'test/selected_successful_no_hit': 0.13158937772493065, 'test/selected_failed_hit': 0.31985731272294887, 'test/selected_failed_no_hit': 0.12227506936187078, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 51.2, 'test/accuracy_QuestionType_one': 47.4, 'test/accuracy_QuestionType_eight': 48.65, 'test/accuracy_QuestionType_other': 54.41, 'test/accuracy_QuestionType_seven': 49.49, 'test/accuracy_QuestionType_four': 57.18, 'test/accuracy_QuestionType_five': 51.93, 'test/accuracy_QuestionType_three': 49.77, 'test/accuracy_QuestionType_nine': 42.86, 'test/accuracy_QuestionType_ten': 54.42, 'test/accuracy_QuestionType_two': 55.7, 'test/accuracy_QuestionType_six': 52.62, 'test/accuracy_AnswerType_other': 51.2, 'test/epoch': 4}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5578676179151804, 'test/exact_match_at_2': 0.6797463337296868, 'test/exact_match_at_3': 0.7173999207292905, 'test/exact_match_at_4': 0.7281014665081252, 'test/exact_match_at_5': 0.7304795877923107, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3717003567181927, 'test/gold_recall_at_5': 0.6644867221561633, 'test/successful_hit': 0.3559254855330955, 'test/successful_no_hit': 0.17098692033293697, 'test/failed_hit': 0.2748711850971066, 'test/failed_no_hit': 0.19821640903686089, 'test/selected_successful_hit': 0.4205311137534681, 'test/selected_successful_no_hit': 0.13733650416171225, 'test/selected_failed_hit': 0.310543004359889, 'test/selected_failed_no_hit': 0.13158937772493065, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 51.28, 'test/accuracy_QuestionType_one': 46.99, 'test/accuracy_QuestionType_eight': 49.81, 'test/accuracy_QuestionType_other': 54.05, 'test/accuracy_QuestionType_seven': 49.58, 'test/accuracy_QuestionType_four': 57.81, 'test/accuracy_QuestionType_five': 52.07, 'test/accuracy_QuestionType_three': 50.09, 'test/accuracy_QuestionType_nine': 40.48, 'test/accuracy_QuestionType_ten': 50.39, 'test/accuracy_QuestionType_two': 56.51, 'test/accuracy_QuestionType_six': 51.63, 'test/accuracy_AnswerType_other': 51.28, 'test/epoch': 4}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5568767340467697, 'test/exact_match_at_2': 0.6801426872770512, 'test/exact_match_at_3': 0.7164090368608799, 'test/exact_match_at_4': 0.7273087594133968, 'test/exact_match_at_5': 0.7281014665081252, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3717003567181927, 'test/gold_recall_at_5': 0.6644867221561633, 'test/successful_hit': 0.35513277843836705, 'test/successful_no_hit': 0.16749900911613158, 'test/failed_hit': 0.2835513277843837, 'test/failed_no_hit': 0.1938168846611177, 'test/selected_successful_hit': 0.4215219976218787, 'test/selected_successful_no_hit': 0.13535473642489101, 'test/selected_failed_hit': 0.31232659532302814, 'test/selected_failed_no_hit': 0.13079667063020214, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 51.11, 'test/accuracy_QuestionType_one': 47.7, 'test/accuracy_QuestionType_eight': 48.3, 'test/accuracy_QuestionType_other': 53.09, 'test/accuracy_QuestionType_seven': 50.84, 'test/accuracy_QuestionType_four': 56.51, 'test/accuracy_QuestionType_five': 53.39, 'test/accuracy_QuestionType_three': 48.88, 'test/accuracy_QuestionType_nine': 44.29, 'test/accuracy_QuestionType_ten': 50.85, 'test/accuracy_QuestionType_two': 54.42, 'test/accuracy_QuestionType_six': 52.48, 'test/accuracy_AnswerType_other': 51.11, 'test/epoch': 4}[0m
Epoch 4, global step 1410: 'test/accuracy_overall' reached 51.11000 (best 51.11000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-T5-CLIP-32.21804959/train/saved_model/model_4.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5584621482362268, 'test/exact_match_at_2': 0.6848989298454221, 'test/exact_match_at_3': 0.7185889813713833, 'test/exact_match_at_4': 0.7269124058660325, 'test/exact_match_at_5': 0.7281014665081252, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3717003567181927, 'test/gold_recall_at_5': 0.6644867221561633, 'test/successful_hit': 0.356718192627824, 'test/successful_no_hit': 0.1715021799445105, 'test/failed_hit': 0.26777645659928656, 'test/failed_no_hit': 0.2040031708283789, 'test/selected_successful_hit': 0.42092746730083236, 'test/selected_successful_no_hit': 0.13753468093539437, 'test/selected_failed_hit': 0.2968688069758224, 'test/selected_failed_no_hit': 0.14466904478795084, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 51.23, 'test/accuracy_QuestionType_one': 46.89, 'test/accuracy_QuestionType_eight': 49.12, 'test/accuracy_QuestionType_other': 54.31, 'test/accuracy_QuestionType_seven': 51.36, 'test/accuracy_QuestionType_four': 56.83, 'test/accuracy_QuestionType_five': 52.47, 'test/accuracy_QuestionType_three': 48.69, 'test/accuracy_QuestionType_nine': 45.24, 'test/accuracy_QuestionType_ten': 48.99, 'test/accuracy_QuestionType_two': 57.44, 'test/accuracy_QuestionType_six': 51.49, 'test/accuracy_AnswerType_other': 51.23, 'test/epoch': 5}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5580657946888624, 'test/exact_match_at_2': 0.683115338882283, 'test/exact_match_at_3': 0.7160126833135156, 'test/exact_match_at_4': 0.7249306381292112, 'test/exact_match_at_5': 0.726119698771304, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3717003567181927, 'test/gold_recall_at_5': 0.6644867221561633, 'test/successful_hit': 0.35548949663099483, 'test/successful_no_hit': 0.1756242568370987, 'test/failed_hit': 0.26298057867617913, 'test/failed_no_hit': 0.2059056678557273, 'test/selected_successful_hit': 0.41854934601664684, 'test/selected_successful_no_hit': 0.1395164486722156, 'test/selected_failed_hit': 0.29964328180737215, 'test/selected_failed_no_hit': 0.14229092350376535, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 51.18, 'test/accuracy_QuestionType_one': 47.7, 'test/accuracy_QuestionType_eight': 49.26, 'test/accuracy_QuestionType_other': 53.41, 'test/accuracy_QuestionType_seven': 50.37, 'test/accuracy_QuestionType_four': 57.88, 'test/accuracy_QuestionType_five': 51.48, 'test/accuracy_QuestionType_three': 50.05, 'test/accuracy_QuestionType_nine': 40.0, 'test/accuracy_QuestionType_ten': 50.85, 'test/accuracy_QuestionType_two': 55.23, 'test/accuracy_QuestionType_six': 52.34, 'test/accuracy_AnswerType_other': 51.18, 'test/epoch': 5}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.562029330162505, 'test/exact_match_at_2': 0.6841062227506937, 'test/exact_match_at_3': 0.7160126833135156, 'test/exact_match_at_4': 0.7237415774871185, 'test/exact_match_at_5': 0.7255251684502576, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3717003567181927, 'test/gold_recall_at_5': 0.6644867221561633, 'test/successful_hit': 0.35453824811732065, 'test/successful_no_hit': 0.17871581450653984, 'test/failed_hit': 0.2544193420531114, 'test/failed_no_hit': 0.21232659532302814, 'test/selected_successful_hit': 0.4183511692429647, 'test/selected_successful_no_hit': 0.14367816091954022, 'test/selected_failed_hit': 0.291319857312723, 'test/selected_failed_no_hit': 0.1466508125247721, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 51.53, 'test/accuracy_QuestionType_one': 47.68, 'test/accuracy_QuestionType_eight': 49.16, 'test/accuracy_QuestionType_other': 54.57, 'test/accuracy_QuestionType_seven': 49.81, 'test/accuracy_QuestionType_four': 58.98, 'test/accuracy_QuestionType_five': 52.07, 'test/accuracy_QuestionType_three': 49.44, 'test/accuracy_QuestionType_nine': 44.76, 'test/accuracy_QuestionType_ten': 51.78, 'test/accuracy_QuestionType_two': 55.35, 'test/accuracy_QuestionType_six': 52.34, 'test/accuracy_AnswerType_other': 51.53, 'test/epoch': 5}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5560840269520412, 'test/exact_match_at_2': 0.6813317479191439, 'test/exact_match_at_3': 0.7160126833135156, 'test/exact_match_at_4': 0.7259215219976218, 'test/exact_match_at_5': 0.727705112960761, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3717003567181927, 'test/gold_recall_at_5': 0.6644867221561633, 'test/successful_hit': 0.3536662703131193, 'test/successful_no_hit': 0.1724534284581847, 'test/failed_hit': 0.27078874355925486, 'test/failed_no_hit': 0.20309155766944115, 'test/selected_successful_hit': 0.417162108600872, 'test/selected_successful_no_hit': 0.13892191835116924, 'test/selected_failed_hit': 0.30737217598097505, 'test/selected_failed_no_hit': 0.13654379706698375, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 50.94, 'test/accuracy_QuestionType_one': 46.86, 'test/accuracy_QuestionType_eight': 48.6, 'test/accuracy_QuestionType_other': 52.96, 'test/accuracy_QuestionType_seven': 49.86, 'test/accuracy_QuestionType_four': 58.55, 'test/accuracy_QuestionType_five': 51.93, 'test/accuracy_QuestionType_three': 49.44, 'test/accuracy_QuestionType_nine': 44.05, 'test/accuracy_QuestionType_ten': 50.08, 'test/accuracy_QuestionType_two': 55.35, 'test/accuracy_QuestionType_six': 51.06, 'test/accuracy_AnswerType_other': 50.94, 'test/epoch': 5}[0m
