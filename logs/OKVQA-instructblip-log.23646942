Namespace(DATA_FOLDER='', EXPERIMENT_FOLDER='', accelerator='auto', accumulate_grad_batches=None, amp_backend='native', amp_level=None, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, benchmark=None, check_val_every_n_epoch=1, checkpoint_callback=None, config='../configs/okvqa/RAVQA_instructblip.jsonnet', default_root_dir=None, detect_anomaly=False, deterministic=None, devices='1', enable_checkpointing=True, enable_model_summary=True, enable_progress_bar=True, experiment_name='OKVQA-instructblip.23646942', fast_dev_run=False, flush_logs_every_n_steps=None, gpus=None, gradient_clip_algorithm=None, gradient_clip_val=None, ipus=None, limit_predict_batches=None, limit_test_batches=None, limit_train_batches=None, limit_val_batches=None, log_every_n_steps=50, log_gpu_memory=None, log_prediction_tables=False, logger=True, max_epochs=None, max_steps=-1, max_time=None, min_epochs=None, min_steps=None, mode='train', modules=['force_existence'], move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=None, num_sanity_val_steps=2, opts=['train.epochs=8', 'train.batch_size=1', 'valid.step_size=0.5', 'valid.batch_size=4', 'train.additional.gradient_accumulation_steps=32', 'train.lr=0.00006', 'train.retriever_lr=0.00001', 'train.scheduler=linear', 'model_config.loss_ratio.additional_loss=1', 'model_config.RAVQA_loss_type=Approach6', 'data_loader.additional.num_knowledge_passages=5'], overfit_batches=0.0, plugins=None, precision='bf16', prepare_data_per_node=None, process_position=0, profiler=None, progress_bar_refresh_rate=None, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, reset=False, resume_from_checkpoint=None, stochastic_weight_avg=False, strategy=None, sync_batchnorm=False, tags=[], terminate_on_nan=None, test_batch_size=-1, test_evaluation_name='', tpu_cores=None, track_grad_norm=-1, val_check_interval=None, weights_save_path=None, weights_summary='top')
input value {} is not a number, parse to string.
input value {} is not a number, parse to string.
{'DATA_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data', 'EXPERIMENT_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments', 'TENSORBOARD_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data_TB/tb_logs', 'WANDB': {'CACHE_DIR': '', 'entity': 'xl544', 'project': 'RAVQA', 'tags': ['OKVQA']}, 'cache': {'default_folder': '../data/ok-vqa/cache', 'regenerate': {'clip_embeddings': 0, 'ocr_feature_preprocessed': 0, 'qformer_embeddings': 0, 'test_data_preprocessed': 0, 'train_data_preprocessed': 0, 'vinvl_feature_preprocessed': 0}}, 'cuda': 0, 'data_loader': {'additional': {'max_decoder_source_length': 512, 'max_source_length': 512, 'max_target_length': 10, 'num_knowledge_passages': 5}, 'dataset_modules': {'module_dict': {'LoadClipEmbeddings': {'config': {'clip_embeddings': {'train': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_train2014.pkl', 'val': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_val2014.pkl'}, 'instructBLIP_embeddings': {'train': '../data/ok-vqa/pre-extracted_features/blip2_head_embeddings/coco_instructblip_qformer_train2014.pkl', 'val': '../data/ok-vqa/pre-extracted_features/blip2_head_embeddings/coco_instructblip_qformer_val2014.pkl'}, 'qformer_embeddings': {'train': '../data/ok-vqa/pre-extracted_features/blip2_head_embeddings/coco_hgface_qformer_train2014.pkl', 'val': '../data/ok-vqa/pre-extracted_features/blip2_head_embeddings/coco_hgface_qformer_val2014.pkl'}}, 'option': 'default', 'type': 'EmbeddingInput'}, 'LoadGoogleOCRFeatures': {'config': {'combine_with_vinvl': True, 'test': '../data/ok-vqa/pre-extracted_features/OCR/valid', 'train': '../data/ok-vqa/pre-extracted_features/OCR/train'}, 'option': 'default', 'type': 'LoadGoogleOCRFeatures'}, 'LoadGoogleSearchAnnotations': {'config': {'annotations_path': {'test': '../data/ok-vqa/pre-extracted_features/passages/retriever_test.json', 'train': '../data/ok-vqa/pre-extracted_features/passages/retriever_train.json', 'valid': '../data/ok-vqa/pre-extracted_features/passages/retriever_testdev.json'}}, 'option': 'default', 'type': 'LoadGoogleSearchAnnotations'}, 'LoadGoogleSearchPassageData': {'config': {'passage_data_path': {'full': '../data/ok-vqa/pre-extracted_features/passages/okvqa_full_corpus.csv', 'train': '../data/ok-vqa/pre-extracted_features/passages/okvqa_train_corpus.csv'}, 'use_full_split': True}, 'option': 'default', 'type': 'LoadGoogleSearchPassageData'}, 'LoadOKVQAData': {'config': {'image_data_path': {'test': '../data/ok-vqa/val2014', 'train': '../data/ok-vqa/train2014'}, 'vqa_data_path': {'annotation_files': {'test': '../data/ok-vqa/mscoco_val2014_annotations.json', 'train': '../data/ok-vqa/mscoco_train2014_annotations.json'}, 'question_files': {'test': '../data/ok-vqa/OpenEnded_mscoco_val2014_questions.json', 'train': '../data/ok-vqa/OpenEnded_mscoco_train2014_questions.json'}}}, 'option': 'default', 'type': 'LoadOKVQAData'}, 'LoadOscarCaptionFeatures': {'config': {'test': '../data/ok-vqa/pre-extracted_features/captions/test_predictions.json', 'train': '../data/ok-vqa/pre-extracted_features/captions/train_predictions.json', 'valid': '../data/ok-vqa/pre-extracted_features/captions/valid_predictions.json'}, 'option': 'default', 'type': 'LoadOscarCaptionFeatures'}, 'LoadPretrainedDPROutputForGoogleSearchPassage': {'config': {'pretrained_dpr_outputs': {'test': '../Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/test/test_evaluation/test_predictions.json', 'train': '../Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/test/test_evaluation/train_predictions.json'}}, 'option': 'none', 'type': 'LoadPretrainedDPROutputForGoogleSearchPassage'}, 'LoadVinVLFeatures': {'config': {'test': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_testset_full/inference/vinvl_vg_x152c4/predictions.tsv', 'train': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_trainset_full/inference/vinvl_vg_x152c4/predictions.tsv'}, 'option': 'default', 'type': 'LoadVinVLFeatures'}}, 'module_list': ['LoadVinVLFeatures', 'LoadGoogleOCRFeatures', 'LoadOscarCaptionFeatures', 'LoadOKVQAData', 'LoadGoogleSearchPassageData']}, 'dataset_type': 'OKVQADatasetBLIP2Text', 'dummy_dataloader': 0, 'index_files': {'index_passages_path': '../data/ok-vqa/pre-extracted_features/faiss/ok-vqa-passages-full-caption-pretrained-NewRun/my_knowledge_dataset', 'index_path': '../data/ok-vqa/pre-extracted_features/faiss/ok-vqa-passages-full-caption-pretrained-NewRun/my_knowledge_dataset_hnsw_index.faiss'}, 'type': 'DataLoaderOKVQAWithKnowledge'}, 'experiment_name': 'OKVQA-instructblip.23646942', 'gpu_device': 0, 'ignore_pretrained_weights': [], 'metrics': [{'name': 'compute_exact_match'}, {'name': 'compute_retrieval_metrics'}, {'name': 'compute_okvqa_scores'}], 'model_config': {'DECODER_SPECIAL_TOKENS': {'additional_special_tokens': [], 'bos_token': '<PAD>', 'pad_token': '<PAD>'}, 'DecoderTokenizerClass': 'InstructBlipProcessor', 'DecoderTokenizerModelVersion': 'Salesforce/instructblip-flan-t5-xl', 'GeneratorModelClass': 'InstructBlipForConditionalGeneration', 'GeneratorModelVersion': 'Salesforce/instructblip-flan-t5-xl', 'LoadPretrainMLP': 0, 'ModelClass': 'RagModelInstructBLIP', 'QueryEncoderConfigClass': 'DPRConfig', 'QueryEncoderModelClass': 'DPRQuestionEncoder', 'QueryEncoderModelVersion': '/home/xl544/rds/rds-cvnlp-hirYTW1FQIw/wl356/projects/Retrieval-Augmented-Visual-Question-Answering/Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/train/saved_model/epoch6/query_encoder', 'RAVQA_loss_type': 'Approach6', 'SPECIAL_TOKENS': {'additional_special_tokens': []}, 'TokenizerClass': 'DPRQuestionEncoderTokenizer', 'TokenizerModelVersion': 'facebook/dpr-question_encoder-single-nq-base', 'UseInstructBLIPEmb': 0, 'UsePrefixEmb': 0, 'UseQformerEmb': 0, 'base_model': 'RAG', 'decoder_input_modules': {'module_list': [], 'postprocess_module_list': []}, 'input_modules': {'module_list': [{'option': 'default', 'separation_tokens': {'end': '', 'start': 'Question:'}, 'type': 'QuestionInput'}], 'postprocess_module_list': [{'option': 'default', 'type': 'PostProcessInputTokenization'}]}, 'loss_ratio': {'additional_loss': 1, 'nll_loss': 1, 'rag_loss': 0}, 'modules': ['force_existence'], 'output_modules': {'module_list': [{'option': 'default', 'type': 'GenerationOutput'}], 'postprocess_module_list': [{'option': 'default', 'type': 'PostProcessOutputTokenizationInstructBLIP'}]}, 'pretrained': 1, 'rag_modules': {'module_list': []}}, 'platform_type': 'pytorch', 'seed': 2021, 'test': {'additional': {'multiprocessing': 4}, 'batch_size': 32, 'evaluation_name': 'test_evaluation', 'load_best_model': 0, 'load_epoch': -1, 'load_model_path': '', 'num_evaluation': 0}, 'train': {'MLP_lr': 0.0001, 'adam_epsilon': 1e-08, 'additional': {'gradient_accumulation_steps': 32, 'gradient_clipping': 0, 'plugins': [], 'save_top_k': 1, 'save_top_k_metric': 'test/accuracy_overall', 'save_top_k_mode': 'max', 'warmup_steps': 0}, 'batch_size': 1, 'epochs': 8, 'load_best_model': 0, 'load_epoch': -1, 'load_model_path': '', 'lr': 6e-05, 'retriever_lr': 1e-05, 'save_interval': 1, 'scheduler': 'linear', 'type': 'RagExecutorInstructBLIP'}, 'valid': {'additional': {}, 'batch_size': 4, 'break_interval': 3000, 'step_size': 0.5}, 'reset': False, 'mode': 'train', 'log_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA-instructblip.23646942/train', 'experiment_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA-instructblip.23646942', 'saved_model_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA-instructblip.23646942/train/saved_model', 'imgs_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA-instructblip.23646942/train/imgs', 'tensorboard_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data_TB/tb_logs/OKVQA-instructblip.23646942', 'args': {'config': '../configs/okvqa/RAVQA_instructblip.jsonnet', 'DATA_FOLDER': '', 'EXPERIMENT_FOLDER': '', 'mode': 'train', 'reset': False, 'experiment_name': 'OKVQA-instructblip.23646942', 'tags': [], 'modules': ['force_existence'], 'log_prediction_tables': False, 'test_batch_size': -1, 'test_evaluation_name': '', 'logger': True, 'checkpoint_callback': None, 'enable_checkpointing': True, 'default_root_dir': None, 'gradient_clip_val': None, 'gradient_clip_algorithm': None, 'process_position': 0, 'num_nodes': 1, 'num_processes': None, 'devices': '1', 'gpus': None, 'auto_select_gpus': False, 'tpu_cores': None, 'ipus': None, 'log_gpu_memory': None, 'progress_bar_refresh_rate': None, 'enable_progress_bar': True, 'overfit_batches': 0.0, 'track_grad_norm': -1, 'check_val_every_n_epoch': 1, 'fast_dev_run': False, 'accumulate_grad_batches': None, 'max_epochs': None, 'min_epochs': None, 'max_steps': -1, 'min_steps': None, 'max_time': None, 'limit_train_batches': None, 'limit_val_batches': None, 'limit_test_batches': None, 'limit_predict_batches': None, 'val_check_interval': None, 'flush_logs_every_n_steps': None, 'log_every_n_steps': 50, 'accelerator': 'auto', 'strategy': None, 'sync_batchnorm': False, 'precision': 'bf16', 'enable_model_summary': True, 'weights_summary': 'top', 'weights_save_path': None, 'num_sanity_val_steps': 2, 'resume_from_checkpoint': None, 'profiler': None, 'benchmark': None, 'deterministic': None, 'reload_dataloaders_every_n_epochs': 0, 'auto_lr_find': False, 'replace_sampler_ddp': True, 'detect_anomaly': False, 'auto_scale_batch_size': False, 'prepare_data_per_node': None, 'plugins': None, 'amp_backend': 'native', 'amp_level': None, 'move_metrics_to_cpu': False, 'multiple_trainloader_mode': 'max_size_cycle', 'stochastic_weight_avg': False, 'terminate_on_nan': None, 'opts': ['train.epochs=8', 'train.batch_size=1', 'valid.step_size=0.5', 'valid.batch_size=4', 'train.additional.gradient_accumulation_steps=32', 'train.lr=0.00006', 'train.retriever_lr=0.00001', 'train.scheduler=linear', 'model_config.loss_ratio.additional_loss=1', 'model_config.RAVQA_loss_type=Approach6', 'data_loader.additional.num_knowledge_passages=5']}}
['/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA-instructblip.23646942/train', '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA-instructblip.23646942/train/saved_model', '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA-instructblip.23646942/train/imgs', '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data_TB/tb_logs/OKVQA-instructblip.23646942']
{'DATA_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data',
 'EXPERIMENT_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments',
 'TENSORBOARD_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data_TB/tb_logs',
 'WANDB': {'entity': 'xl544',
           'name': 'OKVQA-instructblip.23646942',
           'project': 'RAVQA',
           'tags': ['OKVQA', 'RAG', 'force_existence']},
 'args': {'DATA_FOLDER': '',
          'EXPERIMENT_FOLDER': '',
          'accelerator': 'auto',
          'accumulate_grad_batches': None,
          'amp_backend': 'native',
          'amp_level': None,
          'auto_lr_find': False,
          'auto_scale_batch_size': False,
          'auto_select_gpus': False,
          'benchmark': None,
          'check_val_every_n_epoch': 1,
          'checkpoint_callback': None,
          'config': '../configs/okvqa/RAVQA_instructblip.jsonnet',
          'default_root_dir': None,
          'detect_anomaly': False,
          'deterministic': None,
          'devices': '1',
          'enable_checkpointing': True,
          'enable_model_summary': True,
          'enable_progress_bar': True,
          'experiment_name': 'OKVQA-instructblip.23646942',
          'fast_dev_run': False,
          'flush_logs_every_n_steps': None,
          'gpus': None,
          'gradient_clip_algorithm': None,
          'gradient_clip_val': None,
          'ipus': None,
          'limit_predict_batches': None,
          'limit_test_batches': None,
          'limit_train_batches': None,
          'limit_val_batches': None,
          'log_every_n_steps': 50,
          'log_gpu_memory': None,
          'log_prediction_tables': False,
          'logger': True,
          'max_epochs': None,
          'max_steps': -1,
          'max_time': None,
          'min_epochs': None,
          'min_steps': None,
          'mode': 'train',
          'modules': ['force_existence'],
          'move_metrics_to_cpu': False,
          'multiple_trainloader_mode': 'max_size_cycle',
          'num_nodes': 1,
          'num_processes': None,
          'num_sanity_val_steps': 2,
          'opts': ['train.epochs=8',
                   'train.batch_size=1',
                   'valid.step_size=0.5',
                   'valid.batch_size=4',
                   'train.additional.gradient_accumulation_steps=32',
                   'train.lr=0.00006',
                   'train.retriever_lr=0.00001',
                   'train.scheduler=linear',
                   'model_config.loss_ratio.additional_loss=1',
                   'model_config.RAVQA_loss_type=Approach6',
                   'data_loader.additional.num_knowledge_passages=5'],
          'overfit_batches': 0.0,
          'plugins': None,
          'precision': 'bf16',
          'prepare_data_per_node': None,
          'process_position': 0,
          'profiler': None,
          'progress_bar_refresh_rate': None,
          'reload_dataloaders_every_n_epochs': 0,
          'replace_sampler_ddp': True,
          'reset': False,
          'resume_from_checkpoint': None,
          'stochastic_weight_avg': False,
          'strategy': None,
          'sync_batchnorm': False,
          'tags': [],
          'terminate_on_nan': None,
          'test_batch_size': -1,
          'test_evaluation_name': '',
          'tpu_cores': None,
          'track_grad_norm': -1,
          'val_check_interval': None,
          'weights_save_path': None,
          'weights_summary': 'top'},
 'cache': {'default_folder': '../data/ok-vqa/cache',
           'regenerate': {'clip_embeddings': 0,
                          'ocr_feature_preprocessed': 0,
                          'qformer_embeddings': 0,
                          'test_data_preprocessed': 0,
                          'train_data_preprocessed': 0,
                          'vinvl_feature_preprocessed': 0}},
 'cuda': 0,
 'data_loader': {'additional': {'max_decoder_source_length': 512,
                                'max_source_length': 512,
                                'max_target_length': 10,
                                'num_knowledge_passages': 5},
                 'dataset_modules': {'module_dict': {'LoadClipEmbeddings': {'config': {'clip_embeddings': {'train': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_train2014.pkl',
                                                                                                           'val': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_val2014.pkl'},
                                                                                       'instructBLIP_embeddings': {'train': '../data/ok-vqa/pre-extracted_features/blip2_head_embeddings/coco_instructblip_qformer_train2014.pkl',
                                                                                                                   'val': '../data/ok-vqa/pre-extracted_features/blip2_head_embeddings/coco_instructblip_qformer_val2014.pkl'},
                                                                                       'qformer_embeddings': {'train': '../data/ok-vqa/pre-extracted_features/blip2_head_embeddings/coco_hgface_qformer_train2014.pkl',
                                                                                                              'val': '../data/ok-vqa/pre-extracted_features/blip2_head_embeddings/coco_hgface_qformer_val2014.pkl'}},
                                                                            'option': 'default',
                                                                            'type': 'EmbeddingInput'},
                                                     'LoadGoogleOCRFeatures': {'config': {'combine_with_vinvl': True,
                                                                                          'test': '../data/ok-vqa/pre-extracted_features/OCR/valid',
                                                                                          'train': '../data/ok-vqa/pre-extracted_features/OCR/train'},
                                                                               'option': 'default',
                                                                               'type': 'LoadGoogleOCRFeatures'},
                                                     'LoadGoogleSearchAnnotations': {'config': {'annotations_path': {'test': '../data/ok-vqa/pre-extracted_features/passages/retriever_test.json',
                                                                                                                     'train': '../data/ok-vqa/pre-extracted_features/passages/retriever_train.json',
                                                                                                                     'valid': '../data/ok-vqa/pre-extracted_features/passages/retriever_testdev.json'}},
                                                                                     'option': 'default',
                                                                                     'type': 'LoadGoogleSearchAnnotations'},
                                                     'LoadGoogleSearchPassageData': {'config': {'passage_data_path': {'full': '../data/ok-vqa/pre-extracted_features/passages/okvqa_full_corpus.csv',
                                                                                                                      'train': '../data/ok-vqa/pre-extracted_features/passages/okvqa_train_corpus.csv'},
                                                                                                'use_full_split': True},
                                                                                     'option': 'default',
                                                                                     'type': 'LoadGoogleSearchPassageData'},
                                                     'LoadOKVQAData': {'config': {'image_data_path': {'test': '../data/ok-vqa/val2014',
                                                                                                      'train': '../data/ok-vqa/train2014'},
                                                                                  'vqa_data_path': {'annotation_files': {'test': '../data/ok-vqa/mscoco_val2014_annotations.json',
                                                                                                                         'train': '../data/ok-vqa/mscoco_train2014_annotations.json'},
                                                                                                    'question_files': {'test': '../data/ok-vqa/OpenEnded_mscoco_val2014_questions.json',
                                                                                                                       'train': '../data/ok-vqa/OpenEnded_mscoco_train2014_questions.json'}}},
                                                                       'option': 'default',
                                                                       'type': 'LoadOKVQAData'},
                                                     'LoadOscarCaptionFeatures': {'config': {'test': '../data/ok-vqa/pre-extracted_features/captions/test_predictions.json',
                                                                                             'train': '../data/ok-vqa/pre-extracted_features/captions/train_predictions.json',
                                                                                             'valid': '../data/ok-vqa/pre-extracted_features/captions/valid_predictions.json'},
                                                                                  'option': 'default',
                                                                                  'type': 'LoadOscarCaptionFeatures'},
                                                     'LoadPretrainedDPROutputForGoogleSearchPassage': {'config': {'pretrained_dpr_outputs': {'test': '../Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/test/test_evaluation/test_predictions.json',
                                                                                                                                             'train': '../Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/test/test_evaluation/train_predictions.json'}},
                                                                                                       'option': 'none',
                                                                                                       'type': 'LoadPretrainedDPROutputForGoogleSearchPassage'},
                                                     'LoadVinVLFeatures': {'config': {'test': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_testset_full/inference/vinvl_vg_x152c4/predictions.tsv',
                                                                                      'train': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_trainset_full/inference/vinvl_vg_x152c4/predictions.tsv'},
                                                                           'option': 'default',
                                                                           'type': 'LoadVinVLFeatures'}},
                                     'module_list': ['LoadVinVLFeatures',
                                                     'LoadGoogleOCRFeatures',
                                                     'LoadOscarCaptionFeatures',
                                                     'LoadOKVQAData',
                                                     'LoadGoogleSearchPassageData']},
                 'dataset_type': 'OKVQADatasetBLIP2Text',
                 'dummy_dataloader': 0,
                 'index_files': {'index_passages_path': '../data/ok-vqa/pre-extracted_features/faiss/ok-vqa-passages-full-caption-pretrained-NewRun/my_knowledge_dataset',
                                 'index_path': '../data/ok-vqa/pre-extracted_features/faiss/ok-vqa-passages-full-caption-pretrained-NewRun/my_knowledge_dataset_hnsw_index.faiss'},
                 'type': 'DataLoaderOKVQAWithKnowledge'},
 'experiment_name': 'OKVQA-instructblip.23646942',
 'experiment_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA-instructblip.23646942',
 'gpu_device': 0,
 'ignore_pretrained_weights': [],
 'imgs_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA-instructblip.23646942/train/imgs',
 'log_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA-instructblip.23646942/train',
 'metrics': [{'name': 'compute_exact_match'},
             {'name': 'compute_retrieval_metrics'},
             {'name': 'compute_okvqa_scores'}],
 'mode': 'train',
 'model_config': {'DECODER_SPECIAL_TOKENS': {'additional_special_tokens': [],
                                             'bos_token': '<PAD>',
                                             'pad_token': '<PAD>'},
                  'DecoderTokenizerClass': 'InstructBlipProcessor',
                  'DecoderTokenizerModelVersion': 'Salesforce/instructblip-flan-t5-xl',
                  'GeneratorModelClass': 'InstructBlipForConditionalGeneration',
                  'GeneratorModelVersion': 'Salesforce/instructblip-flan-t5-xl',
                  'LoadPretrainMLP': 0,
                  'ModelClass': 'RagModelInstructBLIP',
                  'QueryEncoderConfigClass': 'DPRConfig',
                  'QueryEncoderModelClass': 'DPRQuestionEncoder',
                  'QueryEncoderModelVersion': '/home/xl544/rds/rds-cvnlp-hirYTW1FQIw/wl356/projects/Retrieval-Augmented-Visual-Question-Answering/Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/train/saved_model/epoch6/query_encoder',
                  'RAVQA_loss_type': 'Approach6',
                  'SPECIAL_TOKENS': {'additional_special_tokens': []},
                  'TokenizerClass': 'DPRQuestionEncoderTokenizer',
                  'TokenizerModelVersion': 'facebook/dpr-question_encoder-single-nq-base',
                  'UseInstructBLIPEmb': 0,
                  'UsePrefixEmb': 0,
                  'UseQformerEmb': 0,
                  'base_model': 'RAG',
                  'decoder_input_modules': {'module_list': [],
                                            'postprocess_module_list': []},
                  'input_modules': {'module_list': [{'option': 'default',
                                                     'separation_tokens': {'end': '',
                                                                           'start': 'Question:'},
                                                     'type': 'QuestionInput'}],
                                    'postprocess_module_list': [{'option': 'default',
                                                                 'type': 'PostProcessInputTokenization'}]},
                  'loss_ratio': {'additional_loss': 1,
                                 'nll_loss': 1,
                                 'rag_loss': 0},
                  'modules': ['force_existence'],
                  'output_modules': {'module_list': [{'option': 'default',
                                                      'type': 'GenerationOutput'}],
                                     'postprocess_module_list': [{'option': 'default',
                                                                  'type': 'PostProcessOutputTokenizationInstructBLIP'}]},
                  'pretrained': 1,
                  'rag_modules': {'module_list': []}},
 'platform_type': 'pytorch',
 'reset': False,
 'saved_model_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA-instructblip.23646942/train/saved_model',
 'seed': 2021,
 'tensorboard_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data_TB/tb_logs/OKVQA-instructblip.23646942',
 'test': {'additional': {'multiprocessing': 4},
          'batch_size': 32,
          'evaluation_name': 'test_evaluation',
          'load_best_model': 0,
          'load_epoch': -1,
          'load_model_path': '',
          'num_evaluation': 0},
 'train': {'MLP_lr': 0.0001,
           'adam_epsilon': 1e-08,
           'additional': {'gradient_accumulation_steps': 32,
                          'gradient_clipping': 0,
                          'plugins': [],
                          'save_top_k': 1,
                          'save_top_k_metric': 'test/accuracy_overall',
                          'save_top_k_mode': 'max',
                          'warmup_steps': 0},
           'batch_size': 1,
           'epochs': 8,
           'load_best_model': 0,
           'load_epoch': -1,
           'load_model_path': '',
           'lr': 6e-05,
           'retriever_lr': 1e-05,
           'save_interval': 1,
           'scheduler': 'linear',
           'type': 'RagExecutorInstructBLIP'},
 'valid': {'additional': {},
           'batch_size': 4,
           'break_interval': 3000,
           'step_size': 0.5}}
data columns: dict_keys(['vinvl_features'])
data columns: dict_keys(['vinvl_features', 'ocr_features'])
[Data Statistics] Caption features 123287
data columns: dict_keys(['vinvl_features', 'ocr_features', 'caption_features'])
loading VQA annotations and questions into memory...
time elpased:  0:00:00.194207
creating index...
index created!
loading VQA annotations and questions into memory...
time elpased:  0:00:00.112232
creating index...
index created!
creating index...
index created!
year: 2019
version: 1.0
description: This is v1.0 of the OK-VQA dataset.
creating index...
index created!
year: 2019
version: 1.0
description: This is v1.0 of the OK-VQA dataset.
data columns: dict_keys(['vinvl_features', 'ocr_features', 'caption_features', 'okvqa_data', 'vqa_data'])
data columns: dict_keys(['vinvl_features', 'ocr_features', 'caption_features', 'okvqa_data', 'vqa_data', 'passages'])
Uses full checkpoint from Salesforce/instructblip-flan-t5-xl
r value for LoRA is 8
trainable params: 4718592 || all params: 2854475776 || trainable%: 0.16530502867367827
initializing retrieval
Sanity Checking: 0it [00:00, ?it/s]Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]race <---> racing    (<pad> racing</s><pad><pad><pad><pad>)
vine <---> succulent    (<pad> succulent</s><pad><pad><pad><pad>)
stuffed animal <---> teddy bear    (<pad> teddy bear</s>)
mouth <---> mouth    (<pad> mouth</s><pad><pad><pad><pad>)
Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:11<00:11, 11.64s/it]Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:11<00:11, 11.64s/it]cloth <---> luggage    (<pad> luggage</s><pad><pad><pad><pad><pad>)
man <---> no one    (<pad> no one</s><pad><pad><pad><pad>)
down <---> down    (<pad> down</s><pad><pad><pad><pad><pad>)
island <---> island    (<pad> island</s><pad><pad><pad><pad><pad>)
Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:12<00:00,  5.39s/it]Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:12<00:00,  5.39s/it]Loading and preparing results...     
{'test/epoch': 0,
 'test/exact_match_at_1': 0.5,
 'test/exact_match_at_2': 0.625,
 'test/exact_match_at_3': 0.75,
 'test/exact_match_at_4': 0.875,
 'test/exact_match_at_5': 0.875,
 'test/failed_hit': 0.15,
 'test/failed_no_hit': 0.4,
 'test/gold_precision_at_5': 0.275,
 'test/gold_recall_at_5': 0.5,
 'test/n_retrieved_docs': 5,
 'test/precision_at_5': 0.35000000000000003,
 'test/recall_at_5': 0.5,
 'test/selected_failed_hit': 0.25,
 'test/selected_failed_no_hit': 0.25,
 'test/selected_successful_hit': 0.375,
 'test/selected_successful_no_hit': 0.125,
 'test/successful_hit': 0.25,
 'test/successful_no_hit': 0.2}
{'retrieval_predictions_epoch0_MODE(train)_SET(TEST)_K(5)': <wandb.data_types.Table object at 0x151f24896c10>}
                                                                           Training: 0it [00:00, ?it/s]Training:   0%|          | 0/11533 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/11533 [00:00<?, ?it/s] 