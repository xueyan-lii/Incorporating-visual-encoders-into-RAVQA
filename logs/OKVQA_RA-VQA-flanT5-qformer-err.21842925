/home/xl544/.conda/envs/RAVQA/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning
  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')
[38;20m[INFO] - __main__ : Initialization done with the config: {'DATA_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data', 'EXPERIMENT_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments', 'TENSORBOARD_FOLDER': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data_TB/tb_logs', 'WANDB': {'entity': 'xl544', 'project': 'RAVQA', 'tags': ['OKVQA', 'RAG', 'freeze_question_encoder', 'force_existence'], 'name': 'OKVQA_RA-VQA-flanT5-qformer.21842925'}, 'cache': {'default_folder': '../data/ok-vqa/cache', 'regenerate': {'clip_embeddings': 0, 'ocr_feature_preprocessed': 0, 'qformer_embeddings': 0, 'test_data_preprocessed': 0, 'train_data_preprocessed': 0, 'vinvl_feature_preprocessed': 0}}, 'cuda': 0, 'data_loader': {'additional': {'max_decoder_source_length': 512, 'max_source_length': 512, 'max_target_length': 10, 'num_knowledge_passages': 5}, 'dataset_modules': {'module_dict': {'LoadClipEmbeddings': {'config': {'clip_embeddings': {'train': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_train2014.pkl', 'val': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_val2014.pkl'}, 'qformer_embeddings': {'train': '../data/ok-vqa/pre-extracted_features/blip2_head_embeddings/coco_eva_clip_g_qformer_train2014.pkl', 'val': '../data/ok-vqa/pre-extracted_features/blip2_head_embeddings/coco_eva_clip_g_qformer_val2014.pkl'}}, 'option': 'default', 'type': 'EmbeddingInput'}, 'LoadGoogleOCRFeatures': {'config': {'combine_with_vinvl': True, 'test': '../data/ok-vqa/pre-extracted_features/OCR/valid', 'train': '../data/ok-vqa/pre-extracted_features/OCR/train'}, 'option': 'default', 'type': 'LoadGoogleOCRFeatures'}, 'LoadGoogleSearchAnnotations': {'config': {'annotations_path': {'test': '../data/ok-vqa/pre-extracted_features/passages/retriever_test.json', 'train': '../data/ok-vqa/pre-extracted_features/passages/retriever_train.json', 'valid': '../data/ok-vqa/pre-extracted_features/passages/retriever_testdev.json'}}, 'option': 'default', 'type': 'LoadGoogleSearchAnnotations'}, 'LoadGoogleSearchPassageData': {'config': {'passage_data_path': {'full': '../data/ok-vqa/pre-extracted_features/passages/okvqa_full_corpus.csv', 'train': '../data/ok-vqa/pre-extracted_features/passages/okvqa_train_corpus.csv'}, 'use_full_split': True}, 'option': 'default', 'type': 'LoadGoogleSearchPassageData'}, 'LoadOKVQAData': {'config': {'image_data_path': {'test': '../data/ok-vqa/val2014', 'train': '../data/ok-vqa/train2014'}, 'vqa_data_path': {'annotation_files': {'test': '../data/ok-vqa/mscoco_val2014_annotations.json', 'train': '../data/ok-vqa/mscoco_train2014_annotations.json'}, 'question_files': {'test': '../data/ok-vqa/OpenEnded_mscoco_val2014_questions.json', 'train': '../data/ok-vqa/OpenEnded_mscoco_train2014_questions.json'}}}, 'option': 'default', 'type': 'LoadOKVQAData'}, 'LoadOscarCaptionFeatures': {'config': {'test': '../data/ok-vqa/pre-extracted_features/captions/test_predictions.json', 'train': '../data/ok-vqa/pre-extracted_features/captions/train_predictions.json', 'valid': '../data/ok-vqa/pre-extracted_features/captions/valid_predictions.json'}, 'option': 'default', 'type': 'LoadOscarCaptionFeatures'}, 'LoadPretrainedDPROutputForGoogleSearchPassage': {'config': {'pretrained_dpr_outputs': {'test': '../Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/test/test_evaluation/test_predictions.json', 'train': '../Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/test/test_evaluation/train_predictions.json'}}, 'option': 'none', 'type': 'LoadPretrainedDPROutputForGoogleSearchPassage'}, 'LoadVinVLFeatures': {'config': {'test': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_testset_full/inference/vinvl_vg_x152c4/predictions.tsv', 'train': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_trainset_full/inference/vinvl_vg_x152c4/predictions.tsv'}, 'option': 'default', 'type': 'LoadVinVLFeatures'}}, 'module_list': ['LoadVinVLFeatures', 'LoadGoogleOCRFeatures', 'LoadOscarCaptionFeatures', 'LoadOKVQAData', 'LoadGoogleSearchPassageData', 'LoadClipEmbeddings']}, 'dataset_type': 'OKVQADataset', 'dummy_dataloader': 0, 'index_files': {'index_passages_path': '../data/ok-vqa/pre-extracted_features/faiss/ok-vqa-passages-full-caption-pretrained-NewRun/my_knowledge_dataset', 'index_path': '../data/ok-vqa/pre-extracted_features/faiss/ok-vqa-passages-full-caption-pretrained-NewRun/my_knowledge_dataset_hnsw_index.faiss'}, 'type': 'DataLoaderOKVQAWithKnowledge'}, 'experiment_name': 'OKVQA_RA-VQA-flanT5-qformer.21842925', 'gpu_device': 0, 'ignore_pretrained_weights': [], 'metrics': [{'name': 'compute_exact_match'}, {'name': 'compute_retrieval_metrics'}, {'name': 'compute_okvqa_scores'}], 'model_config': {'DECODER_SPECIAL_TOKENS': {'additional_special_tokens': ['<BOV>', '<SOV>', '<EOV>', '<BOQ>', '<EOQ>', '<BOC>', '<EOC>', '<BOK>', '<EOK>'], 'bos_token': '<PAD>', 'pad_token': '<PAD>'}, 'DecoderTokenizerClass': 'T5Tokenizer', 'DecoderTokenizerModelVersion': 'google/flan-t5-large', 'GeneratorConfigClass': 'T5Config', 'GeneratorModelClass': 'T5ForConditionalGeneration', 'GeneratorModelVersion': 'google/flan-t5-large', 'LoadPretrainedMLPWeights': 0, 'ModelClass': 'RagModel', 'PretrainedMLPPath': '', 'QueryEncoderConfigClass': 'DPRConfig', 'QueryEncoderModelClass': 'DPRQuestionEncoder', 'QueryEncoderModelVersion': '../Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/train/saved_model/epoch6/query_encoder', 'RAVQA_loss_type': 'Approach6', 'SPECIAL_TOKENS': {'additional_special_tokens': ['<BOV>', '<SOV>', '<EOV>', '<BOQ>', '<EOQ>', '<BOC>', '<EOC>', '<BOK>', '<EOK>']}, 'TokenizerClass': 'DPRQuestionEncoderTokenizer', 'TokenizerModelVersion': 'facebook/dpr-question_encoder-single-nq-base', 'UsePrefixEmb': 0.5, 'UseQformerEmb': 1, 'base_model': 'RAG', 'decoder_input_modules': {'module_list': [], 'postprocess_module_list': []}, 'input_modules': {'module_list': [{'option': 'default', 'separation_tokens': {'end': '<EOQ>', 'start': '<BOQ>'}, 'type': 'QuestionInput'}, {'option': 'caption', 'separation_tokens': {'end': '<EOC>', 'start': '<BOC>'}, 'type': 'TextBasedVisionInput'}, {'attribute_max': 3, 'attribute_thres': 0.05, 'object_max': 40, 'ocr': 1, 'option': 'object', 'separation_tokens': {'end': '<EOV>', 'sep': '<SOV>', 'start': '<BOV>'}, 'type': 'TextBasedVisionInput'}, {'option': 'default', 'type': 'EmbeddingInput'}], 'postprocess_module_list': [{'option': 'default', 'type': 'PostProcessInputTokenization'}, {'option': 'default', 'type': 'PostProcessClipEmbeddings'}]}, 'loss_ratio': {'additional_loss': 0, 'nll_loss': 1, 'rag_loss': 0, 'retrieval_pseudo_loss': 0}, 'modules': ['freeze_question_encoder', 'force_existence'], 'output_modules': {'module_list': [{'option': 'default', 'type': 'GenerationOutput'}], 'postprocess_module_list': [{'option': 'default', 'type': 'PostProcessOutputTokenization'}]}, 'pretrained': 1, 'rag_modules': {'module_list': []}}, 'platform_type': 'pytorch', 'seed': 2021, 'test': {'additional': {'multiprocessing': 4}, 'batch_size': 32, 'evaluation_name': 'test_evaluation', 'load_best_model': 0, 'load_epoch': -1, 'load_model_path': '', 'num_evaluation': 0}, 'train': {'adam_epsilon': 1e-08, 'additional': {'gradient_accumulation_steps': 16, 'gradient_clipping': 0, 'plugins': [], 'save_top_k': 1, 'save_top_k_metric': 'test/accuracy_overall', 'save_top_k_mode': 'max', 'warmup_steps': 0}, 'batch_size': 2, 'epochs': 8, 'load_best_model': 0, 'load_epoch': -1, 'load_model_path': '', 'lr': 6e-05, 'retriever_lr': 1e-05, 'save_interval': 1, 'scheduler': 'linear', 'type': 'RagExecutor'}, 'valid': {'additional': {}, 'batch_size': 32, 'break_interval': 3000, 'step_size': 0.25}, 'reset': False, 'mode': 'train', 'log_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-flanT5-qformer.21842925/train', 'experiment_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-flanT5-qformer.21842925', 'saved_model_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-flanT5-qformer.21842925/train/saved_model', 'imgs_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-flanT5-qformer.21842925/train/imgs', 'tensorboard_path': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data_TB/tb_logs/OKVQA_RA-VQA-flanT5-qformer.21842925', 'args': {'config': '../configs/okvqa/RAVQA.jsonnet', 'DATA_FOLDER': '', 'EXPERIMENT_FOLDER': '', 'mode': 'train', 'reset': False, 'experiment_name': 'OKVQA_RA-VQA-flanT5-qformer.21842925', 'tags': [], 'modules': ['freeze_question_encoder', 'force_existence'], 'log_prediction_tables': False, 'test_batch_size': -1, 'test_evaluation_name': '', 'logger': True, 'checkpoint_callback': None, 'enable_checkpointing': True, 'default_root_dir': None, 'gradient_clip_val': None, 'gradient_clip_algorithm': None, 'process_position': 0, 'num_nodes': 1, 'num_processes': None, 'devices': '1', 'gpus': None, 'auto_select_gpus': False, 'tpu_cores': None, 'ipus': None, 'log_gpu_memory': None, 'progress_bar_refresh_rate': None, 'enable_progress_bar': True, 'overfit_batches': 0.0, 'track_grad_norm': -1, 'check_val_every_n_epoch': 1, 'fast_dev_run': False, 'accumulate_grad_batches': None, 'max_epochs': None, 'min_epochs': None, 'max_steps': -1, 'min_steps': None, 'max_time': None, 'limit_train_batches': None, 'limit_val_batches': None, 'limit_test_batches': None, 'limit_predict_batches': None, 'val_check_interval': None, 'flush_logs_every_n_steps': None, 'log_every_n_steps': 50, 'accelerator': 'auto', 'strategy': None, 'sync_batchnorm': False, 'precision': 32, 'enable_model_summary': True, 'weights_summary': 'top', 'weights_save_path': None, 'num_sanity_val_steps': 2, 'resume_from_checkpoint': None, 'profiler': None, 'benchmark': None, 'deterministic': None, 'reload_dataloaders_every_n_epochs': 0, 'auto_lr_find': False, 'replace_sampler_ddp': True, 'detect_anomaly': False, 'auto_scale_batch_size': False, 'prepare_data_per_node': None, 'plugins': None, 'amp_backend': 'native', 'amp_level': None, 'move_metrics_to_cpu': False, 'multiple_trainloader_mode': 'max_size_cycle', 'stochastic_weight_avg': False, 'terminate_on_nan': None, 'opts': ['train.epochs=8', 'train.batch_size=2', 'valid.step_size=0.25', 'valid.batch_size=32', 'train.additional.gradient_accumulation_steps=16', 'train.lr=0.00006', 'train.retriever_lr=0.00001', 'train.scheduler=linear', 'data_loader.additional.num_knowledge_passages=5', 'model_config.UseQformerEmb=1', 'model_config.UsePrefixEmb=0.5', 'model_config.DecoderTokenizerModelVersion=google/flan-t5-large', 'model_config.GeneratorModelVersion=google/flan-t5-large']}}[0m
Global seed set to 2021
[38;20m[INFO] - __main__ : All seeds have been set to 2021[0m
[38;20m[INFO] - __main__ : init wandb logger with the following settings: {'entity': 'xl544', 'project': 'RAVQA', 'tags': ['OKVQA', 'RAG', 'freeze_question_encoder', 'force_existence'], 'name': 'OKVQA_RA-VQA-flanT5-qformer.21842925'}[0m
wandb: Currently logged in as: xl544. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.2
wandb: Run data is saved locally in /rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/src/wandb/run-20230607_022237-axoq6kpf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run OKVQA_RA-VQA-flanT5-qformer.21842925
wandb: ⭐️ View project at https://wandb.ai/xl544/RAVQA
wandb: 🚀 View run at https://wandb.ai/xl544/RAVQA/runs/axoq6kpf
Multiprocessing is handled by SLURM.
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[38;20m[INFO] - __main__ : arguments passed to trainer: Namespace(DATA_FOLDER='', EXPERIMENT_FOLDER='', accelerator='auto', accumulate_grad_batches=None, amp_backend='native', amp_level=None, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, benchmark=None, check_val_every_n_epoch=1, checkpoint_callback=None, config='../configs/okvqa/RAVQA.jsonnet', default_root_dir=None, detect_anomaly=False, deterministic=None, devices='1', enable_checkpointing=True, enable_model_summary=True, enable_progress_bar=True, experiment_name='OKVQA_RA-VQA-flanT5-qformer.21842925', fast_dev_run=False, flush_logs_every_n_steps=None, gpus=None, gradient_clip_algorithm=None, gradient_clip_val=None, ipus=None, limit_predict_batches=None, limit_test_batches=None, limit_train_batches=None, limit_val_batches=None, log_every_n_steps=50, log_gpu_memory=None, log_prediction_tables=False, logger=True, max_epochs=None, max_steps=-1, max_time=None, min_epochs=None, min_steps=None, mode='train', modules=['freeze_question_encoder', 'force_existence'], move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=None, num_sanity_val_steps=2, opts=['train.epochs=8', 'train.batch_size=2', 'valid.step_size=0.25', 'valid.batch_size=32', 'train.additional.gradient_accumulation_steps=16', 'train.lr=0.00006', 'train.retriever_lr=0.00001', 'train.scheduler=linear', 'data_loader.additional.num_knowledge_passages=5', 'model_config.UseQformerEmb=1', 'model_config.UsePrefixEmb=0.5', 'model_config.DecoderTokenizerModelVersion=google/flan-t5-large', 'model_config.GeneratorModelVersion=google/flan-t5-large'], overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=None, process_position=0, profiler=None, progress_bar_refresh_rate=None, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, reset=False, resume_from_checkpoint=None, stochastic_weight_avg=False, strategy=None, sync_batchnorm=False, tags=[], terminate_on_nan=None, test_batch_size=-1, test_evaluation_name='', tpu_cores=None, track_grad_norm=-1, val_check_interval=None, weights_save_path=None, weights_summary='top')[0m
[38;20m[INFO] - __main__ : additional arguments passed to trainer: {'accumulate_grad_batches': 16, 'default_root_dir': '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-flanT5-qformer.21842925/train/saved_model', 'max_epochs': 8, 'logger': [<pytorch_lightning.loggers.tensorboard.TensorBoardLogger object at 0x145a885d57f0>, <pytorch_lightning.loggers.wandb.WandbLogger object at 0x145a885d56d0>, <utils.metrics_log_callback.MetricsHistoryLogger object at 0x145a88109c10>], 'callbacks': [<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x145a885d5d60>, <pytorch_lightning.callbacks.progress.tqdm_progress.TQDMProgressBar object at 0x145a880b77c0>, <pytorch_lightning.callbacks.model_summary.ModelSummary object at 0x145a880b70a0>, <pytorch_lightning.callbacks.gradient_accumulation_scheduler.GradientAccumulationScheduler object at 0x145a880b7220>], 'plugins': [], 'log_every_n_steps': 10, 'val_check_interval': 0.25}[0m
[33;20m[WARNING] - __main__ : No checkpoint exists from '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-flanT5-qformer.21842925/train/saved_model/last.ckpt'. Skipping...[0m
[38;20m[INFO] - __main__ : **First time to train**[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'test': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_testset_full/inference/vinvl_vg_x152c4/predictions.tsv', 'train': '../data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_trainset_full/inference/vinvl_vg_x152c4/predictions.tsv'}, 'option': 'default', 'type': 'LoadVinVLFeatures'}[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/vinvl_feature_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data Statistics] VinVL features 14031[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'combine_with_vinvl': True, 'test': '../data/ok-vqa/pre-extracted_features/OCR/valid', 'train': '../data/ok-vqa/pre-extracted_features/OCR/train'}, 'option': 'default', 'type': 'LoadGoogleOCRFeatures'}[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/ocr_feature_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data Statistics] OCR features 14031, 5462 has annotations.[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : OCR feature detected in VinVL feature dict...skipping..[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'test': '../data/ok-vqa/pre-extracted_features/captions/test_predictions.json', 'train': '../data/ok-vqa/pre-extracted_features/captions/train_predictions.json', 'valid': '../data/ok-vqa/pre-extracted_features/captions/valid_predictions.json'}, 'option': 'default', 'type': 'LoadOscarCaptionFeatures'}[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'image_data_path': {'test': '../data/ok-vqa/val2014', 'train': '../data/ok-vqa/train2014'}, 'vqa_data_path': {'annotation_files': {'test': '../data/ok-vqa/mscoco_val2014_annotations.json', 'train': '../data/ok-vqa/mscoco_train2014_annotations.json'}, 'question_files': {'test': '../data/ok-vqa/OpenEnded_mscoco_val2014_questions.json', 'train': '../data/ok-vqa/OpenEnded_mscoco_train2014_questions.json'}}}, 'option': 'default', 'type': 'LoadOKVQAData'}[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/train_data_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data statistics] split: train  entries: 9009[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/test_data_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data statistics] split: test  entries: 5046[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'passage_data_path': {'full': '../data/ok-vqa/pre-extracted_features/passages/okvqa_full_corpus.csv', 'train': '../data/ok-vqa/pre-extracted_features/passages/okvqa_train_corpus.csv'}, 'use_full_split': True}, 'option': 'default', 'type': 'LoadGoogleSearchPassageData'}[0m
  0%|          | 0/168380 [00:00<?, ?it/s] 10%|▉         | 16445/168380 [00:00<00:00, 164445.06it/s] 20%|█▉        | 32890/168380 [00:00<00:00, 161397.08it/s] 29%|██▉       | 49034/168380 [00:00<00:00, 161096.95it/s] 39%|███▉      | 65470/168380 [00:00<00:00, 162369.96it/s] 49%|████▉     | 82129/168380 [00:00<00:00, 163876.85it/s] 59%|█████▊    | 98537/168380 [00:00<00:00, 163943.24it/s] 69%|██████▊   | 115623/168380 [00:00<00:00, 166193.48it/s] 80%|███████▉  | 134470/168380 [00:00<00:00, 173268.05it/s] 91%|█████████▏| 154012/168380 [00:00<00:00, 180180.76it/s]100%|█████████▉| 168307/168380 [00:00<00:00, 172745.63it/s]
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'clip_embeddings': {'train': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_train2014.pkl', 'val': '../data/ok-vqa/pre-extracted_features/clip_embeddings/coco_ViT-L_14@336px_val2014.pkl'}, 'qformer_embeddings': {'train': '../data/ok-vqa/pre-extracted_features/blip2_head_embeddings/coco_eva_clip_g_qformer_train2014.pkl', 'val': '../data/ok-vqa/pre-extracted_features/blip2_head_embeddings/coco_eva_clip_g_qformer_val2014.pkl'}}, 'option': 'default', 'type': 'EmbeddingInput'}[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from ../data/ok-vqa/cache/qformer_embeddings.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data Statistics] CLIP embeddings 14031[0m
[38;20m[INFO] - data_loader_manager.datasets.okvqa_datasets : initialising OKVQADataset...[0m
[38;20m[INFO] - data_loader_manager.datasets.okvqa_datasets : initialising OKVQADataset...[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa_with_knowledge : [Data Statistics]: training data loader: 4505;  test data loader: 158[0m
[38;20m[INFO] - torch.distributed.nn.jit.instantiator : Created a temporary directory at /tmp/tmp7j7voi8j[0m
[38;20m[INFO] - torch.distributed.nn.jit.instantiator : Writing /tmp/tmp7j7voi8j/_remote_module_non_scriptable.py[0m
[38;20m[INFO] - trainers.base_executor : Initializing RagExecutor...[0m
[38;20m[INFO] - __main__ : config file was successfully saved to /rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-flanT5-qformer.21842925 for future use.[0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[38;20m[INFO] - trainers.rag_executor : using different learning rate for retriever[0m
[38;20m[INFO] - trainers.rag_executor : #params: 558   lr: 6e-05[0m
[38;20m[INFO] - trainers.rag_executor : #params: 2   lr: 1e-05[0m
Loading `train_dataloader` to estimate number of stepping batches.
/home/xl544/.conda/envs/RAVQA/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(

  | Name  | Type     | Params
-----------------------------------
0 | model | RagModel | 893 M 
-----------------------------------
783 M     Trainable params
109 M     Non-trainable params
893 M     Total params
3,573.559 Total estimated model params size (MB)
Missing logger folder: /rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Data_TB/tb_logs/OKVQA_RA-VQA-flanT5-qformer.21842925/OKVQA_RA-VQA-flanT5-qformer.21842925
SLURM auto-requeueing enabled. Setting signal handlers.
/home/xl544/.conda/envs/RAVQA/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Failed to compute OKVQA scores: Results do not correspond to current VQA set. Either the results do not have predictions for all question ids in annotation file or there is atleast one question id that does not belong to the question ids in the annotation file.This could be due to the fact that OKVQA parser requires all questions to evaluatethe accuracy. Ignore this error if this is the sanity check.[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [sanity_check]: {'test/exact_match_at_1': 0.046875, 'test/exact_match_at_2': 0.046875, 'test/exact_match_at_3': 0.046875, 'test/exact_match_at_4': 0.046875, 'test/exact_match_at_5': 0.046875, 'test/recall_at_5': 0.796875, 'test/precision_at_5': 0.515625, 'test/gold_precision_at_5': 0.34062500000000007, 'test/gold_recall_at_5': 0.65625, 'test/successful_hit': 0.009375, 'test/successful_no_hit': 0.0, 'test/failed_hit': 0.515625, 'test/failed_no_hit': 0.475, 'test/selected_successful_hit': 0.046875, 'test/selected_successful_no_hit': 0.0, 'test/selected_failed_hit': 0.625, 'test/selected_failed_no_hit': 0.328125, 'test/n_retrieved_docs': 5, 'test/epoch': 0}[0m
[33;20m[WARNING] - root : Sanity check mode, not saving to loggers.[0m
/home/xl544/.conda/envs/RAVQA/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.48434403487911215, 'test/exact_match_at_2': 0.6123662306777645, 'test/exact_match_at_3': 0.6630994847403884, 'test/exact_match_at_4': 0.678359096313912, 'test/exact_match_at_5': 0.6829171621086009, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3737613951644867, 'test/gold_recall_at_5': 0.6666666666666666, 'test/successful_hit': 0.32508917954815697, 'test/successful_no_hit': 0.1090764962346413, 'test/failed_hit': 0.39865239793896157, 'test/failed_no_hit': 0.1671819262782402, 'test/selected_successful_hit': 0.3923900118906064, 'test/selected_successful_no_hit': 0.09195402298850575, 'test/selected_failed_hit': 0.4288545382481173, 'test/selected_failed_no_hit': 0.08680142687277051, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 44.49, 'test/accuracy_QuestionType_one': 39.56, 'test/accuracy_QuestionType_eight': 42.65, 'test/accuracy_QuestionType_other': 45.53, 'test/accuracy_QuestionType_seven': 45.7, 'test/accuracy_QuestionType_four': 51.71, 'test/accuracy_QuestionType_five': 46.04, 'test/accuracy_QuestionType_three': 41.78, 'test/accuracy_QuestionType_nine': 32.14, 'test/accuracy_QuestionType_ten': 43.72, 'test/accuracy_QuestionType_two': 51.63, 'test/accuracy_QuestionType_six': 45.67, 'test/accuracy_AnswerType_other': 44.49, 'test/epoch': 0}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5164486722156163, 'test/exact_match_at_2': 0.6422909235037654, 'test/exact_match_at_3': 0.683115338882283, 'test/exact_match_at_4': 0.6969877130400317, 'test/exact_match_at_5': 0.70055489496631, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3737613951644867, 'test/gold_recall_at_5': 0.6666666666666666, 'test/successful_hit': 0.3410622275069362, 'test/successful_no_hit': 0.13111375346809354, 'test/failed_hit': 0.34050733254062626, 'test/failed_no_hit': 0.18731668648434405, 'test/selected_successful_hit': 0.40883868410622276, 'test/selected_successful_no_hit': 0.10760998810939358, 'test/selected_failed_hit': 0.375743162901308, 'test/selected_failed_no_hit': 0.10780816488307571, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 47.57, 'test/accuracy_QuestionType_one': 41.6, 'test/accuracy_QuestionType_eight': 45.88, 'test/accuracy_QuestionType_other': 49.21, 'test/accuracy_QuestionType_seven': 48.74, 'test/accuracy_QuestionType_four': 54.89, 'test/accuracy_QuestionType_five': 48.92, 'test/accuracy_QuestionType_three': 44.58, 'test/accuracy_QuestionType_nine': 38.33, 'test/accuracy_QuestionType_ten': 47.6, 'test/accuracy_QuestionType_two': 56.4, 'test/accuracy_QuestionType_six': 48.37, 'test/accuracy_AnswerType_other': 47.57, 'test/epoch': 0}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5338882282996433, 'test/exact_match_at_2': 0.6569560047562426, 'test/exact_match_at_3': 0.6950059453032105, 'test/exact_match_at_4': 0.7098692033293698, 'test/exact_match_at_5': 0.713238208481966, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3737613951644867, 'test/gold_recall_at_5': 0.6666666666666666, 'test/successful_hit': 0.34926674593737617, 'test/successful_no_hit': 0.13777249306381292, 'test/failed_hit': 0.3363456202933016, 'test/failed_no_hit': 0.17661514070550932, 'test/selected_successful_hit': 0.42588188664288545, 'test/selected_successful_no_hit': 0.10800634165675783, 'test/selected_failed_hit': 0.3654379706698375, 'test/selected_failed_no_hit': 0.10067380103051922, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 49.0, 'test/accuracy_QuestionType_one': 44.85, 'test/accuracy_QuestionType_eight': 46.51, 'test/accuracy_QuestionType_other': 51.11, 'test/accuracy_QuestionType_seven': 49.16, 'test/accuracy_QuestionType_four': 55.24, 'test/accuracy_QuestionType_five': 51.16, 'test/accuracy_QuestionType_three': 47.38, 'test/accuracy_QuestionType_nine': 35.0, 'test/accuracy_QuestionType_ten': 46.51, 'test/accuracy_QuestionType_two': 53.6, 'test/accuracy_QuestionType_six': 50.92, 'test/accuracy_AnswerType_other': 49.0, 'test/epoch': 0}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5392390011890607, 'test/exact_match_at_2': 0.6607213634562029, 'test/exact_match_at_3': 0.7059056678557273, 'test/exact_match_at_4': 0.7205707491082045, 'test/exact_match_at_5': 0.7251288149028934, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3737613951644867, 'test/gold_recall_at_5': 0.6666666666666666, 'test/successful_hit': 0.35766944114149823, 'test/successful_no_hit': 0.1300832342449465, 'test/failed_hit': 0.35616329766151406, 'test/failed_no_hit': 0.1560840269520412, 'test/selected_successful_hit': 0.43876337693222356, 'test/selected_successful_no_hit': 0.10047562425683709, 'test/selected_failed_hit': 0.3803012286959968, 'test/selected_failed_no_hit': 0.08045977011494253, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 49.5, 'test/accuracy_QuestionType_one': 45.76, 'test/accuracy_QuestionType_eight': 48.16, 'test/accuracy_QuestionType_other': 50.44, 'test/accuracy_QuestionType_seven': 49.16, 'test/accuracy_QuestionType_four': 55.38, 'test/accuracy_QuestionType_five': 50.21, 'test/accuracy_QuestionType_three': 49.35, 'test/accuracy_QuestionType_nine': 40.24, 'test/accuracy_QuestionType_ten': 45.58, 'test/accuracy_QuestionType_two': 54.42, 'test/accuracy_QuestionType_six': 52.2, 'test/accuracy_AnswerType_other': 49.5, 'test/epoch': 0}[0m
/home/xl544/.conda/envs/RAVQA/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
Epoch 0, global step 282: 'test/accuracy_overall' reached 49.50000 (best 49.50000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-flanT5-qformer.21842925/train/saved_model/model_0.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5517241379310345, 'test/exact_match_at_2': 0.6805390408244154, 'test/exact_match_at_3': 0.7203725723345223, 'test/exact_match_at_4': 0.7328577090764963, 'test/exact_match_at_5': 0.7358303606817281, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3737613951644867, 'test/gold_recall_at_5': 0.6666666666666666, 'test/successful_hit': 0.36393182718985334, 'test/successful_no_hit': 0.1471264367816092, 'test/failed_hit': 0.3301228695996829, 'test/failed_no_hit': 0.15881886642885454, 'test/selected_successful_hit': 0.4393579072532699, 'test/selected_successful_no_hit': 0.11236623067776456, 'test/selected_failed_hit': 0.3571145461751883, 'test/selected_failed_no_hit': 0.09116131589377725, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 50.56, 'test/accuracy_QuestionType_one': 46.54, 'test/accuracy_QuestionType_eight': 48.98, 'test/accuracy_QuestionType_other': 52.99, 'test/accuracy_QuestionType_seven': 50.09, 'test/accuracy_QuestionType_four': 56.68, 'test/accuracy_QuestionType_five': 52.0, 'test/accuracy_QuestionType_three': 51.31, 'test/accuracy_QuestionType_nine': 36.9, 'test/accuracy_QuestionType_ten': 48.06, 'test/accuracy_QuestionType_two': 50.35, 'test/accuracy_QuestionType_six': 49.65, 'test/accuracy_AnswerType_other': 50.56, 'test/epoch': 1}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5576694411414982, 'test/exact_match_at_2': 0.6805390408244154, 'test/exact_match_at_3': 0.7168053904082442, 'test/exact_match_at_4': 0.7294887039239001, 'test/exact_match_at_5': 0.7312722948870393, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3737613951644867, 'test/gold_recall_at_5': 0.6666666666666666, 'test/successful_hit': 0.361474435196195, 'test/successful_no_hit': 0.15548949663099484, 'test/failed_hit': 0.3173999207292905, 'test/failed_no_hit': 0.16563614744351962, 'test/selected_successful_hit': 0.43717796274276655, 'test/selected_successful_no_hit': 0.12049147839873167, 'test/selected_failed_hit': 0.34145858105430044, 'test/selected_failed_no_hit': 0.10087197780420135, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 51.26, 'test/accuracy_QuestionType_one': 47.6, 'test/accuracy_QuestionType_eight': 50.3, 'test/accuracy_QuestionType_other': 53.02, 'test/accuracy_QuestionType_seven': 50.93, 'test/accuracy_QuestionType_four': 57.25, 'test/accuracy_QuestionType_five': 52.35, 'test/accuracy_QuestionType_three': 48.46, 'test/accuracy_QuestionType_nine': 39.05, 'test/accuracy_QuestionType_ten': 53.8, 'test/accuracy_QuestionType_two': 54.42, 'test/accuracy_QuestionType_six': 50.64, 'test/accuracy_AnswerType_other': 51.26, 'test/epoch': 1}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5584621482362268, 'test/exact_match_at_2': 0.6874752279032897, 'test/exact_match_at_3': 0.7233452239397543, 'test/exact_match_at_4': 0.7346413000396353, 'test/exact_match_at_5': 0.737217598097503, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3737613951644867, 'test/gold_recall_at_5': 0.6666666666666666, 'test/successful_hit': 0.36583432421720175, 'test/successful_no_hit': 0.15434007134363853, 'test/failed_hit': 0.3133967499009116, 'test/failed_no_hit': 0.16642885453824813, 'test/selected_successful_hit': 0.43896155370590567, 'test/selected_successful_no_hit': 0.11950059453032105, 'test/selected_failed_hit': 0.3499801823226318, 'test/selected_failed_no_hit': 0.09155766944114149, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 51.29, 'test/accuracy_QuestionType_one': 47.4, 'test/accuracy_QuestionType_eight': 50.09, 'test/accuracy_QuestionType_other': 51.89, 'test/accuracy_QuestionType_seven': 51.78, 'test/accuracy_QuestionType_four': 57.14, 'test/accuracy_QuestionType_five': 53.66, 'test/accuracy_QuestionType_three': 48.74, 'test/accuracy_QuestionType_nine': 42.62, 'test/accuracy_QuestionType_ten': 54.11, 'test/accuracy_QuestionType_two': 53.02, 'test/accuracy_QuestionType_six': 48.09, 'test/accuracy_AnswerType_other': 51.29, 'test/epoch': 1}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5687673404676972, 'test/exact_match_at_2': 0.6967895362663495, 'test/exact_match_at_3': 0.7282996432818074, 'test/exact_match_at_4': 0.7425683709869203, 'test/exact_match_at_5': 0.7459373761395165, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3737613951644867, 'test/gold_recall_at_5': 0.6666666666666666, 'test/successful_hit': 0.3669837495045581, 'test/successful_no_hit': 0.16139516448672214, 'test/failed_hit': 0.3073325406262386, 'test/failed_no_hit': 0.16428854538248117, 'test/selected_successful_hit': 0.4409433214427269, 'test/selected_successful_no_hit': 0.1278240190249703, 'test/selected_failed_hit': 0.33610780816488306, 'test/selected_failed_no_hit': 0.09512485136741974, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 52.3, 'test/accuracy_QuestionType_one': 48.86, 'test/accuracy_QuestionType_eight': 50.3, 'test/accuracy_QuestionType_other': 54.47, 'test/accuracy_QuestionType_seven': 52.62, 'test/accuracy_QuestionType_four': 57.39, 'test/accuracy_QuestionType_five': 53.49, 'test/accuracy_QuestionType_three': 52.06, 'test/accuracy_QuestionType_nine': 45.71, 'test/accuracy_QuestionType_ten': 53.49, 'test/accuracy_QuestionType_two': 53.37, 'test/accuracy_QuestionType_six': 48.79, 'test/accuracy_AnswerType_other': 52.3, 'test/epoch': 1}[0m
Epoch 1, global step 564: 'test/accuracy_overall' reached 52.30000 (best 52.30000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-flanT5-qformer.21842925/train/saved_model/model_1.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5596512088783194, 'test/exact_match_at_2': 0.6922314704716607, 'test/exact_match_at_3': 0.7288941736028538, 'test/exact_match_at_4': 0.7443519619500595, 'test/exact_match_at_5': 0.7477209671026556, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3737613951644867, 'test/gold_recall_at_5': 0.6666666666666666, 'test/successful_hit': 0.3667063020214031, 'test/successful_no_hit': 0.15790725326991675, 'test/failed_hit': 0.31030519223147046, 'test/failed_no_hit': 0.16508125247720967, 'test/selected_successful_hit': 0.43697978596908443, 'test/selected_successful_no_hit': 0.12267142290923504, 'test/selected_failed_hit': 0.3434403487911217, 'test/selected_failed_no_hit': 0.09690844233055886, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 51.46, 'test/accuracy_QuestionType_one': 46.96, 'test/accuracy_QuestionType_eight': 50.07, 'test/accuracy_QuestionType_other': 53.09, 'test/accuracy_QuestionType_seven': 51.12, 'test/accuracy_QuestionType_four': 57.64, 'test/accuracy_QuestionType_five': 53.32, 'test/accuracy_QuestionType_three': 50.61, 'test/accuracy_QuestionType_nine': 43.33, 'test/accuracy_QuestionType_ten': 50.39, 'test/accuracy_QuestionType_two': 53.6, 'test/accuracy_QuestionType_six': 50.21, 'test/accuracy_AnswerType_other': 51.46, 'test/epoch': 2}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5683709869203329, 'test/exact_match_at_2': 0.6971858898137139, 'test/exact_match_at_3': 0.7320650019817677, 'test/exact_match_at_4': 0.7447483154974237, 'test/exact_match_at_5': 0.748513674197384, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3737613951644867, 'test/gold_recall_at_5': 0.6666666666666666, 'test/successful_hit': 0.36741973840665876, 'test/successful_no_hit': 0.16476416963931828, 'test/failed_hit': 0.29175584621482364, 'test/failed_no_hit': 0.17606024573919937, 'test/selected_successful_hit': 0.4405469678953627, 'test/selected_successful_no_hit': 0.1278240190249703, 'test/selected_failed_hit': 0.3273880301228696, 'test/selected_failed_no_hit': 0.10424098295679746, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 52.29, 'test/accuracy_QuestionType_one': 47.95, 'test/accuracy_QuestionType_eight': 51.58, 'test/accuracy_QuestionType_other': 53.6, 'test/accuracy_QuestionType_seven': 52.29, 'test/accuracy_QuestionType_four': 59.05, 'test/accuracy_QuestionType_five': 53.17, 'test/accuracy_QuestionType_three': 51.59, 'test/accuracy_QuestionType_nine': 40.71, 'test/accuracy_QuestionType_ten': 55.04, 'test/accuracy_QuestionType_two': 51.86, 'test/accuracy_QuestionType_six': 50.64, 'test/accuracy_AnswerType_other': 52.29, 'test/epoch': 2}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5731272294887039, 'test/exact_match_at_2': 0.7007530717399921, 'test/exact_match_at_3': 0.7342449464922711, 'test/exact_match_at_4': 0.7449464922711059, 'test/exact_match_at_5': 0.7471264367816092, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3737613951644867, 'test/gold_recall_at_5': 0.6666666666666666, 'test/successful_hit': 0.36987713040031706, 'test/successful_no_hit': 0.16888624653190645, 'test/failed_hit': 0.29036860879904874, 'test/failed_no_hit': 0.1708680142687277, 'test/selected_successful_hit': 0.4425287356321839, 'test/selected_successful_no_hit': 0.13059849385652, 'test/selected_failed_hit': 0.3224336107808165, 'test/selected_failed_no_hit': 0.10443915973047958, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 52.64, 'test/accuracy_QuestionType_one': 49.52, 'test/accuracy_QuestionType_eight': 50.23, 'test/accuracy_QuestionType_other': 55.54, 'test/accuracy_QuestionType_seven': 51.73, 'test/accuracy_QuestionType_four': 59.89, 'test/accuracy_QuestionType_five': 53.69, 'test/accuracy_QuestionType_three': 51.12, 'test/accuracy_QuestionType_nine': 42.62, 'test/accuracy_QuestionType_ten': 50.7, 'test/accuracy_QuestionType_two': 54.07, 'test/accuracy_QuestionType_six': 50.64, 'test/accuracy_AnswerType_other': 52.64, 'test/epoch': 2}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5760998810939358, 'test/exact_match_at_2': 0.7007530717399921, 'test/exact_match_at_3': 0.7352358303606817, 'test/exact_match_at_4': 0.746730083234245, 'test/exact_match_at_5': 0.7487118509710662, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3737613951644867, 'test/gold_recall_at_5': 0.6666666666666666, 'test/successful_hit': 0.36785572730875943, 'test/successful_no_hit': 0.17098692033293697, 'test/failed_hit': 0.2930241775663892, 'test/failed_no_hit': 0.1681331747919144, 'test/selected_successful_hit': 0.4423305588585018, 'test/selected_successful_no_hit': 0.133769322235434, 'test/selected_failed_hit': 0.326397146254459, 'test/selected_failed_no_hit': 0.09750297265160524, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.0, 'test/accuracy_QuestionType_one': 49.37, 'test/accuracy_QuestionType_eight': 51.51, 'test/accuracy_QuestionType_other': 54.96, 'test/accuracy_QuestionType_seven': 51.21, 'test/accuracy_QuestionType_four': 58.77, 'test/accuracy_QuestionType_five': 54.96, 'test/accuracy_QuestionType_three': 51.78, 'test/accuracy_QuestionType_nine': 47.14, 'test/accuracy_QuestionType_ten': 52.09, 'test/accuracy_QuestionType_two': 55.12, 'test/accuracy_QuestionType_six': 51.06, 'test/accuracy_AnswerType_other': 53.0, 'test/epoch': 2}[0m
Epoch 2, global step 846: 'test/accuracy_overall' reached 53.00000 (best 53.00000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-flanT5-qformer.21842925/train/saved_model/model_2.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5794688862465319, 'test/exact_match_at_2': 0.7031311930241776, 'test/exact_match_at_3': 0.7326595323028141, 'test/exact_match_at_4': 0.7429647245342846, 'test/exact_match_at_5': 0.7443519619500595, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3737613951644867, 'test/gold_recall_at_5': 0.6666666666666666, 'test/successful_hit': 0.3661117717003567, 'test/successful_no_hit': 0.17534680935394373, 'test/failed_hit': 0.27027348394768136, 'test/failed_no_hit': 0.18826793499801822, 'test/selected_successful_hit': 0.43955608402695207, 'test/selected_successful_no_hit': 0.13991280221957986, 'test/selected_failed_hit': 0.30439952437574314, 'test/selected_failed_no_hit': 0.11613158937772493, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.22, 'test/accuracy_QuestionType_one': 48.91, 'test/accuracy_QuestionType_eight': 52.47, 'test/accuracy_QuestionType_other': 54.96, 'test/accuracy_QuestionType_seven': 51.5, 'test/accuracy_QuestionType_four': 59.82, 'test/accuracy_QuestionType_five': 55.3, 'test/accuracy_QuestionType_three': 51.78, 'test/accuracy_QuestionType_nine': 46.43, 'test/accuracy_QuestionType_ten': 50.54, 'test/accuracy_QuestionType_two': 54.53, 'test/accuracy_QuestionType_six': 51.21, 'test/accuracy_AnswerType_other': 53.22, 'test/epoch': 3}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.56995640110979, 'test/exact_match_at_2': 0.6981767736821245, 'test/exact_match_at_3': 0.7326595323028141, 'test/exact_match_at_4': 0.7453428458184701, 'test/exact_match_at_5': 0.7481173206500198, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3737613951644867, 'test/gold_recall_at_5': 0.6666666666666666, 'test/successful_hit': 0.3667063020214031, 'test/successful_no_hit': 0.1668648434403488, 'test/failed_hit': 0.29207292905271504, 'test/failed_no_hit': 0.1743559254855331, 'test/selected_successful_hit': 0.4393579072532699, 'test/selected_successful_no_hit': 0.13059849385652, 'test/selected_failed_hit': 0.3222354340071344, 'test/selected_failed_no_hit': 0.10780816488307571, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 52.42, 'test/accuracy_QuestionType_one': 48.41, 'test/accuracy_QuestionType_eight': 50.79, 'test/accuracy_QuestionType_other': 55.28, 'test/accuracy_QuestionType_seven': 50.33, 'test/accuracy_QuestionType_four': 58.73, 'test/accuracy_QuestionType_five': 54.09, 'test/accuracy_QuestionType_three': 51.68, 'test/accuracy_QuestionType_nine': 42.14, 'test/accuracy_QuestionType_ten': 50.54, 'test/accuracy_QuestionType_two': 54.77, 'test/accuracy_QuestionType_six': 51.63, 'test/accuracy_AnswerType_other': 52.42, 'test/epoch': 3}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5776852952833927, 'test/exact_match_at_2': 0.7013476020610384, 'test/exact_match_at_3': 0.7324613555291319, 'test/exact_match_at_4': 0.7413793103448276, 'test/exact_match_at_5': 0.7435592548553309, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3737613951644867, 'test/gold_recall_at_5': 0.6666666666666666, 'test/successful_hit': 0.36845025762980577, 'test/successful_no_hit': 0.17621878715814507, 'test/failed_hit': 0.27827982560443915, 'test/failed_no_hit': 0.17705112960761, 'test/selected_successful_hit': 0.43717796274276655, 'test/selected_successful_no_hit': 0.14050733254062625, 'test/selected_failed_hit': 0.3093539437177963, 'test/selected_failed_no_hit': 0.11296076099881094, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 52.98, 'test/accuracy_QuestionType_one': 49.32, 'test/accuracy_QuestionType_eight': 52.09, 'test/accuracy_QuestionType_other': 54.18, 'test/accuracy_QuestionType_seven': 51.87, 'test/accuracy_QuestionType_four': 58.73, 'test/accuracy_QuestionType_five': 54.83, 'test/accuracy_QuestionType_three': 52.66, 'test/accuracy_QuestionType_nine': 37.86, 'test/accuracy_QuestionType_ten': 51.78, 'test/accuracy_QuestionType_two': 56.05, 'test/accuracy_QuestionType_six': 51.06, 'test/accuracy_AnswerType_other': 52.98, 'test/epoch': 3}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5824415378517638, 'test/exact_match_at_2': 0.6993658343242172, 'test/exact_match_at_3': 0.7312722948870393, 'test/exact_match_at_4': 0.7415774871185097, 'test/exact_match_at_5': 0.7435592548553309, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3737613951644867, 'test/gold_recall_at_5': 0.6666666666666666, 'test/successful_hit': 0.36535869996036463, 'test/successful_no_hit': 0.17808164883075703, 'test/failed_hit': 0.2768133174791914, 'test/failed_no_hit': 0.17974633372968687, 'test/selected_successful_hit': 0.4403487911216805, 'test/selected_successful_no_hit': 0.14209274673008324, 'test/selected_failed_hit': 0.30439952437574314, 'test/selected_failed_no_hit': 0.11315893777249307, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.66, 'test/accuracy_QuestionType_one': 50.11, 'test/accuracy_QuestionType_eight': 51.98, 'test/accuracy_QuestionType_other': 56.28, 'test/accuracy_QuestionType_seven': 51.64, 'test/accuracy_QuestionType_four': 60.85, 'test/accuracy_QuestionType_five': 54.43, 'test/accuracy_QuestionType_three': 53.41, 'test/accuracy_QuestionType_nine': 45.0, 'test/accuracy_QuestionType_ten': 53.8, 'test/accuracy_QuestionType_two': 55.0, 'test/accuracy_QuestionType_six': 49.93, 'test/accuracy_AnswerType_other': 53.66, 'test/epoch': 3}[0m
Epoch 3, global step 1128: 'test/accuracy_overall' reached 53.66000 (best 53.66000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-flanT5-qformer.21842925/train/saved_model/model_3.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5868014268727705, 'test/exact_match_at_2': 0.7031311930241776, 'test/exact_match_at_3': 0.7352358303606817, 'test/exact_match_at_4': 0.7437574316290131, 'test/exact_match_at_5': 0.7447483154974237, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3737613951644867, 'test/gold_recall_at_5': 0.6666666666666666, 'test/successful_hit': 0.36662703131193025, 'test/successful_no_hit': 0.17970669837495046, 'test/failed_hit': 0.2657154181529925, 'test/failed_no_hit': 0.18795085216012683, 'test/selected_successful_hit': 0.44193420531113753, 'test/selected_successful_no_hit': 0.14486722156163298, 'test/selected_failed_hit': 0.29429250891795483, 'test/selected_failed_no_hit': 0.11890606420927467, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.92, 'test/accuracy_QuestionType_one': 49.64, 'test/accuracy_QuestionType_eight': 52.37, 'test/accuracy_QuestionType_other': 56.45, 'test/accuracy_QuestionType_seven': 51.17, 'test/accuracy_QuestionType_four': 60.92, 'test/accuracy_QuestionType_five': 56.25, 'test/accuracy_QuestionType_three': 53.08, 'test/accuracy_QuestionType_nine': 45.0, 'test/accuracy_QuestionType_ten': 51.94, 'test/accuracy_QuestionType_two': 55.7, 'test/accuracy_QuestionType_six': 51.21, 'test/accuracy_AnswerType_other': 53.92, 'test/epoch': 4}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.582639714625446, 'test/exact_match_at_2': 0.7031311930241776, 'test/exact_match_at_3': 0.734046769718589, 'test/exact_match_at_4': 0.7441537851763773, 'test/exact_match_at_5': 0.746730083234245, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3737613951644867, 'test/gold_recall_at_5': 0.6666666666666666, 'test/successful_hit': 0.3654379706698375, 'test/successful_no_hit': 0.17665477606024574, 'test/failed_hit': 0.2696393182718985, 'test/failed_no_hit': 0.18826793499801822, 'test/selected_successful_hit': 0.4403487911216805, 'test/selected_successful_no_hit': 0.14229092350376535, 'test/selected_failed_hit': 0.30063416567578277, 'test/selected_failed_no_hit': 0.11672611969877131, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.65, 'test/accuracy_QuestionType_one': 49.0, 'test/accuracy_QuestionType_eight': 52.0, 'test/accuracy_QuestionType_other': 56.54, 'test/accuracy_QuestionType_seven': 52.43, 'test/accuracy_QuestionType_four': 61.2, 'test/accuracy_QuestionType_five': 55.28, 'test/accuracy_QuestionType_three': 53.55, 'test/accuracy_QuestionType_nine': 40.24, 'test/accuracy_QuestionType_ten': 51.32, 'test/accuracy_QuestionType_two': 54.77, 'test/accuracy_QuestionType_six': 50.92, 'test/accuracy_AnswerType_other': 53.65, 'test/epoch': 4}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5850178359096314, 'test/exact_match_at_2': 0.7063020214030915, 'test/exact_match_at_3': 0.737217598097503, 'test/exact_match_at_4': 0.7457391993658343, 'test/exact_match_at_5': 0.7487118509710662, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3737613951644867, 'test/gold_recall_at_5': 0.6666666666666666, 'test/successful_hit': 0.36741973840665876, 'test/successful_no_hit': 0.18085612366230677, 'test/failed_hit': 0.26183115338882285, 'test/failed_no_hit': 0.18989298454221165, 'test/selected_successful_hit': 0.4401506143479984, 'test/selected_successful_no_hit': 0.14486722156163298, 'test/selected_failed_hit': 0.2980578676179152, 'test/selected_failed_no_hit': 0.11692429647245343, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.8, 'test/accuracy_QuestionType_one': 49.64, 'test/accuracy_QuestionType_eight': 52.23, 'test/accuracy_QuestionType_other': 55.9, 'test/accuracy_QuestionType_seven': 51.36, 'test/accuracy_QuestionType_four': 61.45, 'test/accuracy_QuestionType_five': 55.35, 'test/accuracy_QuestionType_three': 54.11, 'test/accuracy_QuestionType_nine': 42.14, 'test/accuracy_QuestionType_ten': 52.56, 'test/accuracy_QuestionType_two': 54.65, 'test/accuracy_QuestionType_six': 52.06, 'test/accuracy_AnswerType_other': 53.8, 'test/epoch': 4}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5834324217201744, 'test/exact_match_at_2': 0.7007530717399921, 'test/exact_match_at_3': 0.734046769718589, 'test/exact_match_at_4': 0.7429647245342846, 'test/exact_match_at_5': 0.7437574316290131, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3737613951644867, 'test/gold_recall_at_5': 0.6666666666666666, 'test/successful_hit': 0.3660721363456203, 'test/successful_no_hit': 0.18252080856123662, 'test/failed_hit': 0.2557273087594134, 'test/failed_no_hit': 0.1956797463337297, 'test/selected_successful_hit': 0.43816884661117717, 'test/selected_successful_no_hit': 0.14526357510899723, 'test/selected_failed_hit': 0.2911216805390408, 'test/selected_failed_no_hit': 0.12544589774078477, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.66, 'test/accuracy_QuestionType_one': 50.01, 'test/accuracy_QuestionType_eight': 51.88, 'test/accuracy_QuestionType_other': 55.77, 'test/accuracy_QuestionType_seven': 51.68, 'test/accuracy_QuestionType_four': 61.66, 'test/accuracy_QuestionType_five': 55.01, 'test/accuracy_QuestionType_three': 53.64, 'test/accuracy_QuestionType_nine': 42.14, 'test/accuracy_QuestionType_ten': 52.4, 'test/accuracy_QuestionType_two': 52.79, 'test/accuracy_QuestionType_six': 51.49, 'test/accuracy_AnswerType_other': 53.66, 'test/epoch': 4}[0m
Epoch 4, global step 1410: 'test/accuracy_overall' was not in top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.587594133967499, 'test/exact_match_at_2': 0.7007530717399921, 'test/exact_match_at_3': 0.7342449464922711, 'test/exact_match_at_4': 0.7427665477606025, 'test/exact_match_at_5': 0.7447483154974237, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3737613951644867, 'test/gold_recall_at_5': 0.6666666666666666, 'test/successful_hit': 0.36329766151407056, 'test/successful_no_hit': 0.18474038842647642, 'test/failed_hit': 0.25691636940150614, 'test/failed_no_hit': 0.1950455806579469, 'test/selected_successful_hit': 0.43876337693222356, 'test/selected_successful_no_hit': 0.14883075703527546, 'test/selected_failed_hit': 0.28814902893380895, 'test/selected_failed_no_hit': 0.12425683709869204, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 54.02, 'test/accuracy_QuestionType_one': 50.14, 'test/accuracy_QuestionType_eight': 52.84, 'test/accuracy_QuestionType_other': 56.83, 'test/accuracy_QuestionType_seven': 52.2, 'test/accuracy_QuestionType_four': 61.55, 'test/accuracy_QuestionType_five': 54.81, 'test/accuracy_QuestionType_three': 53.04, 'test/accuracy_QuestionType_nine': 40.95, 'test/accuracy_QuestionType_ten': 53.49, 'test/accuracy_QuestionType_two': 55.0, 'test/accuracy_QuestionType_six': 52.06, 'test/accuracy_AnswerType_other': 54.02, 'test/epoch': 5}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.582639714625446, 'test/exact_match_at_2': 0.7027348394768134, 'test/exact_match_at_3': 0.7360285374554102, 'test/exact_match_at_4': 0.7455410225921522, 'test/exact_match_at_5': 0.7473246135552913, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3737613951644867, 'test/gold_recall_at_5': 0.6666666666666666, 'test/successful_hit': 0.3654776060245739, 'test/successful_no_hit': 0.18240190249702734, 'test/failed_hit': 0.2613951644867222, 'test/failed_no_hit': 0.19072532699167657, 'test/selected_successful_hit': 0.4349980182322632, 'test/selected_successful_no_hit': 0.14764169639318273, 'test/selected_failed_hit': 0.2909235037653587, 'test/selected_failed_no_hit': 0.12643678160919541, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.63, 'test/accuracy_QuestionType_one': 49.35, 'test/accuracy_QuestionType_eight': 51.14, 'test/accuracy_QuestionType_other': 56.09, 'test/accuracy_QuestionType_seven': 51.36, 'test/accuracy_QuestionType_four': 61.27, 'test/accuracy_QuestionType_five': 55.58, 'test/accuracy_QuestionType_three': 52.62, 'test/accuracy_QuestionType_nine': 40.48, 'test/accuracy_QuestionType_ten': 55.04, 'test/accuracy_QuestionType_two': 55.58, 'test/accuracy_QuestionType_six': 55.18, 'test/accuracy_AnswerType_other': 53.63, 'test/epoch': 5}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5856123662306778, 'test/exact_match_at_2': 0.7009512485136742, 'test/exact_match_at_3': 0.7330558858501783, 'test/exact_match_at_4': 0.7409829567974633, 'test/exact_match_at_5': 0.7425683709869203, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3737613951644867, 'test/gold_recall_at_5': 0.6666666666666666, 'test/successful_hit': 0.3648038049940547, 'test/successful_no_hit': 0.187871581450654, 'test/failed_hit': 0.24589774078478002, 'test/failed_no_hit': 0.2014268727705113, 'test/selected_successful_hit': 0.4353943717796274, 'test/selected_successful_no_hit': 0.15021799445105033, 'test/selected_failed_hit': 0.27764565992865636, 'test/selected_failed_no_hit': 0.1367419738406659, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.84, 'test/accuracy_QuestionType_one': 49.08, 'test/accuracy_QuestionType_eight': 52.05, 'test/accuracy_QuestionType_other': 57.06, 'test/accuracy_QuestionType_seven': 50.33, 'test/accuracy_QuestionType_four': 62.19, 'test/accuracy_QuestionType_five': 55.4, 'test/accuracy_QuestionType_three': 53.36, 'test/accuracy_QuestionType_nine': 44.52, 'test/accuracy_QuestionType_ten': 53.18, 'test/accuracy_QuestionType_two': 54.65, 'test/accuracy_QuestionType_six': 53.05, 'test/accuracy_AnswerType_other': 53.84, 'test/epoch': 5}[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.586008719778042, 'test/exact_match_at_2': 0.7011494252873564, 'test/exact_match_at_3': 0.7334522393975426, 'test/exact_match_at_4': 0.7395957193816884, 'test/exact_match_at_5': 0.7413793103448276, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3737613951644867, 'test/gold_recall_at_5': 0.6666666666666666, 'test/successful_hit': 0.36551724137931035, 'test/successful_no_hit': 0.18398731668648435, 'test/failed_hit': 0.2541418945699564, 'test/failed_no_hit': 0.1963535473642489, 'test/selected_successful_hit': 0.43737613951644866, 'test/selected_successful_no_hit': 0.14863258026159334, 'test/selected_failed_hit': 0.2845818470075307, 'test/selected_failed_no_hit': 0.12940943321442727, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 53.91, 'test/accuracy_QuestionType_one': 49.47, 'test/accuracy_QuestionType_eight': 52.7, 'test/accuracy_QuestionType_other': 56.16, 'test/accuracy_QuestionType_seven': 50.75, 'test/accuracy_QuestionType_four': 61.94, 'test/accuracy_QuestionType_five': 55.58, 'test/accuracy_QuestionType_three': 52.71, 'test/accuracy_QuestionType_nine': 42.86, 'test/accuracy_QuestionType_ten': 54.11, 'test/accuracy_QuestionType_two': 55.0, 'test/accuracy_QuestionType_six': 53.48, 'test/accuracy_AnswerType_other': 53.91, 'test/epoch': 5}[0m
Epoch 5, global step 1692: 'test/accuracy_overall' reached 53.91000 (best 53.91000), saving model to '/rds/user/xl544/hpc-work/Retrieval-Augmented-Visual-Question-Answering/Experiments/OKVQA_RA-VQA-flanT5-qformer.21842925/train/saved_model/model_5.ckpt' as top 1
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_exact_match'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_retrieval_metrics'}...[0m
[38;20m[INFO] - trainers.metrics_processors : Running metrics {'name': 'compute_okvqa_scores'}...[0m
[38;20m[INFO] - trainers.rag_executor : Evaluation results [validate]: {'test/exact_match_at_1': 0.5883868410622275, 'test/exact_match_at_2': 0.698969480776853, 'test/exact_match_at_3': 0.7312722948870393, 'test/exact_match_at_4': 0.7376139516448672, 'test/exact_match_at_5': 0.7395957193816884, 'test/recall_at_5': 0.8127229488703924, 'test/precision_at_5': 0.5183511692429646, 'test/gold_precision_at_5': 0.3737613951644867, 'test/gold_recall_at_5': 0.6666666666666666, 'test/successful_hit': 0.3615537059056679, 'test/successful_no_hit': 0.18767340467697186, 'test/failed_hit': 0.2460166468489893, 'test/failed_no_hit': 0.204756242568371, 'test/selected_successful_hit': 0.43638525564803804, 'test/selected_successful_no_hit': 0.15200158541418946, 'test/selected_failed_hit': 0.27724930638129214, 'test/selected_failed_no_hit': 0.13436385255648037, 'test/n_retrieved_docs': 5, 'test/accuracy_overall': 54.15, 'test/accuracy_QuestionType_one': 50.18, 'test/accuracy_QuestionType_eight': 53.09, 'test/accuracy_QuestionType_other': 55.22, 'test/accuracy_QuestionType_seven': 51.21, 'test/accuracy_QuestionType_four': 61.69, 'test/accuracy_QuestionType_five': 56.1, 'test/accuracy_QuestionType_three': 53.69, 'test/accuracy_QuestionType_nine': 45.24, 'test/accuracy_QuestionType_ten': 53.02, 'test/accuracy_QuestionType_two': 55.0, 'test/accuracy_QuestionType_six': 53.05, 'test/accuracy_AnswerType_other': 54.15, 'test/epoch': 6}[0m
